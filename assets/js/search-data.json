{
  
    
        "post0": {
            "title": "脳が対側支配をする進化的な利点は何か",
            "content": "神経解剖を学ぶと、次のような疑問は自然と生じると思います。それは「なぜ脳では対側支配(contralateral innervation)があるのか」というものです。ここでの対側とは、身体の正中線に対し反対側のことを指します。例としては左運動野が右側の筋群を、右運動野 が 左側の筋群 を制御するといった対側制御(contralateral control)が挙げられます。何ともややこしい配線ですが、なぜ同側支配ではダメだったのでしょうか？対側支配をする利点はあるのでしょうか？ . かなり基本的な内容に対する疑問ではありますが、この問題はRamón y Cajalの1898年の論文に端を発します (Ramón y Cajal. 1898)。古くはHippocratesも、左頭部負傷が右の筋群の運動に影響を与えることを知っていたようです (Vulliemoz et al., Lancet Neurol. 2005)。しかしながら、神経科学の著名な教科書にもこれに対する明確な回答はありません (カンデル神経科学や神経科学-脳の探求-を一通りざっと見ただけですが)。幸い、英語でGoogle検索すると当然ながら同様の疑問を抱く人は多数いるようで、Quoraにおいていくつかの解答を見つけました (Why does our left hemisphere of brain control our right side of our body and the right our left? - Quora)。 . 【追記(2020/03/10)】WikipediaのContralateral brainのページとも内容が被っていることに気づきました。 . Quoraの記事で紹介されている論文を足掛かりに文献を調べた結果ですが、この問題に関する優れた総説として(Vulliemoz et al., Lancet Neurol. 2005; Mora et al., Neurosurg. Focus. 2019)があります。 . 本記事では、対側支配の神経解剖を復習しつつ、脳が対側支配をする進化的な利点の仮説を紹介していきます。なお、仮説ということを強調しているのはこれが進化的な話を含み、実験的証明が難しいためです。また、筆者は神経解剖学・神経発生学の教室でも研究をしていますが、本記事における内容に関しては全く研究していないのでご了承ください。 . 正中を横切る神経線維 . まず、正中を横切る神経線維は交叉(decussation)や交連(commissure)と言った名称がつけられています。ここではいくつかの経路を簡単にまとめましたが、詳しくは何処のご家庭にもある神経解剖の教科書を読みましょう (今回は寺島先生の神経解剖学講義ノートを参考にしました)。 . 視交叉(optic chiasma) . 視神経の交叉のことです。両生類、魚類、鳥類では視神経が完全に交叉します(完全交叉)が、ネコなどの食肉類や霊長類では50%の視神経のみが交叉します(半交叉)。 . 運動路における交叉 . 運動路で代表的なのが皮質脊髄路(corticospinal tract, CST; または錐体路とも)です。皮質脊髄路は主に運動性皮質L5の錐体細胞を起始とし、脊髄に終わる経路です。皮質脊髄路の途中で延髄腹側の錐体において神経線維が交叉しますが、これを錐体交叉(または運動交叉) (pyramidal decussation, motor decussation)と言います。錐体交叉では全ての神経が交叉するわけではないですが、交叉しなかった線維も脊髄の白交連(anterior white commissure)で最終的には交叉します。なので、皮質脊髄路線維は全て対側の運動ニューロンを支配することになります。他には赤核脊髄路 (rubrospinal tract)における腹側被蓋交叉(tegmental decussation)や、視蓋脊髄路 (tectospinal tract)における背側被蓋交叉(dorsal tegmental decussation)があります。 . 感覚路における交叉 . 感覚路も色々ありますが、頭部以外の体性感覚の伝導路であれば後索・内側毛帯系と脊髄視床路系があります。後索・内側毛帯系では毛帯交叉(sensory decussation, decussation of the lemniscus)、脊髄視床路系では脊髄の白交連を経由して対側の体性感覚野に至ります。 . 小脳の経路における交叉 . 念のため触れておきます。まず、小脳は同側支配です。例えば右小脳半球外側部を損傷すると、右側 (患側)に小脳性運動失調が現れます。この理由は、小脳からの出力線維が交叉しない(例えば非交叉性室頂核前庭線維)か、2回交叉するためです。2回交叉は裏の裏が表になるのと同じことで、例えば上記の症例における経路は、小脳半球外側部から視床VL核までで上小脳脚交叉があり、視床から大脳皮質は交叉無し、大脳皮質から皮質脊髄路の錐体交叉でもう一度交叉する、となっています。 . 神経線維の交叉に関連した疾患 . 神経系の発生においては軸索誘導分子(axon guidance molecules)の寄与が必要ですが、それらをコードする遺伝子に異常が生じると神経発生が正常に行われなくなります。そのような疾患の例として、クリッペル・ファイル症候群(Klippel-Feil syndrome)、 X連鎖性カルマン症候群(X-linked Kallmann’s syndrome)などでは非交叉性の皮質遠心性線維が形成されることで鏡像運動(mirror movements; 随意的な運動を行うときに、対側にも不随意的に運動が生じる症状)が生じます (Comer et al., Neural Dev. 2019)。しかしながら病態機序はいくつか考えられるようなので、詳細は書きません (cf. 脳科学辞典の鏡像運動の項目)。 . Ramón y Cajalの仮説 . この節は、Ramón y Cajalの仮説についての説明をします。なお、Cajalの発表の後となりますが英国の医師であったFrancis DixonもCajalと類似の説を提唱しています(Dixon, The Dublin Journal of Medical Science. 1907; Dixon, The Dublin Journal of Medical Science. 1918)。 . さて、Cajalの視交叉についての仮説は、脳内での外界の連続性を維持するためには交叉が必要である、ということです。次図は前方に眼があり視交叉が無い生物が矢を見た場合の図です。網膜で外界が反転されるため、交叉が無いと矢尻と矢羽が繋がった像となります。そのため、2つの視野像から脳内で外界を復元するには像を交叉させる必要があります。 . . (Ramón y Cajal. 1898; 図の間接的な引用元は Ramón Y Cajal. Histología del sistema nervioso del hombre y de los vertebrados. Tomo II. Segunda parte. 2012) . 次図の上部は、前述の通り視交叉により外界が正しく復元されることを示しています。また、完全な交叉ではない半交叉であることが立体視に重要であることも指摘していました。Cajalはさらに説を広げ、運動路と感覚路に交叉がある理由も視交叉と関連付けました。これが対側支配におけるCajalの仮説です。次図中央部では連続した外界が復元されていますが、このままでは像が反転しています。Cajalは視野の右側を体の右側と、視野の左側を体の左側と一致させるために、運動路と体性感覚路を交叉させる必要があると主張しました。 . . (Ramón y Cajal. 1898) . しかし、Cajalの仮説は完全に誤っているわけではないものの、いくつかの問題点があることが指摘されています (de Lussanet &amp; Osse, Animal Biol. 2012)。まず、視交叉の説について、眼球は動くため、網膜像も動きます。そのため、位置合わせを完全に正確にすることはできません (予測により補間はしていると思われるので、Cajalが間違っているとは言えませんが)。次に、運動路と感覚路を関連させた説については、同側の視覚情報・感覚情報のみが運動野に至るというのは最適ではないということです。運動することを考えれば、両側の情報を統合して座標系を生み出した方がよいでしょうし、実際に脳はそうしています。 . ということで、Cajalの仮説は完全に誤りではないにせよ、間違っている点も見られるということになります。ただ、Cajalが偉大だったことに変わりはないなと、今回スケッチを見直して思いました。 . ねじれ仮説 (twist hypothesis) . 無脊椎動物から脊椎動物へ至る間に体のねじれが生じた、というのがねじれ仮説 (twist hypothesis) です。これは対側支配が生じた目的ではなく、その過程についての仮説となっています。 . ねじれ仮説には(Kinsbourne, Neuropsychology. 2013)の体性ねじれ説 (somatic twist hypothesis) と、(de Lussanet &amp; Osse, Animal Biol. 2012)の軸ねじれ説 (axial twist hypothesis) の2つが提案されています。まず、Kinsbourneの体性ねじれ説は、180度の回転が一回起こった、というものです。一方で、de LussanetとOsseの軸ねじれ説は2つの90度の回転が起こった、というものです。両者の差異は分かりづらいですが、de LussanetとOsseによって解説されています (de Lussanet &amp; Osse, Neuropsychology. 2015)。 . . (de Lussanet &amp; Osse, Neuropsychology. 2015, Fig.1) . AはKinsbourneの体性ねじれ説を示しており、前口動物(Protostome)が脊椎動物(vertebrate)へと進化するに伴い、前脳と眼の領域が180度回転したことを表します。Bはde LussanetとOsseの軸ねじれ説で、黒線が背側、白線が腹側、点は眼となる領域、ovはoptic vesicleを意味します。重要なのはB2で、前脳と眼の領域が90度の時計回り、中脳より下が90度の反時計回りをすることで合計で180度のねじれが生まれます。 . どちらの仮説も視交叉や、交叉しない嗅覚経路、および小脳の同側支配などを説明します。しかし、de LussanetとOsseは軸ねじれ説であれば、心臓や腸管などの臓器が左右対称ではないこと、などが説明できると主張しています。また、両者の説の弱いところとして視交叉などでの全交叉は説明しても、一部の神経が同側に向かう半交叉は説明できない、といった点を挙げています。 . いずれにせよ、進化の過程で何らかのねじれが生じたことは誤りではないでしょうが、何故そのねじれが保存されてきたのでしょうか？やはり、そこには対側支配の利点があったのでしょう。次節からは対側支配(主に皮質脊髄路や赤核脊髄路の運動制御、脊髄視床路の体性感覚)の進化的利点についての仮説を見ていきます。 . 回避行動仮説 (avoidance behavior hypothesis) . 危険な刺激を知覚し回避行動を取るために体の構造の進化に伴って交叉が必要となった、というのが回避行動仮説 (avoidance behavior hypothesis) です (Vulliemoz et al., Lancet Neurol. 2005)。 . . (Vulliemoz et al., Lancet Neurol. 2005, Fig.2) . まず、交叉が無く、手足も無い原始的な種の場合を考えます(図上部; なお図では魚のような見た目ですが、魚類の錐体路も交叉が存在するようです)。このような生物の場合、危険な刺激は右脳半球によって知覚され、右半身の筋肉の収縮を引き起こし、屈曲させることで危険を回避します。このとき、交叉していない原始的な経路 (網様体脊髄路や前庭脊髄路)を介して筋肉は制御されます。 . 一方で、四肢を持つ脊椎動物(図下部)は、左肢を伸ばすことにより、左側の刺激を回避しようとします。このとき、交叉のある(系統発生的に)新しい経路 (皮質脊髄路や赤核脊髄路)を介して筋肉は制御されます。 . 特に根拠についての記載はありませんでしたが、１つの説と言えます。 . 保護機構仮説 (protective mechanism hypothesis) . 対側支配であることは身体保護の観点で優れている、という説が保護機構仮説 (protective mechanism hypothesis) です (Whitehead &amp; Banihani, Laterality. 2014)。前節の回避行動仮説と体を守るという観点では似ていますが、少し異なります。これまでに紹介してきた論文と異なり、WhiteheadとBanihaniは中程度の損傷を受けた場合、対側支配の方が生存しやすいことを簡単な数理モデルを用いて説明しています。ここでは数理モデルの紹介は省略し、簡単なお気持ちだけ説明します。 . ここでは、生物の体が左右の脳半球(と神経)及び左右の筋群の4つから成ると考えます。両半身を損傷すると致命的となることが多いですが、片半身だけの損傷であれば生存することも可能です。このとき、例えば右脳半球と左筋群の組み合わせが損傷を受けることはあまりなく、どちらかというと転倒や衝突により片側の脳半球と筋群が傷害されやすいでしょう。 . 同側支配の場合、片側の脳半球と筋群が同時に損傷を受けると、損傷を受けた半球と損傷を受けた筋群という組み合わせにより片側は完全に動かすことができなくなります。一方で対側支配の場合、各脳半球は反対側を制御するため、例えば右半身を損傷した場合は、損傷した右半身を制御する無傷の左脳半球と、無傷の左側を制御する損傷した右脳半球という状況になります。無傷の左半球は損傷しているとは言え、一部の右側の筋群を動かすことができるでしょう。また、損傷した右半球であっても左側の筋群は無事であるためにある程度は制御できるでしょう。 . 片側が完全に機能しない場合に比べ、両側に影響はあるものの何とか動かすことができる場合の方が生存しやすいと仮定すると、交叉が保存されてきたことにも納得できます。 . まとめ . 本記事では主に運動野と感覚野の対側支配がどのように生じたのか、また何故対側支配が保存されてきたのか、ということについての仮説を見てきました (重ねて言いますが仮説止まりです)。現在提案されている説をまとめると、体がねじれることで対側支配が生まれ、それが生存にとって有利であったために保存されてきた、と言えます。冒頭に述べたように、進化が絡むと直接的な証拠というものを見つけづらいとは思いますが (他の研究例を知らないだけかもしれませんが)、将来は何らかの説に落ち着くのではないか、と思っています。 . 参考文献 . Carson RG. Neural pathways mediating bilateral interactions between the upper limbs. Brain Res Rev. 2005;49(3):641–662. doi:10.1016/j.brainresrev.2005.03.005 https://www.sciencedirect.com/science/article/abs/pii/S0165017305000482?via%3Dihub . | Comer JD, Alvarez S, Butler SJ, Kaltschmidt JA. Commissural axon guidance in the developing spinal cord: from Cajal to the present day. Neural Dev. 2019;14(1):9. doi:10.1186/s13064-019-0133-1 . | . https://neuraldevelopment.biomedcentral.com/articles/10.1186/s13064-019-0133-1 . de Lussanet MH, Osse JW. Decussation as an axial twist: A comment on Kinsbourne (2013). Neuropsychology. 2015;29(5):713–714. doi:10.1037/neu0000163 | . https://peerj.com/preprints/432.pdf . de Lussanet, MH, Osse, JW. An ancestral axial twist explains the contralateral forebrain and the optic chiasm in vertebrates. Animal Biol. 2012; 62: 193-216. doi:10.1163/157075611X617102. | . https://arxiv.org/abs/1003.1872 . Dixon AF. Why are the great motor and sensory tracts of the central nervous system crossed?. The Dublin Journal of Medical Science. 1907; 124, 1–4. doi:10.1007/BF02972358 | . https://link.springer.com/article/10.1007/BF02972358 . Dixon AF. Why are the cerebral motor and sensory cortical areas arranged in an inverted order?. The Dublin Journal of Medical Science. 1918; 145, 154–160. doi:10.1007/BF02958527 https://link.springer.com/article/10.1007%2FBF02958527 . | Kinsbourne M. Somatic twist: a model for the evolution of decussation. Neuropsychology. 2013;27(5):511–515. doi:10.1037/a0033662 https://www.semanticscholar.org/paper/Somatic-twist%3A-a-model-for-the-evolution-of-Kinsbourne/fe2d36ddc63af9f26aa22ec1bbce115f7aa35387 . | Loosemore RG. The inversion hypothesis: A novel explanation for the contralaterality of the human brain. Biosci Hypotheses. 2009;2:375–382 . | . https://www.sciencedirect.com/science/article/abs/pii/S1756239209001347 . Mora C, Velásquez C, Martino J. The neural pathway midline crossing theory: a historical analysis of Santiago Rámon y Cajal’s contribution on cerebral localization and on contralateral forebrain organization. Neurosurg Focus. 2019;47(3):E10. doi:10.3171/2019.6.FOCUS19341 | . https://thejns.org/focus/view/journals/neurosurg-focus/47/3/article-pE10.xml . Ramón y Cajal, S. Estructura del kiasma óptico y teoría general de los entrecruzamientos de las vías nerviosas. 1898. [german 1899, english 2004]. Rev. Trim. Microgràfica 3, 15–65. . | Ramón Y Cajal, S. Histología del sistema nervioso del hombre y de los vertebrados. Tomo II. Segunda parte. Boletín Oficial del Estado. 2012. https://digital.csic.es/bitstream/10261/158606/1/Cap%C3%ADtulo32-T2-2%C2%AA%20parte.pdf . | Shinbrot T, Young W. Why decussate? Topological constraints on 3D wiring. Anat Rec (Hoboken). 2008;291(10):1278–1292. doi:10.1002/ar.20731 . | . https://anatomypubs.onlinelibrary.wiley.com/doi/full/10.1002/ar.20731 . Vulliemoz S, Raineteau O, Jabaudon D. Reaching beyond the midline: why are human brains cross wired?. Lancet Neurol. 2005;4(2):87–99. doi:10.1016/S1474-4422(05)00990-7 | . https://www.thelancet.com/journals/laneur/article/PIIS1474-4422(05)00990-7/fulltext . Welniarz Q, Dusart I, Roze E. The corticospinal tract: Evolution, development, and human disorders. Dev Neurobiol. 2017;77(7):810–829. doi:10.1002/dneu.22455 | . https://onlinelibrary.wiley.com/doi/abs/10.1002/dneu.22455 . Whitehead L, Banihani S. The evolution of contralateral control of the body by the brain: is it a protective mechanism?. Laterality. 2014;19(3):325–339. doi:10.1080/1357650X.2013.824461 | . https://www.researchgate.net/publication/255732556_The_evolution_of_contralateral_control_of_the_body_by_the_brain_Is_it_a_protective_mechanism .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/neuroscience/2020/03/07/contralateral_brain.html",
            "relUrl": "/neuroscience/2020/03/07/contralateral_brain.html",
            "date": " • Mar 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Pythonによる分位点回帰 (Quantile regression)",
            "content": "Distributional Reinforcement Learningの理解のために必要だったのでメモとして残しておきます (本当はexpectile regressionも書かなければならないですが)。内容としてはNumpyで勾配法により分位点回帰をする、というものになっています。 . &#20998;&#20301;&#28857;&#22238;&#24112; (Quantile Regression)&#12392;&#12399; . 通常の最小二乗法による線形回帰(Ordinary least squares regression)は、誤差が正規分布と仮定したときのX(説明変数)に対するY(目的変数)の期待値E[Y]を求めます。これに対して分位点回帰(quantile regression)では、Xに対するYの分布における分位点を通るような直線を引きます。 . 分位点(または分位数)についてですが、簡単なのが四分位数です。箱ひげ図などで出てきますが、例えば第一四分位数は分布を25:75に分ける数、第二四分位数(中央値)は分布を50:50に分ける数です。同様に$q$分位数($q$-quantile)というと分布を$q:1-q$に分ける数となっています。 . さて、分位点回帰に戻りましょう。下図は$x sim U(0, 5), quad y=3x+x cdot xi, quad xi sim N(0,1)$とした500個の点に対する分位点回帰です(コードは図の下にあります)。青い領域はX=1,2,3,4でのYの分布を示しています。紫、緑、黄色の直線はそれぞれ10, 50, 90%tile回帰の結果です。例えば50%tile回帰の結果は、Xが与えられたときのYの中央値(50%tile点)を通るような直線となっています。同様に90%tile回帰の結果は90%tile点を通るような直線となっています。 . #collapse-hide import numpy as np np.random.seed(0) from matplotlib import pyplot as plt from tqdm import tqdm def QuantileGradientDescent(X, y, init_theta, tau, lr=1e-4, num_iters=10000): theta = init_theta for i in range(num_iters): y_hat = X @ theta # predictions delta = y - y_hat # error indic = np.array(delta &lt;= 0., dtype=np.float32) # indicator grad = np.abs(tau - indic) * np.sign(delta) # gradient theta += lr * X.T @ grad # Update return theta def gaussian_func(x, mu, sigma): return (0.8/sigma)*np.exp( - (x - mu)**2 / (2 * sigma**2)) cmap = plt.cm.viridis(np.linspace(0., 1., 3)) # Generate Toy datas N = 500 # sample size x = np.random.rand(N)*5 y = 3*x + x*np.random.randn(N) X = np.ones((N, 2)) # design matrix X[:, 1] = x taus = np.array([0.1, 0.5, 0.9]) m = len(taus) Y = np.zeros((m, N)) # memory array for i in tqdm(range(m)): init_theta = np.zeros(2) # init variables theta = QuantileGradientDescent(X, y, init_theta, tau=taus[i]) y_hat = X @ theta Y[i] = y_hat # Results plot plt.figure(figsize=(5,4)) plt.title(&quot;Quantile Regression&quot;) plt.scatter(x, y, color=&quot;gray&quot;, s=5) # samples for i in range(m): plt.plot([min(x), max(x)], [min(Y[i]), max(Y[i])], linewidth=2, color=cmap[i], label=str(int(taus[i]*100))+&quot;%tile&quot;) # regression line for loc in range(1,5): noise_y = np.arange(0, 6*loc, 1e-3) noise_x = loc + gaussian_func(noise_y, 3*loc, loc) plt.fill_between(noise_x, -1, noise_y, color=&#39;#539ecd&#39;, linewidth=2, alpha=0.5) plt.plot(noise_x, noise_y, color=&#39;#539ecd&#39;, linewidth=2) plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;y&quot;) plt.ylim(0, 25) plt.legend() plt.tight_layout() plt.show() . . 100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00&lt;00:00, 7.67it/s] . 分位点回帰の利点としては、外れ値に対して堅牢(ロバスト)である、Yの分布が非対称である場合にも適応できる、などがあります (Das et al., Nat Methods. 2019)。 . それでは分位点回帰をPythonで行う方法を見ていきましょう。 . &#21246;&#37197;&#27861;&#12395;&#12424;&#12427;&#32218;&#24418;&#22238;&#24112; (&#26368;&#23567;&#20108;&#20055;&#27861;) . 最小二乗法による線形回帰と異なり、分位点回帰は解析的に求めることができません。そのため、数値的に勾配法で求めるのですが、一先ずは最小二乗法による回帰直線を勾配法で求めてみましょう。 . 簡単のために単回帰の場合を考えます。パラメータを$ theta in mathbb{R}^2$, サンプルサイズを$n$, 説明変数の計画行列を$n times 2$の行列$X$, 目的変数を$y in mathbb{R}^n$とします。ただし$X$と$y$は観測値です。$y$の予測値は$X theta$なので、誤差 $ delta in mathbb{R}^n$は $$ delta = y-X theta$$ と表せます (符号は逆のことが多いですが)。最小二乗法において最適化したい目的関数は$$L( delta)= sum_{i=1}^n delta_i^2 = | delta |^2= delta^T delta$$ であり、$$ frac{ partial L}{ partial theta}=- frac{1}{n} delta X$$ と表せるので、$ theta$の更新式は$ theta leftarrow theta + alpha cdot dfrac{1}{n} delta X$と書けます ($ alpha$は学習率です)。 . さて、これを実装したコードと結果は次のようになります。 . #collapse-hide # Ordinary least squares regression def OLSRegGradientDescent(X, y, init_theta, lr=1e-4, num_iters=10000): theta = init_theta for i in range(num_iters): y_hat = X @ theta # predictions delta = y - y_hat # error theta += lr * delta @ X # Update return theta # Generate Toy datas N = 500 # sample size x = np.linspace(0, 10, N) y = 3*x + x*np.random.randn(N) X = np.ones((N, 2)) # design matrix X[:, 1] = x # Gradient descent init_theta = np.zeros(2) # init variables lr = 1e-4 # learning rate num_iters = 1000 # training iterations theta = OLSRegGradientDescent(X, y, init_theta) y_hat = X @ theta # predictions # Results plot plt.figure(figsize=(5,4)) plt.title(&quot;Least Squares Regression&quot;) plt.scatter(x, y, color=&quot;gray&quot;, s=5) # samples plt.plot([min(x), max(x)], [min(y_hat), max(y_hat)], color=&#39;red&#39;) # regression line plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;y&quot;) plt.tight_layout() plt.show() . . 上図において赤色の線が回帰直線です。今回は誤差を正規分布としているので綺麗な結果ですが、実際には外れ値の影響を受けたりします。 . &#21246;&#37197;&#27861;&#12395;&#12424;&#12427;&#20998;&#20301;&#28857;&#22238;&#24112; . 本題の分位点回帰です。前節と同様の設定とします。ここで $ delta$の関数を $$ rho_{ tau}( delta)= left| tau- mathbb{I}_{ delta leq 0} right| cdot | delta|= left( tau- mathbb{I}_{ delta leq 0} right) cdot delta$$ とします。ただし、$ tau$は関心のある分位点(quantile)、$ mathbb{I}$は指示関数(indicator function)です。この場合、$ mathbb{I}_{ delta leq 0}$は$ delta gt 0$なら0, $ delta leq 0$なら1となります。このとき、分位点回帰の目的関数は $$L_{ tau}( delta) = sum_{i=1}^n rho_{ tau}( delta_i)$$ です。なぜこの目的関数の最適化が$ tau$-分位点の回帰となるかについてはQuantile regressionのWikipediaに詳細に書いてあります。また、$ rho_{ tau}( delta)$を色々な $ tau$についてplotすると次図のようになります。 . #collapse-hide delta = np.arange(-5, 5, 0.1) tau= np.arange(0.2, 1.0, 0.2) cmap = plt.cm.brg(np.linspace(0, 0.5, len(tau))) plt.figure(figsize=(4,3)) for i in range(len(tau)): indic = delta &lt;= 0 y = (tau[i]-indic)*delta plt.plot(delta, y, label=r&quot;$ tau=$&quot;+&quot;{0:.1f}&quot;.format(tau[i]), color=cmap[i]) plt.xlabel(&quot;Error&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend() plt.tight_layout() plt.show() . . それでは$L_ tau$を最小化するような$ theta$の更新式について考えていきましょう。まず、$$ frac{ partial rho_{ tau}( delta)}{ partial delta}= rho_{ tau}^{ prime}( delta)= left| tau- mathbb{I}_{ delta leq 0} right| cdot operatorname{sign}( delta)$$ です (ただしsignは符号関数)。さらに$$ frac{ partial L_{ tau}}{ partial theta}= frac{ partial L_{ tau}}{ partial delta} frac{ partial delta( theta)}{ partial theta}=- frac{1}{n} rho_{ tau}^{ prime}( delta) X$$ が成り立つので、$ theta$の更新式は$ theta leftarrow theta + alpha cdot dfrac{1}{n} rho_{ tau}^{ prime}( delta) X$と書けます ($ alpha$は学習率です)。ゆえに実装には前節のコードを少し修正すればよいです。 . #collapse-hide def QuantileRegGradientDescent(X, y, init_theta, tau, lr=1e-4, num_iters=10000): theta = init_theta for i in range(num_iters): y_hat = X @ theta # predictions delta = y - y_hat # error indic = np.array(delta &lt;= 0., dtype=np.float32) # indicator grad = np.abs(tau - indic) * np.sign(delta) # gradient theta += lr * grad @ X # Update return theta cmap = plt.cm.viridis(np.linspace(0., 1., 5)) # Generate Toy datas N = 500 # sample size x = np.random.rand(N)*5 y = 3*x + x*np.random.randn(N) X = np.ones((N, 2)) # design matrix X[:, 1] = x taus = np.array([0.01, 0.1, 0.5, 0.9, 0.99]) m = len(taus) Y = np.zeros((m, N)) # memory array for i in tqdm(range(m)): init_theta = np.zeros(2) # init variables theta = QuantileRegGradientDescent(X, y, init_theta, tau=taus[i]) y_hat = X @ theta # prediction Y[i] = y_hat # memory # Results plot plt.figure(figsize=(5,4)) plt.title(&quot;Quantile Regression&quot;) plt.scatter(x, y, color=&quot;gray&quot;, s=5) # samples for i in range(m): plt.plot([min(x), max(x)], [min(Y[i]), max(Y[i])], linewidth=2, color=cmap[i], label=r&quot;$ tau=$&quot;+str(taus[i])) # regression line plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;y&quot;) plt.legend() plt.tight_layout() plt.show() . . 100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00&lt;00:00, 7.95it/s] . ただし、分位点回帰を単純な勾配法で求める場合、勾配が0となって解が求まらない可能性があるので避けた方が良いという話はあります。そのために、目的関数を滑らかにするという研究もあります (Zheng. IJMLC. 2011)。 . Statsmodels&#12395;&#12424;&#12427;&#20998;&#20301;&#28857;&#22238;&#24112; . 前節のように分位点回帰は実装できますが、より簡単かつ高速に行うにはライブラリを用いるのがよいです。Statsmodelには分位点回帰のモデルがあり、それを最適化するだけで回帰直線が得られます。 . #collapse-hide import statsmodels.api as sm from statsmodels.regression.quantile_regression import QuantReg cmap = plt.cm.viridis(np.linspace(0., 1., 5)) # Generate Toy datas N = 500 # sample size x = np.random.rand(N)*5 y = 3*x + x*np.random.randn(N) X = sm.add_constant(x) model = QuantReg(y, X) taus = np.array([0.01, 0.1, 0.5, 0.9, 0.99]) m = len(taus) Y = np.zeros((m, N)) # memory array for i in range(m): results = model.fit(q=taus[i]) y_hat = X @ results.params Y[i] = y_hat plt.figure(figsize=(5,4)) plt.scatter(x, y, color=&quot;gray&quot;, s=5) # samples for i in range(m): plt.plot([min(x), max(x)], [min(Y[i]), max(Y[i])], linewidth=2, color=cmap[i], label=r&quot;$ tau=$&quot;+str(taus[i])) # regression line plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;y&quot;) plt.legend() plt.tight_layout() plt.show() . . &#21442;&#32771;&#25991;&#29486; . https://en.wikipedia.org/wiki/Quantile_regression | Das, K., Krzywinski, M. &amp; Altman, N. Quantile regression. Nat Methods 16, 451–452 (2019) doi:10.1038/s41592-019-0406-y | Quantile and Expectile Regressions (pdf) | .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/statistics/2020/01/21/quantile_regression.html",
            "relUrl": "/statistics/2020/01/21/quantile_regression.html",
            "date": " • Jan 21, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Distributional Reinforcement Learningの仕組み",
            "content": "この記事は(Dabney, et al., Nature. 2020)におけるDistributional Reinforcement Learningを実装しながら理解しようという内容です。解説とか言うのは恐れ多いので自分用のメモだと思ってください…。また、どちらかというと神経科学寄りの内容です(深層強化学習への応用については触れません)。 . この研究はDeepMindとHarvardの内田先生のラボの共同研究で、アニメーション付きの解説記事をDeepMindが書いています (DeepMindのブログ)。Botvinick氏と内田先生の講演をCiNetで聞いたにも関わらず理解が疎かだったのですが、論文が公開されたので、ちゃんと理解しておこうという次第です。また、コード(MATLAB, Python)も公開されており(https://doi.org/10.17605/OSF.IO/UX5RG) 、この記事ではこのコードをかなり参考にしています。 . Classical TD learning vs Distributional TD learning . Classical TD learning . TD (Temporal difference) learningにおいて、報酬予測誤差(reward prediction error, RPE) $ delta_{i}$は次のように計算されます (この式はDistributional TD learningでも共通です)。 $$ delta_{i}=r+ gamma V_{j} left(x^{ prime} right)-V_{i}(x) $$ ただし、現在の状態を$x$, 次の状態を$x&#39;$, 予測価値分布を$V(x)$, 報酬信号を$r$, 時間割引率(time discount)を$ gamma$としました。 また、$V_{j} left(x^{ prime} right)$は予測価値分布$V left(x^{ prime} right)$からのサンプルです。 このRPEは脳内において主に中脳のVTA(腹側被蓋野)やSNc(黒質緻密部)におけるドパミン(dopamine)ニューロンの発火率として表現されています。 . ただし、VTAとSNcのドパミンニューロンの役割は同一ではありません。ドパミンニューロンへの入力が異なっています (Watabe-Uchida et al., Neuron. 2012)00281-4)。 また、細かいですがドパミンニューロンの発火は報酬量に対して線形ではなく、やや飽和する非線形な応答関数 (Hill functionで近似可能)を持ちます(Eshel et al., Nat. Neurosci. 2016)。このため著者実装では報酬 $r$に非線形関数がかかっているものもあります。 . 先ほどRPEはドパミンニューロンの発火率で表現されている、といいました。RPEが正の場合はドパミンニューロンの発火で表現できますが、単純に考えると負の発火率というものはないため、負のRPEは表現できないように思います。ではどうしているかというと、RPEが0（予想通りの報酬が得られた場合）でもドパミンニューロンは発火しており、RPEが正の場合にはベースラインよりも発火率が上がるようになっています。逆にRPEが負の場合にはベースラインよりも発火率が減少する(抑制される)ようになっています (Schultz et al., Science. 1997; Chang et al., Nat Neurosci. 2016)。発火率というのを言い換えればISI (inter-spike interval, 発火間隔)の長さによってPREが符号化されている(ISIが短いと正のRPE, ISIが長いと負のRPEを表現)ともいえます (Bayer et al., J. Neurophysiol. 2007)。 . 予測価値(分布) $V(x)$ですが、これは線条体(striatum)のパッチ (SNcに抑制性の投射をする)やVTAのGABAニューロン (VTAのドパミンニューロンに投射して減算抑制をする, (Eshel, et al., Nature. 2015))などにおいて表現されています。 この予測価値は通常のTD learningでは次式により更新されます。 $$ V_{i}(x) leftarrow V_{i}(x)+ alpha_{i} f left( delta_{i} right) $$ ただし、$ alpha_{i}$は学習率(learning rate), $f( cdot)$はRPEに対する応答関数です。生理学的には$f( delta)= delta$を使うのが妥当ですが、後の分位数(quantile)モデルでは$f( delta)= text{sign}( delta)$を用います。 . Distributional TD learning . Distributional TD learningではRPEの正負に応じて、予測報酬の更新を異なる学習率($ alpha_{i}^{+}, alpha_{i}^{-}$)を用いて行います。 $$ begin{cases} V_{i}(x) leftarrow V_{i}(x)+ alpha_{i}^{+} f left( delta_{i} right) &amp; text{for } delta_{i} gt 0 V_{i}(x) leftarrow V_{i}(x)+ alpha_{i}^{-} f left( delta_{i} right) &amp; text{for } delta_{i} leq 0 end{cases} $$ ここで、シミュレーションにおいては$ alpha_{i}^{+}, alpha_{i}^{-} sim U(0, 1)$とします($U$は一様分布)。さらにasymmetric scaling factor $ tau_i$を次式により定義します。 $$ tau_i= frac{ alpha_{i}^{+}}{ alpha_{i}^{+}+ alpha_{i}^{-}} $$ なお、$ alpha_{i}^{+}, alpha_{i}^{-} in [0, 1]$より$ tau_i in [0,1]$です。 . Classical TD learningとDistributional TD learningにおける各ニューロンのRPEに対する発火率を表現したのが次図となります。 . #collapse-hide import numpy as np from matplotlib import pyplot as plt # Classical TD learning N = 10 cmap = plt.cm.brg(np.linspace(0, 0.5, N)) x = np.arange(-1, 1, 1e-2)[:, None] theta = np.linspace(np.pi/6, np.pi/3, N) alpha = np.tan(theta) y = alpha * x # Plot plt.figure(figsize=(8, 4)) def hide_ticks(): #上と右の軸を表示しないための関数 plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().yaxis.set_ticks_position(&#39;left&#39;) plt.gca().xaxis.set_ticks_position(&#39;bottom&#39;) plt.subplot(1,2,1) plt.axvline(x=0, color=&quot;gray&quot;, linestyle=&quot;dashed&quot;, linewidth=2) plt.axhline(y=0, color=&quot;gray&quot;, linestyle=&quot;dashed&quot;, linewidth=2) for i in range(N): if i == N//2: plt.plot(x, y[:, i], color=cmap[N//2], alpha=1, linewidth=3, label=&quot;Neutral&quot;) else: plt.plot(x, y[:, i], color=cmap[N//2], alpha=0.2) hide_ticks() plt.ylim(-1,1); plt.xlim(-1,1) plt.xticks([]); plt.yticks([]) plt.legend(loc=&#39;upper left&#39;) plt.title(&quot;Classical TD learning&quot;) plt.xlabel(&quot;RPE&quot;) plt.ylabel(&quot;Firing&quot;) # Distributional TD learning N = 20 cmap = plt.cm.brg(np.linspace(0, 0.5, N)) x = np.arange(-1, 1, 1e-2)[:, None] theta = np.linspace(np.pi/16, np.pi*7/16, N) alpha_pos = np.tan(theta) alpha_neg = np.tan(theta)[::-1] y = (alpha_pos*(x&gt;0) + (alpha_neg)*(x&lt;=0))*x # Plot ax = plt.subplot(1,2,2) plt.axvline(x=0, color=&quot;gray&quot;, linestyle=&quot;dashed&quot;, linewidth=2) plt.axhline(y=0, color=&quot;gray&quot;, linestyle=&quot;dashed&quot;, linewidth=2) for i in range(N): if i == 0: plt.plot(x, y[:, i], color=cmap[i], alpha=1, linewidth=3, label=&quot;Pessimistic&quot;) elif i == N//2: plt.plot(x, y[:, i], color=cmap[i], alpha=1, linewidth=3, label=&quot;Neutral&quot;) elif i == N-1: plt.plot(x, y[:, i], color=cmap[i], alpha=1, linewidth=3, label=&quot;Optimistic&quot;) else: plt.plot(x, y[:, i], color=cmap[i], alpha=0.2) hide_ticks() handles, labels = ax.get_legend_handles_labels() ax.legend(reversed(handles), reversed(labels), loc=&#39;upper left&#39;) plt.ylim(-1,1); plt.xlim(-1,1) plt.xticks([]); plt.yticks([]) plt.title(&quot;Distributional TD learning&quot;) plt.xlabel(&quot;RPE&quot;) plt.ylabel(&quot;Firing&quot;) plt.show() . . Classical TD learningではRPEに比例して発火する細胞しかありませんが、Distributional TD learningではRPEの正負に応じて発火率応答が変化していることがわかります。 特に$ alpha_{i}^{+} gt alpha_{i}^{-}$の細胞を楽観的細胞 (optimistic cells)、$ alpha_{i}^{+} lt alpha_{i}^{-}$の細胞を悲観的細胞 (pessimistic cells)と著者らは呼んでいます。実際には2群に分かれているわけではなく、gradientに遷移しています。楽観的・悲観的の意味に関しては後でも触れますが、ここではイメージだけお伝えしておきます。まず楽観的細胞ではRPEが正なら「結構もらえるやん」、RPEが負なら「まあそういうときもあるよね」となり最終的な予測価値は通常よりも高くなります。逆に悲観的細胞ではRPEが正なら「もらえたけどいつもそうではないやろ」、RPEが負なら「やっぱあんまもらえんよな」となり最終的な予測価値は通常よりも低くなります。収束する予測価値が細胞ごとに異なることで、$V$には報酬の期待値ではなく複雑な形状の報酬分布が符号化されます。その仕組みについて、次節から見ていきます。 . &#20998;&#20301;&#25968;(Quantile)&#12514;&#12487;&#12523;&#12392;&#22577;&#37228;&#20998;&#24067;&#12398;&#31526;&#21495;&#21270; . RPE&#12395;&#23550;&#12377;&#12427;&#24540;&#31572;&#12364;sign&#38306;&#25968;&#12398;&#12514;&#12487;&#12523;&#12392;&#22577;&#37228;&#20998;&#24067;&#12398;&#20998;&#20301;&#28857;&#12408;&#12398;&#20104;&#28204;&#20385;&#20516;&#12398;&#21454;&#26463; . さて、Distributional RLモデルでどのようにして報酬分布が学習されるかについてみていきます。この節ではRPEに対する応答関数$f( cdot)$が符合関数(sign function)の場合を考えます。結論から言うと、この場合はasymmetric scaling factor $ tau_i$は分位数(quantile)となり、予測価値 $V_i$は報酬分布の$ tau_i$分位数に収束します。 . どういうことかを簡単なシミュレーションで見てみましょう。今、報酬分布を平均2, 標準偏差5の正規分布とします (すなわち$r sim N(2, 5^2)$となります)。また、$ tau_i = 0.25, 0.5, 0.75 (i=1,2,3)$とします。このとき、3つの予測価値 $V_i (i=1,2,3)$はそれぞれ$N(2, 5^2)$の0.25, 0.5, 0.75分位数に収束します。下図はシミュレーションの結果です。左が$V_i$の変化で、右が報酬分布と0.25, 0.5, 0.75分位数の位置 (黒短線)となっています。対応する分位数に見事に収束していることが分かります。 . #collapse-hide import seaborn as sns from tqdm import tqdm from matplotlib import gridspec ############ ### init ### ############ response_func = lambda r: np.sign(r) # RPEの応答関数 num_cells = 3 # ニューロン(ユニット)の数 num_steps = 5000 # 訓練回数 base_lrate = 0.02 # ベースラインの学習率 reward_mu = 5 # 報酬の平均(正規分布) reward_sigma = 2 # 報酬の標準偏差(正規分布) distribution = np.zeros(num_cells) # 価値分布を記録する配列 dist_trans = np.zeros((num_steps, num_cells)) # 価値分布を記録する配列 alpha_pos = np.array([.1, .2, .3]) # RPEが正のときの学習率 alpha_neg = np.array([.3, .2, .1]) # RPEが負のときの学習率 tau = alpha_pos / (alpha_pos + alpha_neg) # Asymmetric scaling factor ############## # simulation # ############## for step in tqdm(range(num_steps)): # 25000 steps # 報酬がrandomに選ばれる reward = np.random.normal(reward_mu, reward_sigma, size=(1,)) # 報酬誤差(step毎に更新) reward応答をlinearとする delta = reward - distribution # (3, ) # deltaが負なら1, 正なら0 valence = np.array(delta &lt;= 0., dtype=np.float32) # (3, ) # 予測価値分布の更新 alpha = valence * alpha_neg + (1. - valence) * alpha_pos distribution += alpha * response_func(delta) * base_lrate dist_trans[step] = distribution # 予測価値分布変化の記録 ################ # Results plot # ################ steps = np.arange(num_steps) ylim = (0, 10) # y軸のlim gs = gridspec.GridSpec(1, 2, width_ratios=[1, 0.25]) plt.figure(figsize=(6,4)) plt.subplot(gs[0]) # 予測価値の変化 for i in range(num_cells): plt.plot(steps, dist_trans[:, i], label=str((i+1)*25)+&quot;%tile (&quot;+r&quot;$ tau=$&quot;+str((i+1)*0.25)+&quot;)&quot;) plt.title(&quot;Convergence of value prediction to n percentile of reward distribution&quot;) plt.xlim(0, num_steps) plt.ylim(ylim) plt.xlabel(&quot;Learning steps&quot;) plt.ylabel(&quot;Learned Value&quot;) plt.legend() # 報酬のサンプリング rewards = np.random.normal(reward_mu, reward_sigma, size=(1000,)) percentile = np.percentile(rewards, q=[25, 50, 75]) # 報酬の四分位数を取得 plt.subplot(gs[1]) # 報酬分布とその分位数 sns.kdeplot(rewards, bw=1, shade=True, vertical=True) sns.rugplot(percentile, color=&#39;k&#39;, lw=2, height=0.2, vertical=True) plt.title(&quot;Reward n distribution&quot;) plt.ylim(ylim) plt.xlabel(&quot;Density&quot;) plt.tight_layout() plt.show() . . 100%|███████████████████████████████████████████████████████████████████████████| 5000/5000 [00:00&lt;00:00, 79574.72it/s] . ここでoptimisticな細胞($ tau=0.75$)は中央値よりも高い予測価値、pessimisticな細胞($ tau=0.25$)は中央値よりも低い予測価値に収束しています。 つまり細胞の楽観度というものは、細胞が期待する報酬が大きいほど上がります。 . 同様のシミュレーションを今度は200個の細胞 (ユニット)で行います。報酬は0.1, 1, 2 μLのジュースがそれぞれ確率0.3, 0.6, 0.1で出るとします (Extended Data Fig.1と同じような分布にしています)。なお、著者らはシミュレーションとマウスに対してVariable-magnitude task (異なる量の報酬(ジュース)が異なる確率で出る)とVariable-probability task (一定量の報酬がある確率で出る)を行っています。以下はVariable-magnitude taskを行う、ということです。学習結果は次図のようになります。左はGround Truthの報酬分布で、右は$V_i$に対してカーネル密度推定 (KDE)することによって得た予測価値分布です。2つの分布はほぼ一致していることが分かります。 . #collapse-hide response_func = lambda r: np.sign(r) # RPEの応答関数 juice_amounts = np.array([0.1, 1, 2]) # reward(ジュース)の量(uL) juice_probs = np.array([0.3, 0.6, 0.1]) # 各ジュースが出る確率 num_cells = 200 # ニューロン(ユニット)の数 num_steps = 25000 # 訓練回数 base_lrate = 0.02 # ベースラインの学習率 distribution = np.zeros(num_cells) # 価値分布を記録する配列 alpha_pos = np.random.random(size=(num_cells)) # RPEが正のときの学習率 alpha_neg = np.random.random(size=(num_cells)) # RPEが負のときの学習率 tau = alpha_pos / (alpha_pos + alpha_neg) # Asymmetric scaling factor ############## # simulation # ############## for step in tqdm(range(num_steps)): # 25000 steps # 報酬がrandomに選ばれる reward = (np.random.choice(juice_amounts, p=juice_probs)) #(1, ) # 報酬誤差(step毎に更新) reward応答をlinearとする delta = reward - distribution # (200, ) # deltaが負なら1, 正なら0 valence = np.array(delta &lt;= 0., dtype=np.float32) # (200, ) # 予測価値分布の更新 alpha = valence * alpha_neg + (1. - valence) * alpha_pos distribution += alpha* response_func(delta) * base_lrate # tauの大きさでソートする ind = np.argsort(tau) tau = tau[ind] alpha_pos = alpha_pos[ind] alpha_neg = alpha_neg[ind] distribution = distribution[ind] ################ # Results plot # ################ # 報酬をサンプリング rewards = (np.random.choice(juice_amounts,size=1000, p=juice_probs)) # 結果の描画(価値・報酬分布) plt.figure(figsize=(8,4)) plt.subplot(1,2,1) # Ground Truth (Reward分布) plt.title(&quot;Reward distribution&quot;) sns.rugplot(rewards, color=&#39;k&#39;, lw=2, zorder=10) sns.kdeplot(rewards, bw=.15, color=&#39;k&#39;, lw=1., shade=True) plt.xlabel(&quot;Reward&quot;) plt.ylabel(&quot;Density&quot;) plt.subplot(1,2,2) # 学習後のValue(Reward)の分布 plt.title(&quot;Learned Value distribution&quot;) sns.kdeplot(distribution, bw=.15, color=&#39;k&#39;, lw=1., shade=True) sns.rugplot(distribution, color=&#39;k&#39;, lw=2, zorder=10) plt.xlabel(&quot;Value&quot;) plt.ylabel(&quot;Density&quot;) plt.tight_layout() plt.show() . . 100%|█████████████████████████████████████████████████████████████████████████| 25000/25000 [00:00&lt;00:00, 31986.89it/s] . そして$V_i$の経験累積分布関数(CDF)は$r$のサンプリングしたCDFとほぼ同一となっています (下図左)。また、$ tau_i$の関数である$V_i$は分位点関数 (quantile function)または累積分布関数の逆関数 (inverse cumulative distribution function)となっています (下図右)。右の図を転置すると左の青い曲線とだいたい一致しそうなことが分かります。 . #collapse-hide # 結果の描画(累積分布) plt.figure(figsize=(8,4)) plt.subplot(1,2,1) # 累積分布 sns.kdeplot(distribution, cumulative=True,bw=.05, label=&quot;Learned Value&quot;) sns.kdeplot(rewards, cumulative=True, bw=.05, label=&quot;Reward (GT)&quot;) plt.xlabel(&quot;Reward (Learned Value)&quot;) plt.ylabel(&quot;Cumulative probability&quot;) plt.subplot(1,2,2) # 累積分布 plt.plot(tau, distribution) plt.xlabel(&quot;Asymmetric scaling factors (&quot;+ r&quot;$ tau$)&quot;) plt.ylabel(&quot;Learned Value&quot;) plt.tight_layout() plt.show() . . sign&#38306;&#25968;&#12434;&#29992;&#12356;&#12383;Distributional RL&#12392;&#20998;&#20301;&#28857;&#22238;&#24112; . それでは、なぜ予測価値 $V_i$は$ tau_i$ 分位点に収束するのでしょうか。Extended Data Fig.1のように平衡点で考えてもよいのですが、後のために分位点回帰との関連について説明します。分位点回帰については記事を書いたので先にそちらを読んでもらうと分かりやすいと思います (→Pythonによる分位点回帰 (Quantile regression))。 . 実はDistributional RL (かつ、RPEの応答関数にsign関数を用いた場合)における予測報酬 $V_i$の更新式は、分位点回帰(Quantile regression)を勾配法で行うときの更新式とほとんど同じです。分位点回帰では$ delta$の関数$ rho_{ tau}( delta)$を次のように定義します。 $$ rho_{ tau}( delta)= left| tau- mathbb{I}_{ delta leq 0} right| cdot | delta|= left( tau- mathbb{I}_{ delta leq 0} right) cdot delta $$ そして、この関数を最小化することで回帰を行います。ここで$ tau$は分位点です。また$ delta=r-V$としておきます。今回、どんな行動をしても未来の報酬に影響はないので$ gamma=0$としています。 ここで、 $$ frac{ partial rho_{ tau}( delta)}{ partial delta}= rho_{ tau}^{ prime}( delta)= left| tau- mathbb{I}_{ delta leq 0} right| cdot operatorname{sign}( delta) $$ なので、$r$を観測値とすると、 $$ frac{ partial rho_{ tau}( delta)}{ partial V}= frac{ partial rho_{ tau}( delta)}{ partial delta} frac{ partial delta(V)}{ partial V}=- left| tau- mathbb{I}_{ delta leq 0} right| cdot operatorname{sign}( delta) $$ となります。ゆえに$V$の更新式は $$ V leftarrow V - beta cdot frac{ partial rho_{ tau}( delta)}{ partial V}=V+ beta left| tau- mathbb{I}_{ delta leq 0} right| cdot operatorname{sign}( delta) $$ です。ただし、$ beta$はベースラインの学習率です。個々の$V_i$について考え、符号で場合分けをすると $$ begin{cases} V_{i} leftarrow V_{i}+ beta cdot | tau_i| cdot operatorname{sign} left( delta_{i} right) &amp; text { for } delta_{i}&gt;0 V_{i} leftarrow V_{i}+ beta cdot | tau_i-1| cdot operatorname{sign} left( delta_{i} right) &amp; text { for } delta_{i} leq 0 end{cases} $$ となります。$0 leq tau_i leq 1$であり、$ tau_i= alpha_{i}^{+} / left( alpha_{i}^{+} + alpha_{i}^{-} right)$であることに注意すると上式は次のように書けます。 $$ begin{cases} V_{i} leftarrow V_{i}+ beta cdot frac{ alpha_{i}^{+}}{ alpha_{i}^{+}+ alpha_{i}^{-}} cdot operatorname{sign} left( delta_{i} right) &amp; text { for } delta_{i}&gt;0 V_{i} leftarrow V_{i}+ beta cdot frac{ alpha_{i}^{-}}{ alpha_{i}^{+}+ alpha_{i}^{-}} cdot operatorname{sign} left( delta_{i} right) &amp; text { for } delta_{i} leq 0 end{cases} $$ これは前節で述べたDistributional RLの更新式とほぼ同じです。いくつか違う点もありますが、RPEが正の場合と負の場合に更新される値の比は同じとなっています。 . このようにRPEの応答関数にsign関数を用いた場合、報酬分布を上手く符号化することができます。しかし実際のドパミンニューロンはsign関数のような生理的に妥当でない応答はせず、RPEの大きさに応じた活動をします。そこで次節ではRPEの応答関数を線形にしたときの話をします。 . Expectile &#12514;&#12487;&#12523;&#12392;&#12489;&#12497;&#12511;&#12531;&#12491;&#12517;&#12540;&#12525;&#12531;&#12363;&#12425;&#12398;&#22577;&#37228;&#20998;&#24067;&#12398;Decoding . RPE&#12395;&#23550;&#12377;&#12427;&#24540;&#31572;&#12364;&#32218;&#24418;&#12394;&#12514;&#12487;&#12523;&#12392;Expectile&#22238;&#24112; . 節の最後で述べたようにドパミンニューロンの活動はsign関数ではなく線形な応答をする、とした方が生理学的に妥当です (発火率を表現するならば$f( delta)=c+ delta quad(c &gt; 0)$とした方が良いのでしょうが)。それでは予測価値の更新式を $$ begin{cases} V_{i}(x) leftarrow V_{i}(x)+ alpha_{i}^{+} delta_{i} &amp; text{for } delta_{i} gt 0 V_{i}(x) leftarrow V_{i}(x)+ alpha_{i}^{-} delta_{i} &amp; text{for } delta_{i} leq 0 end{cases} $$ とした場合は、分位点回帰ではなく何に対応するのでしょうか。結論から言えば、この場合はエクスペクタイル回帰(Expectile regression)と同じになります。そもそも、expectileというのは聞きなれないですが、expectileという用語自体はexpectationとquantileを合わせたような概念、というところから来ています。中央値(median)に対する分位数(quantile)が、平均(mean)あるいは期待値(expectation)に対するexpectileの関係と同じであると捉えると良いです。 もう少し言えば、前者は誤差のL1ノルム, 後者はL2ノルムの損失関数を最小化することにより得られます (cf. Quantile and Expectile Regressions)。 . 分位点回帰で用いた損失関数は$$ rho_{ tau}( delta)= left| tau- mathbb{I}_{ delta leq 0} right| cdot | delta|$$でしたが、最後の$| delta|$を$ delta^2$として、 $$ rho^E_{ tau}( delta)= left| tau- mathbb{I}_{ delta leq 0} right| cdot delta^2$$ とします。これを微分すれば $$ frac{ partial rho^E_{ tau}( delta)}{ partial delta}= rho_{ tau}^{E prime}( delta)=2 cdot left| tau- mathbb{I}_{ delta leq 0} right| cdot delta $$ となり、上記の予測価値の更新式がExpectile回帰の損失関数から導けることが分かります。 . &#22577;&#37228;&#20998;&#24067;&#12398;&#12487;&#12467;&#12540;&#12487;&#12451;&#12531;&#12464; (decoding) . それで、RPEの応答を線形とした場合は報酬分布を上手く学習できるのかという話ですが、実はRPEの応答をsign関数とした場合と同じように学習後の予測価値の分布を求めても報酬分布は復元されません (簡単な修正で確認できます)。そこで報酬分布をデコーディングする方法を考えます。 . デコーデイングには各細胞が学習した予測価値(またはreversal points) $V_i$, asymmetries $ tau_i$, および報酬分布(ただし報酬の下限と上限からの一様分布)からのサンプル $z_m (m=1,2, cdots, M)$を用います。$N$を推定する$V_i$の数、$M=100$を1つの報酬サンプル集合$ {z_m }$内の要素数としたとき、次の損失関数を最小にする集合$ {z_m }$を求めます。 $$ mathcal{L}(z, V, tau)= frac{1}{M} sum_{m-1}^{M} sum_{n=1}^{N} left| tau_{n}- mathbb{I}_{z_{m} leq V_{n}} right| left(z_{m}-V_{n} right)^{2} $$ ここで、集合$ {z_m }$は20000回サンプリングするとします。損失関数$ mathcal{L}$を最小化する集合の分布が推定された報酬分布となっているので、それをplotします。以下はその結果とコードです (このコードはほとんど著者実装のままです)。灰色が元の報酬分布で、紫がデコーデイングされた分布です。完全とはいきませんが、ある程度は推定できていることが分かります。 . #collapse-hide import scipy.stats import scipy.optimize def expectile_loss_fn(expectiles, taus, samples): &quot;&quot;&quot;Expectile loss function, corresponds to distributional TD model &quot;&quot;&quot; # distributional TD model: delta_t = (r + gamma V*) - V_i # expectile loss: delta = sample - expectile delta = (samples[None, :] - expectiles[:, None]) # distributional TD model: alpha^+ delta if delta &gt; 0, alpha^- delta otherwise # expectile loss: |taus - I_{delta &lt;= 0}| * delta^2 # Note: When used to decode we take the gradient of this loss, # and then evaluate the mean-squared gradient. That is because *samples* must # trade-off errors with all expectiles to zero out the gradient of the # expectile loss. indic = np.array(delta &lt;= 0., dtype=np.float32) grad = -0.5 * np.abs(taus[:, None] - indic) * delta return np.mean(np.square(np.mean(grad, axis=-1))) def run_decoding(reversal_points, taus, minv=0., maxv=1., method=None, max_samples=1000, max_epochs=10, M=100): &quot;&quot;&quot;Run decoding given reversal points and asymmetries (taus).&quot;&quot;&quot; # sort ind = list(np.argsort(reversal_points)) points = reversal_points[ind] tau = taus[ind] # Robustified optimization to infer distribution # Generate max_epochs sets of samples, # each starting the optimization at the best of max_samples initial points. sampled_dist = [] for _ in range(max_epochs): # Randomly search for good initial conditions # This significantly improves the minima found samples = np.random.uniform(minv, maxv, size=(max_samples, M)) fvalues = np.array([expectile_loss_fn(points, tau, x0) for x0 in samples]) # Perform loss minimizing on expectile loss (w.r.t samples) x0 = np.array(sorted(samples[fvalues.argmin()])) fn_to_minimize = lambda x: expectile_loss_fn(points, tau, x) result = scipy.optimize.minimize( fn_to_minimize, method=method, bounds=[(minv, maxv) for _ in x0], x0=x0)[&#39;x&#39;] sampled_dist.extend(result.tolist()) return sampled_dist, expectile_loss_fn(points, tau, np.array(sampled_dist)) # reward distribution juice_amounts = np.array([0.1, 0.3, 1.2, 2.5, 5, 10, 20]) juice_empirical_probs = np.array( [0.06612594, 0.09090909, 0.14847358, 0.15489467, 0.31159175, 0.1509519 , 0.07705306]) # samples of reward (1000, ) sampled_empirical_dist = np.random.choice( juice_amounts, p=juice_empirical_probs, size=1000) n_trials = 10 # num of simulation trial n_epochs = 20000 # num of simulation epoch num_cells = 151 # num of cells or units n_decodings = 5 # num of decodings # Global scale for learning rates beta = 0.2 # Distributional TD simulation and decoding distribution = np.zeros((n_trials, num_cells)) alpha_pos = np.random.random((num_cells))*beta alpha_neg = np.random.random((num_cells))*beta # alpha_neg = beta - alpha_pos としてもよい # Simulation for trial in tqdm(range(n_trials)): for step in range(n_epochs): # Sample reward reward = np.random.choice(juice_amounts, p=juice_empirical_probs) # Compute TD error delta = reward - distribution[trial] # Update distributional value estimate valence = np.array(delta &lt;= 0., dtype=np.float32) alpha = valence * alpha_neg + (1. - valence) * alpha_pos distribution[trial] += alpha * delta # Decoding from distributional TD (DTD) simulation dtd_samples = [] # dtd_losses = [] # decoding loss taus = alpha_pos / (alpha_pos + alpha_neg) asym_variance = 0.2 for t in tqdm(range(n_decodings)): # Add noise to the scaling, but have mean 0.5 giving symmetric updates scaling_noise = np.tanh(np.random.normal(size=len(taus))) * asym_variance noisy_tau = np.clip(taus + scaling_noise, 0., 1.) # add noise # Run decoding for distributional TD values = run_decoding( distribution.mean(0), noisy_tau, minv=juice_amounts.min(), maxv=juice_amounts.max(), max_epochs=1, M=100, max_samples=20000, method=&#39;TNC&#39;) dtd_samples.append(values[0]) dtd_losses.append(values[1]) # print(t, values[1]) # results of decoding dtd_reward_decode = np.array(dtd_samples).flatten() # plot fig = plt.figure(figsize=(8, 5)) # Ground truth sns.kdeplot(sampled_empirical_dist, bw=.75, color=&#39;k&#39;, lw=0., shade=True) sns.rugplot(sampled_empirical_dist, color=&quot;red&quot;, lw=2, zorder=10, label=&quot;Empirical&quot;) # decoded distribution sns.kdeplot(dtd_reward_decode, bw=.75, color=plt.cm.plasma(0), lw=4., zorder=5, shade=False) sns.rugplot(dtd_reward_decode, color=plt.cm.plasma(0), label=&#39;Decoded&#39;) for draw in dtd_samples: sns.kdeplot(draw, bw=.5, color=plt.cm.plasma(0.), alpha=.5, lw=1., shade=False) plt.tick_params(top=False, right=False, labelsize=14) plt.legend(loc=&#39;best&#39;, fontsize=16) plt.xlabel(&quot;Reward&quot;, fontsize=16) plt.ylabel(&quot;Density&quot;, fontsize=16) plt.title(&quot;Distributional TD Decoding&quot;, fontsize=18) plt.tight_layout() plt.show() . . 100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:05&lt;00:00, 1.81it/s] 100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:33&lt;00:00, 6.69s/it] . このようにしてRPEに対する応答が線形であるとした場合でも報酬分布を推定できました。同じことを著者らはドパミンニューロンの活動に対しても行い、報酬分布がデコーデイングされることを示しています。ただ、デコーデイングの手間が結構かかっている気がするので、学習した予測価値分布を利用するときにはどのような処理をしているのかは気になります。 . &#21442;&#32771;&#25991;&#29486; . Dabney, W., Kurth-Nelson, Z., Uchida, N. et al. A distributional code for value in dopamine-based reinforcement learning. Nature (2020). https://doi.org/10.1038/s41586-019-1924-6 | Watabe-Uchida, M. et al. Whole-Brain Mapping of Direct Inputs to Midbrain Dopamine Neurons. Neuron 74, 5, 858 - 873 (2012). https://doi.org/10.1016/j.neuron.2012.03.01700281-4) 00281-4) | Eshel, N., Tian, J., Bukwich, M. et al. Dopamine neurons share common response function for reward prediction error. Nat Neurosci 19, 479–486 (2016). https://doi.org/10.1038/nn.4239 | Schultz, W., Dayan, P., Montague, P.R. A neural substrate of prediction and reward. Science. 275, 1593-9 (1997). doi:10.1126/science.275.5306.1593 | Chang, C., Esber, G., Marrero-Garcia, Y. et al. Brief optogenetic inhibition of dopamine neurons mimics endogenous negative reward prediction errors. Nat Neurosci 19, 111–116 (2016) doi:10.1038/nn.4191 | Bayer, H.M., Lau, B., Glimcher, P.W. Statistics of midbrain dopamine neuron spike trains in the awake primate. J Neurophysiol. 98(3):1428-39 (2007). https://doi.org/10.1152/jn.01140.2006 | Eshel, N., Bukwich, M., Rao, V. et al. Arithmetic and local circuitry underlying dopamine prediction errors. Nature 525, 243–246 (2015). https://doi.org/10.1038/nature14855 | .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/neuroscience/2020/01/20/drl.html",
            "relUrl": "/neuroscience/2020/01/20/drl.html",
            "date": " • Jan 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "NibabelとMayaviによるMR画像の可視化",
            "content": "PythonでNifti (The Neuroimaging Informatics Technology Initiative) 形式のMR画像の可視化をしてみる。まずNifti (nii.gz)形式のファイルはNibabelでloadする。Nibabelはpipで入る：pip install nibabel . なお、Nibabelに依存した機械学習ライブラリとしてNilearnというのもある。今回は使わない。 . Nibabel tutorial . NibabelのtutorialにはEPI法(echo planar imaging)で撮影された拡散強調像 (someones_epi.nii.gz) と、(恐らく)T1 強調像 (someones_anatomy.nii.gz) の2つのデータが用意されている。どちらでもよいが、someones_anatomy.nii.gzの方をダウンロードしておく。 . nibabelの関数(nib.load)でデータを読み込み、matplotlibで可視化する。 . import nibabel as nib import numpy as np import matplotlib.pyplot as plt from matplotlib import gridspec # Data load img = nib.load(&#39;someones_anatomy.nii.gz&#39;) img_data = img.get_fdata() print(img_data.shape) # (57, 67, 56) # Plot gs = gridspec.GridSpec(1, 3, width_ratios=[1.17, 1, 1.17]) plt.figure(figsize=(8,4)) plt.subplot(gs[0]) plt.title(&quot;Sagittal&quot;) plt.imshow(np.flipud(img_data[28].T), cmap=&quot;gray&quot;) plt.subplot(gs[1]) plt.title(&quot;Coronal&quot;) plt.imshow(np.flipud(img_data[:, 33].T), cmap=&quot;gray&quot;) plt.subplot(gs[2]) plt.title(&quot;Horizontal&quot;) plt.imshow(img_data[:, :, 28], cmap=&quot;gray&quot;) plt.tight_layout() plt.show() . . マーモセット脳MRIデータセット . マーモセット(marmoset)脳のMRIデータセット (Marmoset Brain Mapping)が公開された[1, 2]ので、それを読み込んで可視化してみる。 . まず、dataのページから150um dMRIの前処理後のデータ(DTI-Fitted)をダウンロードする。これを選んでいるのは単にデータサイズが小さいため。このデータはマーモセットから脳を取り出した後、ホルマリン固定後、gadoliniumに浸して造影し、7T MRIでスキャンすることで得られたものである。 . zipファイルを解凍後、DTIFIT_FA.nii.gzというデータ (添え字の詳細が無かった)を読み込んで可視化する。 . Nibabelによるマーモセット脳MRIの断面図の可視化 . コード自体は先ほどのtutorialのものとほぼ同じ。 . import nibabel as nib import numpy as np import matplotlib.pyplot as plt from matplotlib import gridspec # Data load img = nib.load(&#39;DTIFIT_FA.nii.gz&#39;) img_data = img.get_fdata() print(img_data.shape) # (192, 256, 192) # Plot gs = gridspec.GridSpec(1, 3, width_ratios=[1.33, 1, 1.33]) plt.figure(figsize=(10,4)) plt.subplot(gs[0]) plt.title(&quot;Sagittal&quot;) plt.imshow(np.flipud(img_data[96].T), cmap=&quot;gray_r&quot;) plt.subplot(gs[1]) plt.title(&quot;Coronal&quot;) plt.imshow(np.flipud(img_data[:, 128].T), cmap=&quot;gray_r&quot;) plt.subplot(gs[2]) plt.title(&quot;Horizontal&quot;) plt.imshow(img_data[:, :, 96], cmap=&quot;gray_r&quot;) plt.tight_layout() plt.show() . . Mayaviによるマーモセット脳の3Dでの可視化 . Pythonで3次元Plotをするのにはmatplotlibやplotlyなどもあるが、今回はMayaviを用いる。installはpip install mayaviでできるが、 . $ git clone https://github.com/enthought/mayavi.git $ cd mayavi $ pip install -r requirements.txt $ pip install PyQt5 # replace this with any supported toolkit $ python setup.py install # or develop . をすると確実。 . import nibabel as nib from mayavi import mlab img = nib.load(&#39;DTIFIT_FA.nii.gz&#39;) img_data = img.get_fdata() fig = mlab.figure(size=(400, 400), bgcolor = (1,1,1)) src = mlab.pipeline.scalar_field(img_data) mlab.pipeline.iso_surface(src, contours=[img_data.min()+0.05*img_data.ptp(), ], opacity=1, color=(.5, .5, .5)) mlab.show() . . 実際にはグリグリと3Dで脳を動かしてみることができる。なお、脳溝がほぼ見えないが、マーモセットは元々脳溝が少ない。 . 参考文献 . Liu, C., Ye, F.Q., Newman, J.D. et al. A resource for the detailed 3D mapping of white matter pathways in the marmoset brain. Nat Neurosci. (2020) doi:10.1038/s41593-019-0575-0 | Liu, C., Ye, F. Q., Yen, C. C., Newman, J. D. et al. A digital 3D atlas of the marmoset brain based on multi-modal MRI. NeuroImage. (2018). doi:10.1016/j.neuroimage.2017.12.004 | .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/neuroscience/2020/01/15/nibabel.html",
            "relUrl": "/neuroscience/2020/01/15/nibabel.html",
            "date": " • Jan 15, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "CIELUV色相環をPythonで描画する",
            "content": "CIE Luv色空間 (CIE 1976 Luv color space)の色相環(hue wheel)をOpenCVとMatplotlibで描画する。 . ぶっちゃけると色覚に関する研究もしていて、JNNS2019でポスター発表はしたが頓挫したりあっちこっちに行ったりしている。今回はその付随したメモである。色空間について、自分は今までHSV色空間を使うなどかなり適当だったが、先行研究がほとんどCIELUV色空間を使っていたので調べることとした。単なるHSVでは色の数値的差が知覚的差と一致しないが、CIELuvでは両者ができるだけ近くなるようにしている。 . 描画にはcolourまたはOpenCVを使う。Pythonのpackageであるcolourはpipで入る：pip install colour-science . なお、変換をscratchでする場合にはhttp://www.easyrgb.com/en/math.phpが参考になる。 . OpenCV&#12434;&#20351;&#12358;&#22580;&#21512; . CIE Luv色空間において値域はL [0, 100], u[-100, 100], v[-100, 100]である。Hue angle の配列を作り、cos, sinに入れて100をかけ、それぞれをu, vとする。そしてLを適当に定めることでLuvの配列ができる。 . 後の面倒な変換はcv2.cvtColor(hogehoge, cv2.COLOR_Luv2RGB)に任せる(cf. 変換できる色空間の一覧)。単なる変換ではRGBのどれかの値が負の値を取るが、それを0にclippingしている。逆にcvtColorを使うと正規化されずにclippingされるので、L=100のときはほぼ白色となる。 . 変換式は下記参照（Miscellaneous Image Transformations — OpenCV 2.4.13.7 documentationより引用）。 . . &#33394;&#30456;&#12496;&#12540; . import matplotlib.pyplot as plt import numpy as np import cv2 N_theta = 1000 luv = np.zeros((1, N_theta, 3)).astype(np.float32) theta = np.linspace(0, 2*np.pi, N_theta) luv[:, :, 0] = 80 # L luv[:, :, 1] = np.cos(theta)*100 # u luv[:, :, 2] = np.sin(theta)*100 # v rgb = cv2.cvtColor(luv, cv2.COLOR_Luv2RGB) plt.imshow(rgb, vmin=0, vmax=1, aspect=100) plt.show() . &#33394;&#30456;&#29872; . N_theta = 1000 luv = np.zeros((1, N_theta, 3)).astype(np.float32) theta = np.linspace(0, 2*np.pi, N_theta) luv[:, :, 0] = 80 # L luv[:, :, 1] = np.cos(theta)*100 # u luv[:, :, 2] = np.sin(theta)*100 # v rgb = cv2.cvtColor(luv, cv2.COLOR_Luv2RGB) # hue wheel plot ax = plt.subplot(111, polar=True) #get coordinates: theta = np.linspace(0, 2*np.pi, rgb.shape[1]+1) r = np.linspace(0.5, 1, rgb.shape[0]+1) Theta,R = np.meshgrid(theta, r) # get color color = rgb.reshape((rgb.shape[0]*rgb.shape[1], rgb.shape[2])) m = plt.pcolormesh(theta,R, rgb[:,:,0], color=color, linewidth=0) # This is necessary to let the `color` argument determine the color m.set_array(None) plt.show() . uv&#24179;&#38754; . uv平面をplotすると次のようになる。 . W = 1000 H = 1000 luv = np.zeros((W, H, 3)).astype(np.float32) u = np.linspace(-100, 100, W) # u v = np.linspace(-100, 100, H) # v U, V = np.meshgrid(u, v) luv[:, :, 0] = 80 luv[:, :, 1] = U luv[:, :, 2] = V RGB = cv2.cvtColor(luv, cv2.COLOR_Luv2RGB) plt.figure(figsize=(5,5)) plt.imshow(RGB, vmin=0, vmax=1) plt.show() . Colour&#12434;&#20351;&#12358;&#22580;&#21512; . colourは色の変換関数が充実している。そこでu&#39;, v&#39;平面上の円周上の色を、xyに変換、さらにXYZに変換し(このときY=1とする)、最後にRGBに変換したものをplotする。 . &#33394;&#30456;&#12496;&#12540; . HSVと比べると青がかなり短いことが分かる。 . import colour N_theta = 500 theta = np.linspace(0, 2*np.pi, N_theta) r = 0.2 u = np.cos(theta)*r + 0.2009 v = np.sin(theta)*r + 0.4610 uv = np.dstack((u, v)) # map -&gt; xy -&gt; XYZ -&gt; sRGB xy = colour.Luv_uv_to_xy(uv) xyz = colour.xy_to_XYZ(xy) rgb = colour.XYZ_to_sRGB(xyz) rgb = colour.utilities.normalise_maximum(rgb, axis=-1) plt.figure(figsize=(5,3)) plt.imshow(np.reshape(rgb, (1, N_theta, 3)), aspect=100) plt.show() . N_theta = 500 theta = np.linspace(0, 2*np.pi, N_theta) r = 0.2 u = np.cos(theta)*r + 0.2009 v = np.sin(theta)*r + 0.4610 uv = np.dstack((u, v)) # map -&gt; xy -&gt; XYZ -&gt; sRGB xy = colour.Luv_uv_to_xy(uv) xyz = colour.xy_to_XYZ(xy) rgb = colour.XYZ_to_sRGB(xyz) rgb = colour.utilities.normalise_maximum(rgb, axis=-1) # hue wheel plot ax = plt.subplot(111, polar=True) #get coordinates: theta = np.linspace(0, 2*np.pi, rgb.shape[1]+1) r = np.linspace(0.5, 1, rgb.shape[0]+1) Theta,R = np.meshgrid(theta, r) # get color color = rgb.reshape((rgb.shape[0]*rgb.shape[1], rgb.shape[2])) m = plt.pcolormesh(theta,R, rgb[:,:,0], color=color, linewidth=0) # This is necessary to let the `color` argument determine the color m.set_array(None) plt.show() . CIE 1976 UCS (uniform chromaticity scale) diagram . 描画方法を2つ示す。 . Method 1 . 黒線は光のスペクトルに対応する点を意味する。 . import matplotlib.pyplot as plt import numpy as np import colour samples = 258 xlim = (0, 1) ylim = (0, 1) wvl = np.arange(420, 700, 5) wvl_XYZ = colour.wavelength_to_XYZ(wvl) wvl_uv = colour.Luv_to_uv(colour.XYZ_to_Luv(wvl_XYZ)) wvl_pts = wvl_uv * samples u = np.linspace(xlim[0], xlim[1], samples) v = np.linspace(ylim[0], ylim[1], samples) uu, vv = np.meshgrid(u, v) # stack u and v for vectorized computations uuvv = np.stack((vv,uu), axis=2) # map -&gt; xy -&gt; XYZ -&gt; sRGB xy = colour.Luv_uv_to_xy(uuvv) xyz = colour.xy_to_XYZ(xy) dat = colour.XYZ_to_sRGB(xyz) dat = colour.normalise_maximum(dat, axis=-1) # now make an alpha/transparency mask to hide the background # and flip u,v axes because of column-major symantics alpha = np.ones((samples, samples)) # * wvl_mask dat = np.swapaxes(np.dstack((dat, alpha)), 0, 1) # lastly, duplicate the lowest wavelength so that the boundary line is closed wvl_uv = np.vstack((wvl_uv, wvl_uv[0,:])) fig, ax = plt.subplots(figsize=(5,5)) ax.imshow(dat, extent=[xlim[0], xlim[1], ylim[0], ylim[1]], interpolation=&#39;None&#39;, origin=&#39;lower&#39;) ax.set(xlim=(0, 0.7), xlabel=&#39;CIE u &#39;&#39;, ylim=(0, 0.7), ylabel=&#39;CIE v &#39;&#39;) ax.plot(wvl_uv[:,0], wvl_uv[:,1], c=&#39;0&#39;, lw=3) plt.show() . Method 2 . colour.plottingを用いる方法。 . import colour.plotting as cpl cpl.plot_chromaticity_diagram_CIE1976UCS(standalone=False) cpl.render( standalone=True, bounding_box=(-0.1, 0.7, -0.05, 0.7), x_tighten=True, y_tighten=True, filename=&quot;CIE1976UCS_diagram.png&quot;) plt.show() .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/python/2020/01/13/cieluv.html",
            "relUrl": "/python/2020/01/13/cieluv.html",
            "date": " • Jan 13, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "scGenの解説",
            "content": "概要 . scGenの論文の解説．実際はほぼVAEの話になった．実験の詳細とかは割愛．LPSが異なる生物に与える影響とかを予測していてかなりアツい． . 細胞が外界から刺激を受けたときにどんな反応をするかを予測する．潜在空間で摂動$ delta$が加えられたときに，遺伝子発現空間での変化をニューラルネットワークを使って予測する． . VAE 　 . variational autoencoder(VAE)では確率分布$P( boldsymbol{x} _ i; boldsymbol{ theta})$に従って新しいデータ点が生成される．ただし，確率分布は$P( boldsymbol{x} _ i; boldsymbol{ theta})$の対数尤度（を各$ boldsymbol{x} _ i$について足したもの）が最大となるように取る．潜在変数を$ boldsymbol{z}$とすれば，この確率は次のようになる： . P(xi;θ)=∫P(xi∣zi;θ)P(zi;θ) dzi. begin{aligned} P( boldsymbol{x} _ i; boldsymbol{ theta})= int P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta})P( boldsymbol{z} _ i; boldsymbol{ theta}) ,d boldsymbol{z} _ i. end{aligned}P(xi​;θ)=∫P(xi​∣zi​;θ)P(zi​;θ)dzi​.​ . $ boldsymbol{x} _ i$を生成しそうな$ boldsymbol{z} _ i$が潜在空間から正規分布$P( boldsymbol{z} _ i; boldsymbol{ theta})$に従ってサンプリングされるような確率分布を求めることが目標になる． . ここで，$P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta})$に近い確率分布$Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ theta})$をニューラルネットワークで作る．２つの確率分布の近さの評価としてKullback Leibler divergenceを使う： . KL(Q(zi∣xi;ϕ)∥P(zi∣xi;θ))=EQ(zi∣xi;ϕ)[log⁡Q(zi∣xi;ϕ)−log⁡P(zi∣xi;θ)]=EQ(zi∣xi;ϕ)[log⁡Q(zi∣xi;ϕ)−log⁡P(zi;θ)P(xi;θ)P(xi∣zi;θ)]=EQ(zi∣xi;ϕ)[log⁡Q(zi∣xi;ϕ)−log⁡P(zi;θ)−log⁡P(xi∣zi;θ)+log⁡P(xi;θ)]=−EQ(zi∣xi;ϕ)[log⁡P(xi∣zi;θ)]+log⁡P(xi;θ)+KL(Q(zi∣xi;ϕ)∥P(zi;θ)). begin{aligned} &amp; text{KL}(Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi}) |P( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ theta})) &amp;= E _ {Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})} left[ log Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})- log P( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ theta}) right] &amp;= E _ {Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})} Bigl[ log Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})- log frac{P( boldsymbol{z} _ i; boldsymbol{ theta})}{P( boldsymbol{x} _ i; boldsymbol{ theta})}P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta}) Bigr] &amp;= E _ {Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})} bigl[ log Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})- log P( boldsymbol{z} _ i; boldsymbol{ theta})- log P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta})+ log P( boldsymbol{x} _ i; boldsymbol{ theta}) bigr] &amp;= -E _ {Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})}[ log P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta})]+ log P( boldsymbol{x} _ i; boldsymbol{ theta})+ text{KL}(Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi}) | P( boldsymbol{z} _ i; boldsymbol{ theta})). end{aligned}​KL(Q(zi​∣xi​;ϕ)∥P(zi​∣xi​;θ))=EQ(zi​∣xi​;ϕ)​[logQ(zi​∣xi​;ϕ)−logP(zi​∣xi​;θ)]=EQ(zi​∣xi​;ϕ)​[logQ(zi​∣xi​;ϕ)−logP(xi​;θ)P(zi​;θ)​P(xi​∣zi​;θ)]=EQ(zi​∣xi​;ϕ)​[logQ(zi​∣xi​;ϕ)−logP(zi​;θ)−logP(xi​∣zi​;θ)+logP(xi​;θ)]=−EQ(zi​∣xi​;ϕ)​[logP(xi​∣zi​;θ)]+logP(xi​;θ)+KL(Q(zi​∣xi​;ϕ)∥P(zi​;θ)).​ . 式変形の途中でBayesの定理を用いた．$Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ theta})$は$P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta})$の近似であるので，これらの量のKullback Leibler divergenceはほぼ$0$である．よって， . log⁡P(xi;θ)≃EQ(zi∣xi;ϕ)[log⁡P(xi∣zi;θ)]−KL(Q(zi∣xi;ϕ)∥P(zi;θ)). log P( boldsymbol{x} _ i; boldsymbol{ theta}) simeq E _ {Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})}[ log P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta})]- text{KL}(Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi}) | P( boldsymbol{z} _ i; boldsymbol{ theta})).logP(xi​;θ)≃EQ(zi​∣xi​;ϕ)​[logP(xi​∣zi​;θ)]−KL(Q(zi​∣xi​;ϕ)∥P(zi​;θ)). . $ eqref{ELBO}$第１項は解析的に解くことは困難なので，Monte Carlo法によるサンプリングによって決める．ただし，$Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})$に従って$ boldsymbol{z} _ i$を選び出す： . zi∼Q(zi∣xi;ϕ) begin{aligned} boldsymbol{z} _ i sim Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi}) end{aligned}zi​∼Q(zi​∣xi​;ϕ)​ . のは不便であるので，再パラメータ化を考える．すなわち， . zi=gϕ(ϵ,xi),ϵ∼p(ϵ) begin{aligned} boldsymbol{z} _ i = g _ { boldsymbol{ phi}}( boldsymbol{ epsilon}, boldsymbol{x} _ i), quad boldsymbol{ epsilon} sim p( boldsymbol{ epsilon}) end{aligned}zi​=gϕ​(ϵ,xi​),ϵ∼p(ϵ)​ . となる関数$g _ { boldsymbol{ phi}}$とノイズ$ boldsymbol{ epsilon}$を適当に選んでやる．こうすれば， . EQ(zi∣xi;ϕ)[f(zi)]=∫Q(zi∣xi;ϕ)f(zi)dzi=∫p(ϵ)f(zi)dϵ=Ep(ϵ)[f(zi)],zi=gϕ(ϵ,xi) begin{aligned} E _ {Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})}[f( boldsymbol{z} _ i)] &amp;= int Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi}) f( boldsymbol{z} _ i)d boldsymbol{z} _ i &amp;= int p( boldsymbol{ epsilon}) f( boldsymbol{z} _ i)d boldsymbol{ epsilon} &amp;= E _ {p( boldsymbol{ epsilon})}[f( boldsymbol{z} _ i)], quad boldsymbol{z} _ i = g _ { boldsymbol{ phi}}( boldsymbol{ epsilon}, boldsymbol{x} _ i) end{aligned}EQ(zi​∣xi​;ϕ)​[f(zi​)]​=∫Q(zi​∣xi​;ϕ)f(zi​)dzi​=∫p(ϵ)f(zi​)dϵ=Ep(ϵ)​[f(zi​)],zi​=gϕ​(ϵ,xi​)​ . となる．よって，再パラメータ化を施せば$ eqref{ELBO}$第１項は . EQ(zi∣xi;ϕ)[log⁡P(xi∣zi;θ)]=1L∑l=1Llog⁡P(xi∣zi(l);θ) begin{aligned} E _ {Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})}[ log P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta})] &amp;= frac{1}{L} sum _ {l=1}^L log P( boldsymbol{x} _ i mid boldsymbol{z} _ i{}^{(l)}; boldsymbol{ theta}) end{aligned}EQ(zi​∣xi​;ϕ)​[logP(xi​∣zi​;θ)]​=L1​l=1∑L​logP(xi​∣zi​(l);θ)​ . となる．ただし， . zi(l)=gϕ(ϵ(l),xi),ϵ(l)∼p(ϵ) begin{aligned} boldsymbol{z} _ i{}^{(l)}=g _ { boldsymbol{ phi}}( boldsymbol{ epsilon}^{(l)}, boldsymbol{x} _ i), quad boldsymbol{ epsilon}^{(l)} sim p( boldsymbol{ epsilon}) end{aligned}zi​(l)=gϕ​(ϵ(l),xi​),ϵ(l)∼p(ϵ)​ . である．よって，$ eqref{ELBO}$は次のようになる： . log⁡P(xi;θ)=1L∑l=1Llog⁡P(xi∣zi(l);θ)−KL(Q(zi∣xi;ϕ)∥P(zi;θ)). begin{aligned} &amp; log P( boldsymbol{x} _ i; boldsymbol{ theta}) &amp;= frac{1}{L} sum _ {l=1}^L log P( boldsymbol{x} _ i mid boldsymbol{z} _ i{}^{(l)}; boldsymbol{ theta})- text{KL}(Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi}) | P( boldsymbol{z} _ i; boldsymbol{ theta})). end{aligned}​logP(xi​;θ)=L1​l=1∑L​logP(xi​∣zi​(l);θ)−KL(Q(zi​∣xi​;ϕ)∥P(zi​;θ)).​ . ここで，$Q( boldsymbol{z} mid boldsymbol{x}; boldsymbol{ theta})$が多変量正規分布であると仮定する： . Q(z∣x;θ)=1(2π)n∣Σ∣exp⁡[−12t(z−μ)Σ−1(z−μ)]. begin{aligned} &amp;Q( boldsymbol{z} mid boldsymbol{x}; boldsymbol{ theta}) &amp;= frac{1}{ sqrt{(2 pi)^n mid Sigma mid}} exp left[- frac{1}{2}{}^t( boldsymbol{z}- boldsymbol mu) Sigma{}^{-1}( boldsymbol{z}- boldsymbol mu) right]. end{aligned}​Q(z∣x;θ)=(2π)n∣Σ∣ . ​1​exp[−21​t(z−μ)Σ−1(z−μ)].​ . ただし，$ Sigma, boldsymbol mu$は$ boldsymbol{x}$によって決まり，特に$ Sigma$は対角行列であるとする： . Σ=(σ12Oσ22⋱Oσn2). begin{aligned} Sigma = begin{pmatrix} sigma_1{}^2 &amp;&amp;&amp;O &amp; sigma_2{}^2 &amp;&amp; &amp;&amp; ddots&amp; O &amp;&amp;&amp; sigma_n{}^2 end{pmatrix}. end{aligned}Σ=⎝⎜⎜⎜⎛​σ1​2O​σ2​2​⋱​Oσn​2​⎠⎟⎟⎟⎞​.​ . さらに，$P( boldsymbol{z}; boldsymbol{ theta})$も正規分布であるとする： . P(z;θ)=1(2π)nexp⁡(−∣z∣22). begin{aligned} P( boldsymbol{z}; boldsymbol{ theta}) = frac{1}{ sqrt{(2 pi)^n}} exp left(- frac{ mid boldsymbol{z} mid^2}{2} right). end{aligned}P(z;θ)=(2π)n . ​1​exp(−2∣z∣2​).​ . まず， . ∫Q(z∣x;θ)log⁡P(z;θ) dz=1(2π)n∣Σ∣∫exp⁡[−12t(z−μ)Σ−1(z−μ)]×[−n2log⁡(2π)−∣z∣22] dz=−n2log⁡(2π)−12(2π)n∣Σ∣×∫∣z∣2exp⁡[−12t(z−μ)Σ−1(z−μ)] dz begin{aligned} &amp; int Q( boldsymbol{z} mid boldsymbol{x}; boldsymbol{ theta}) log P( boldsymbol{z}; boldsymbol{ theta}) ,d boldsymbol{z} &amp;= frac{1}{ sqrt{(2 pi)^n mid Sigma mid}} int exp left[- frac{1}{2}{}^t( boldsymbol{z}- boldsymbol mu) Sigma{}^{-1}( boldsymbol{z}- boldsymbol mu) right] times left[- frac{n}{2} log(2 pi)- frac{ mid boldsymbol{z} mid^2}{2} right] ,d boldsymbol{z} &amp;= - frac{n}{2} log(2 pi)- frac{1}{2 sqrt{(2 pi)^n mid Sigma mid}} times int mid boldsymbol{z} mid^2 exp left[- frac{1}{2}{}^t( boldsymbol{z}- boldsymbol mu) Sigma{}^{-1}( boldsymbol{z}- boldsymbol mu) right] ,d boldsymbol{z} end{aligned}​∫Q(z∣x;θ)logP(z;θ)dz=(2π)n∣Σ∣ . ​1​∫exp[−21​t(z−μ)Σ−1(z−μ)]×[−2n​log(2π)−2∣z∣2​]dz=−2n​log(2π)−2(2π)n∣Σ∣ . ​1​×∫∣z∣2exp[−21​t(z−μ)Σ−1(z−μ)]dz​ . 第２項の積分は . ∑i=1n∫zi2exp⁡[−12∑j=1n(zj−μj)2σj2] dz=∑i=1n∫exp⁡[−(z1−μ1)22σ12] dz1×⋯×∫zi2exp⁡[−(zj−μj)22σj2] dzi×⋯×∫exp⁡[−(zn−μn)22σn2] dzn=∑i=1n(2π)n−12∏j≠iσj∫zi2exp⁡[−(zj−μj)22σj2] dzi=∑i=1n(2π)n2∏j=1nσj(σi2+μi2) begin{aligned} &amp; sum _ {i=1}^n int z_i{}^2 exp left[- frac{1}{2} sum _ {j=1}^n frac{(z_j- mu _ {j})^2}{ sigma_j{}^2} right] ,d boldsymbol{z} &amp;= sum _ {i=1}^n int exp left[- frac{(z_1- mu _ {1})^2}{2 sigma_1{}^2} right] ,dz_1 times &amp; qquad dots times int z_i{}^2 exp left[- frac{(z_j- mu _ {j})^2}{2 sigma_j{}^2} right] ,dz_i times &amp; qquad dots times int exp left[- frac{(z_n- mu _ {n})^2}{2 sigma_n{}^2} right] ,dz_n &amp;= sum _ {i=1}^n(2 pi)^{ frac{n-1}{2}} prod _ {j neq i} sigma_j int z_i{}^2 exp left[- frac{(z_j- mu _ {j})^2}{2 sigma_j{}^2} right] ,dz_i &amp;= sum _ {i=1}^n(2 pi)^{ frac{n}{2}} prod _ {j=1}^n sigma_j( sigma_i{}^2+ mu _ {i}{}^2) end{aligned}​i=1∑n​∫zi​2exp[−21​j=1∑n​σj​2(zj​−μj​)2​]dz=i=1∑n​∫exp[−2σ1​2(z1​−μ1​)2​]dz1​×⋯×∫zi​2exp[−2σj​2(zj​−μj​)2​]dzi​×⋯×∫exp[−2σn​2(zn​−μn​)2​]dzn​=i=1∑n​(2π)2n−1​j​=i∏​σj​∫zi​2exp[−2σj​2(zj​−μj​)2​]dzi​=i=1∑n​(2π)2n​j=1∏n​σj​(σi​2+μi​2)​ . のように変形できるので， . ∫Q(z∣x;θ)log⁡P(z;θ) dz=−n2log⁡(2π)−12∑i=1n(σi2+μi2). int Q( boldsymbol{z} mid boldsymbol{x}; boldsymbol{ theta}) log P( boldsymbol{z}; boldsymbol{ theta}) ,d boldsymbol{z}= - frac{n}{2} log(2 pi)- frac{1}{2} sum _ {i=1}^n( sigma_i{}^2+ mu _ {i}{}^2).∫Q(z∣x;θ)logP(z;θ)dz=−2n​log(2π)−21​i=1∑n​(σi​2+μi​2). . 次に， . ∫Q(z∣x;θ)log⁡Q(z∣x;θ) dz=1(2π)n∣Σ∣∫exp⁡[−12∑i=1n(zi−μi)2σi2]×[−n2log⁡2π−12log⁡∣Σ∣−12∑j=1n(zj−μj)2σj2] dz=−n2log⁡2π−12log⁡∣Σ∣−12(2π)n∣Σ∣×∑i=1n∫(zi−μi)2σi2exp⁡[−∑j=1n(zj−μj)22σj2] dz begin{aligned} &amp; int Q( boldsymbol{z} mid boldsymbol{x}; boldsymbol{ theta}) log Q( boldsymbol{z} mid boldsymbol{x}; boldsymbol{ theta}) ,d boldsymbol{z} &amp;= frac{1}{ sqrt{(2 pi)^n mid Sigma mid}} int exp left[- frac{1}{2} sum _ {i=1}^n frac{(z_i- mu _ {i})^2}{ sigma_i{}^2} right] times left[- frac{n}{2} log 2 pi- frac{1}{2} log mid Sigma mid- frac{1}{2} sum _ {j=1}^n frac{(z_j- mu _ {j})^2}{ sigma_j{}^2} right] ,d boldsymbol{z} &amp;=- frac{n}{2} log 2 pi- frac{1}{2} log mid Sigma mid- frac{1}{2 sqrt{(2 pi)^n mid Sigma mid}} times sum _ {i=1}^n int frac{(z_i- mu _ {i})^2}{ sigma_i{}^2} exp left[- sum _ {j=1}^n frac{(z_j- mu _ {j})^2}{2 sigma_j{}^2} right] ,d boldsymbol{z} end{aligned}​∫Q(z∣x;θ)logQ(z∣x;θ)dz=(2π)n∣Σ∣ . ​1​∫exp[−21​i=1∑n​σi​2(zi​−μi​)2​]×[−2n​log2π−21​log∣Σ∣−21​j=1∑n​σj​2(zj​−μj​)2​]dz=−2n​log2π−21​log∣Σ∣−2(2π)n∣Σ∣ . ​1​×i=1∑n​∫σi​2(zi​−μi​)2​exp[−j=1∑n​2σj​2(zj​−μj​)2​]dz​ . 第３項の積分は . (原因不明のエラーにより中断. 200402) .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/single-cell-analysis/2019/10/02/scgen.html",
            "relUrl": "/single-cell-analysis/2019/10/02/scgen.html",
            "date": " • Oct 2, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "scGenの解説",
            "content": "Principal Curve（主曲線） . Principal ComponentとPrincipal Curve . $ boldsymbol{R}^d$内で$N$個のデータがどのように分布しているかを，比較的簡単な式で表したい（次元を減らしたい）時がある．一つのよく知られた方法は，線形回帰である．これは，線形関数$f$を用いて，$f(x_1, ldots, check{x}_i, ldots,x_d)$の式を考え，実際のデータとの差の分散 . 1N∑k=1N[xi−f(x1,…,xˇi,…,xd)]2 begin{aligned} frac{1}{N} sum_{k=1}^N[x_i-f(x_1, ldots, check{x}_i, ldots,x_d)]^2 end{aligned}N1​k=1∑N​[xi​−f(x1​,…,xˇi​,…,xd​)]2​ . が最小となるようにする．これは，$ boldsymbol{R}^d$内で，$x_i$軸に沿った，データ点と直線の距離の２乗平均を最小にするのに等しい（図）． . これに対し，主成分分析では，同じく直線を考えるが，その直線に関する成分の２乗和 . ∑k=1Nx⋅e1 begin{aligned} sum_{k=1}^N boldsymbol{x} cdot boldsymbol{e}_1 end{aligned}k=1∑N​x⋅e1​​ . が最大となる単位ベクトル$ boldsymbol{e}_1$を方向ベクトルとする直線を第１主成分とする．ここで，各データ点と直線の距離を$d( boldsymbol{x}_k)$とすれば， . d(xk)2=∥x∥2−x⋅e1 begin{aligned} d( boldsymbol{x}_k)^2= | boldsymbol{x} |^2- boldsymbol{x} cdot boldsymbol{e}_1 end{aligned}d(xk​)2=∥x∥2−x⋅e1​​ . なので，主成分分析は各データ点との距離の２乗平均 . 1N∑k=1Nd(xk)2 begin{aligned} frac{1}{N} sum_{k=1}^N d( boldsymbol{x}_k)^2 end{aligned}N1​k=1∑N​d(xk​)2​ . が最小となるような直線を選ぶことに等しい（図）． . Principal Curve Analysisはデータ点を曲線で代表する方法である．各データ点とその曲線上への射影点との距離の２乗平均が最小となるような曲線を考える（図）. . Principal Curveの定義 . $ boldsymbol{R}^p$内に確率密度$h$に従って分布した点を$ boldsymbol{X}$で表す．$E( boldsymbol{X})=0$としても一般性を失わない．$C^ infty$級曲線$ boldsymbol{f}: boldsymbol{R} supset Lambda to boldsymbol{R}^p$を考える．ただし$ boldsymbol{f}$は自己交叉しないものとする： . λ1≠λ2⇒f(λ1)≠f(λ2). begin{aligned} lambda_1 neq lambda_2 Rightarrow boldsymbol{f}( lambda_1) neq boldsymbol{f}( lambda_2). end{aligned}λ1​​=λ2​⇒f(λ1​)​=f(λ2​).​ . また，パラメータ$ forall lambda in Lambda$について$ boldsymbol{f}$は単位速さであるとする： . ∥f′(λ)∥=1. begin{aligned} | boldsymbol{f}&amp;#x27;( lambda) |=1. end{aligned}∥f′(λ)∥=1.​ . $ boldsymbol{X}$の$ boldsymbol{f}$に関するprojection index $ lambda_{ boldsymbol{f}}: boldsymbol{R}^p to boldsymbol{R}$を次のように定義する： . λf(x)=sup⁡λ{λ∣∥x−f(λ)∥=inf⁡μ∥x−f(μ)∥}. begin{aligned} lambda_{ boldsymbol{f}}( boldsymbol{x})= sup_ lambda left { lambda mid | boldsymbol{x}- boldsymbol{f}( lambda) |= inf_ mu | boldsymbol{x}- boldsymbol{f}( mu) | right }. end{aligned}λf​(x)=λsup​{λ∣∥x−f(λ)∥=μinf​∥x−f(μ)∥}.​ . すなわち，$ boldsymbol{x}$の$ boldsymbol{f}$に関するprojection index $ lambda_{ boldsymbol{f}}( boldsymbol{x})$は，$ boldsymbol{x}$に最も近い$ boldsymbol{f}$の点に対応するパラメータのうち最大のものである． . 定義A . 曲線$ boldsymbol{f}$をself-consistentである，もしくはprincipal curveであるとは，全ての$ lambda in Lambda$に対し， . E(X∣λf(X)=λ)=f(λ) begin{aligned} E( boldsymbol{X} mid lambda_{ boldsymbol{f}}( boldsymbol{X})= lambda)= boldsymbol{f}( lambda) end{aligned}E(X∣λf​(X)=λ)=f(λ)​ . が成立することである(Hastie and Stuetzle, 1989)． . . つまり，projection index $ lambda_{ boldsymbol{f}}( boldsymbol{x})$によって$ lambda$に射影される$ boldsymbol{R}^p$の全ての点$ boldsymbol{X}$を考える．そのような$ boldsymbol{X}$の平均がちょうど$ boldsymbol{f}( lambda)$になるのである． . Principal Curveは主成分の自然な拡張である．それは次の定理によって分かる： . 定理1 . $ boldsymbol{u}_0 cdot boldsymbol{v}_0=0$とすると，直線$l( lambda)= boldsymbol{u}_0+ lambda boldsymbol{v}_0$がself-consistentならば，それは主成分である． . 証明 . projection indexの性質（最短距離を指すパラメータが複数ある場合は最大値を取る）によって，異なる$ lambda$の原像$ lambda_{ boldsymbol{f}}{}^{-1}( lambda)$は交わらない．よって，${ boldsymbol{X}}$は，projection indexによって$ lambda$に写る$ boldsymbol{X}$の集合の直和で表される： . begin{align} { boldsymbol{X}}= bigoplus_ lambda{ boldsymbol{X} mid lambda_{ boldsymbol{f}}( boldsymbol{X})= lambda}. end{align} . よって，$ boldsymbol{X}$の平均は，ある$ lambda$にprojection indexによって写される$ boldsymbol{X}$の平均$ boldsymbol{X} lambda=E( boldsymbol{X} mid lambda{ boldsymbol{f}}( boldsymbol{X})= lambda)$の$ lambda$に関する平均に等しい： . 0=E(X)=EλE(X∣λf(X)=λ)=Eλ(u0+λv0)=u0+λˉv0. begin{aligned} 0=E( boldsymbol{X}) &amp;= E_ lambda E( boldsymbol{X} mid lambda_{ boldsymbol{f}}( boldsymbol{X})= lambda) &amp;= E_ lambda( boldsymbol{u}_0+ lambda boldsymbol{v}_0) &amp;= boldsymbol{u}_0+ bar{ lambda} boldsymbol{v}_0. end{aligned}0=E(X)​=Eλ​E(X∣λf​(X)=λ)=Eλ​(u0​+λv0​)=u0​+λˉv0​.​ . 両辺$ boldsymbol{u}_0$で内積を取れば，$ boldsymbol{u}_0 cdot boldsymbol{v}_0=0$なので . u0=0. boldsymbol{u}_0=0.u0​=0. . projection indexによって$ lambda$に写る$ boldsymbol{X}$について$ boldsymbol{u}_0$，つまり$l$は原点を通るので， . λf(X)=X⋅v0=Xtv0. begin{aligned} lambda_{ boldsymbol{f}}( boldsymbol{X}) = boldsymbol{X} cdot boldsymbol{v}_0 = boldsymbol{X}^t boldsymbol{v}_0. end{aligned}λf​(X)=X⋅v0​=Xtv0​.​ . $ boldsymbol{X}$の共分散行列を$ Sigma$とすれば， . Σ=E[(X−E(X))(X−E(X))t]=E(XXt) begin{aligned} Sigma=E left[( boldsymbol{X}-E( boldsymbol{X}))( boldsymbol{X}-E( boldsymbol{X}))^t right]=E( boldsymbol{X} boldsymbol{X}^t) end{aligned}Σ=E[(X−E(X))(X−E(X))t]=E(XXt)​ . となるので， . Σv0=E(XXt)v0=EλE(XXtv0∣λf(X)=λ)=EλE(XXtv0∣Xtv0=λ)=EλE(λX∣Xtv0=λ)=Eλλ2v0. begin{aligned} Sigma boldsymbol{v}_0 &amp;= E( boldsymbol{X} boldsymbol{X}^t) boldsymbol{v}_0 &amp;= E_ lambda E( boldsymbol{X} boldsymbol{X}^t boldsymbol{v}_0 mid lambda_{ boldsymbol{f}}( boldsymbol{X})= lambda) &amp;= E_ lambda E( boldsymbol{X} boldsymbol{X}^t boldsymbol{v}_0 mid boldsymbol{X}^t boldsymbol{v}_0= lambda) &amp;= E_ lambda E( lambda boldsymbol{X} mid boldsymbol{X}^t boldsymbol{v}_0= lambda) &amp;= E_ lambda lambda^2 boldsymbol{v}_0. end{aligned}Σv0​​=E(XXt)v0​=Eλ​E(XXtv0​∣λf​(X)=λ)=Eλ​E(XXtv0​∣Xtv0​=λ)=Eλ​E(λX∣Xtv0​=λ)=Eλ​λ2v0​.​ . projection indexの存在 . 補題2.1 . 全ての$ boldsymbol{x} in boldsymbol{R}^p$と$r&gt;0$に対し，集合$Q{ lambda mid| boldsymbol{x}- boldsymbol{f}( lambda)| leq r}$はコンパクトである． . 証明 . $Q$が$ boldsymbol{R}^{p}$の有界な閉集合であることが言えれば良い． . $| boldsymbol{x}- boldsymbol{f}( lambda)|$は$ lambda$の連続関数で，値域が閉球体$B^ ast(0,r)$であるので，その原像$Q$も閉集合である． . $Q$が有界でないと仮定する．$| boldsymbol{x}- boldsymbol{f}( lambda_i)| leq r$を満たす点列$ lambda_1, ldots$が存在する．$B^ ast( boldsymbol{x},2r)$を考える．$ boldsymbol{f}$は$ lambda_i$から$ lambda_{i+1}$の間で，ずっと$B^ ast( boldsymbol{x},2r)$の中にあるか，一度$B^ ast( boldsymbol{x},2r)$を出た後，再び入ってくる．何れにせよ，$ boldsymbol{f}$は単位速さであったから，$ boldsymbol{f}( lambda_i)$から$ boldsymbol{f}( lambda_i{i+1})$までの曲線の長さは少なくとも$ min(2r, lambda_{i+1}- lambda_i)$である．よって，仮定により${ lambda_n}_{n in boldsymbol{N}}$が$B( boldsymbol{x},2r)$内で無限長を持つことになり，矛盾である． . 補題2.2 . 全ての$ boldsymbol{x} in boldsymbol{R}^p$に対し，$| boldsymbol{x}- boldsymbol{f}( lambda)|= inf_{ mu in Lambda}| boldsymbol{x}- boldsymbol{f}( mu)|$なる$ lambda in Lambda$が存在する． . 証明 . $r= inf_{ mu in Lambda}| boldsymbol{x}- boldsymbol{f}( mu)|$，$B={ mu mid| boldsymbol{x}- boldsymbol{f}( mu)| leq2r}$とする．$B$は空でなく，コンパクトなので，最大値・最小値が存在する．よって，$ inf_{ mu in Lambda}| boldsymbol{x}- boldsymbol{f}( mu)|= inf_{ mu in B}| boldsymbol{x}- boldsymbol{f}( mu)|$が成立する． . . $d( boldsymbol{x}, boldsymbol{f})= inf_{ mu in Lambda}| boldsymbol{x}- boldsymbol{f}( mu)|$とする． . 定理2 . projection index $ lambda_{ boldsymbol{f}}( boldsymbol{x})= sup_ lambda{ lambda mid| boldsymbol{x}- boldsymbol{f}( lambda)|=d( boldsymbol{x}, boldsymbol{f})}$はwell-definedである． . 証明 . ${ lambda mid| boldsymbol{x}- boldsymbol{f}( lambda)|=d( boldsymbol{x}, boldsymbol{f})}$は補題 ref{existence_of_parameter_set}から空でなく，補題 ref{compact_Q}からコンパクトと分かる． . ambiguity pointの集合の測度 . 補題3.1 . $ boldsymbol{x}$の最近点が$ boldsymbol{f}( lambda_0) ( lambda_0 in Lambda_0)$（ただし，$ lambda_0$はパラメータ範囲$ Lambda_0$の端点ではないとする）なら，$ boldsymbol{x}$は超平面$( boldsymbol{x}- boldsymbol{f}( lambda_0)) cdot boldsymbol{f}’( lambda_0)$上にある . 証明 . $ boldsymbol{f}( lambda_0)$は$ boldsymbol{x}$からの距離が最短なので， . 0=ddλ∥x−f(λ)∥2∣λ0=−2(x−f(λ0))⋅f′(λ0). begin{aligned} 0 &amp;= frac{d}{d lambda} | boldsymbol{x}- boldsymbol{f}( lambda) |^2 mid_{ lambda_0} &amp;= -2( boldsymbol{x}- boldsymbol{f}( lambda_0)) cdot boldsymbol{f}&amp;#x27;( lambda_0). end{aligned}0​=dλd​∥x−f(λ)∥2∣λ0​​=−2(x−f(λ0​))⋅f′(λ0​).​ . 定義B . 最近点が複数ある（$ text{card}{ lambda mid| boldsymbol{x}- boldsymbol{f}( lambda)|=d( boldsymbol{x}, boldsymbol{f})}&gt;1$である）点$ boldsymbol{x}$をambiguity pointと呼ぶ． . . $M_ lambda={ boldsymbol{x} mid( boldsymbol{x}- boldsymbol{f}( lambda)) cdot boldsymbol{f}’( lambda)=0}$とする．補題 ref{hyperplane}から，$ boldsymbol{f}( lambda)$が$ boldsymbol{x}$の最近点で$ lambda in Lambda_0$なら$ boldsymbol{x} in M_ lambda$である． . 全ての$ lambda$について，$ boldsymbol{f}’( lambda)$と直交するよう$p-1$個のベクトル場$ boldsymbol{n}1( lambda), ldots, boldsymbol{n}{p-1}( lambda)$を考える．$ boldsymbol{ chi}: Lambda times boldsymbol{R}^{p-1} to boldsymbol{R}^p$を次の様に定める： . χ(λ,v)=f(λ)+∑i=1p−1vini(λ). begin{aligned} boldsymbol{ chi}( lambda, boldsymbol{v})= boldsymbol{f}( lambda)+ sum_{i=1}^{p-1}v_i boldsymbol{n}_i( lambda). end{aligned}χ(λ,v)=f(λ)+i=1∑p−1​vi​ni​(λ).​ . さらに，$M= boldsymbol{ chi}( Lambda, boldsymbol{R}^{p-1})= bigcup_ lambda M_ lambda$を，曲線上の点で，曲線と直交する超平面に属する点の集合とする． . 定義C . $X$の部分集合族$ mathcal{F} in2^X$が$ sigma$加法族であるとは，次の性質を満たすことである： . $A in mathcal{F} Rightarrow A^c in mathcal{F}$； | $n in boldsymbol{N}$に対し，$A_n in mathcal{F} Rightarrow bigcup_{n in boldsymbol{N}}A_n in mathcal{F}$． | . $ sigma$加法族の定義から直ちに次のことが証明される： . $ phi, X in mathcal{F}$． | | $n in boldsymbol{N}$に対し，$A_n in mathcal{F} Rightarrow bigcap_{n in boldsymbol{N}}A_n in mathcal{F}$． | $A,B Rightarrow A cup B in mathcal{F}$． | $A,B Rightarrow A cap B in mathcal{F}$． | 定義D . $ mathcal{F}$を$X$の$ sigma$加法族とする．$ mu: mathcal{F} to boldsymbol{R} cup{+ infty}$が測度であるとは次の性質を満たすことである： . $A in mathcal{F}$に対し$0 leq mu(A) leq+ infty$； | $A_i,A_j in mathcal{F} (i neq j in boldsymbol{N})$に対し，$A_i cap A_j= varnothing Rightarrow mu( bigcup_{i in boldsymbol{N}}A_i)= sum_{i=1}^ infty mu(A_i)$． | . 測度の定義から直ちに次のことが証明される： . $ mu( varnothing)=0$． | $A,B in mathcal{F}, quad A subset B Rightarrow mu(A) leq mu(B)$． | $A,B in mathcal{F}, quad A subset B, quad mu(B)&lt;+ infty Rightarrow mu(B backslash A)= mu(B)- mu(A)$． | $A_i in mathcal{F} (i in boldsymbol{N})$に対し，$ mu( bigcup_{i in boldsymbol{N}}A_i) leq sum_{i=1}^ infty mu(A_i)$． | 補題3.2 . $M$に含まれないambiguity pointの（$p$次元）測度は$0$である：$ mu(A cap M^c)$． . 証明 . $ boldsymbol{x} in A cap M^c$とする．補題 ref{hyperplane}から，このような点$ boldsymbol{x}$が存在するのは，$ Lambda=[ lambda_ text{min}, lambda_ text{max}]$であり，$ boldsymbol{x}$が端点$ boldsymbol{f}( lambda_ text{min})$, $ boldsymbol{f}( lambda_ text{max})$から等距離にあり，かつそれが曲線$ boldsymbol{f}$との最短である時のみである．これは測度$0$の超平面を形成する． . 補題3.3 . $E$を測度$0$の集合とする．ambiguity pointの集合$A$の測度が$0$であるためには，$ forall boldsymbol{x} in boldsymbol{R}^p backslash E$に対し，$ mu(A cap N( boldsymbol{x}))$なる開近傍$N( boldsymbol{x})$が存在することが十分である． . 証明 . 開被覆${N( boldsymbol{x}) mid boldsymbol{x} in boldsymbol{R}^p backslash E}$は明らかに$ bar{A}$を被覆する．$ bar{A}$は$ boldsymbol{R}^p$の有界閉集合なのでコンパクトである．よって，${N( boldsymbol{x}) mid boldsymbol{x} in boldsymbol{R}^p backslash E}$の中から有限個の被覆${N_i}$を選んで，$A subset bar{A} subset bigcup_{i=1}^k N_i$とできる．よって，$ mu(A) leq mu( bigcup_{i=1}^kN_i cap A) leq sum_{i=1}^{k} mu(N_i cap A)=0$． . 補題3.4 . $ Lambda$がコンパクトな場合のみ考えても一般性を失わない． . 証明 . $ Lambda_n=[-n,n]$, $ boldsymbol{f}_n= boldsymbol{f} mid Lambda_n$とし，$A_n$を$ boldsymbol{f}_n$のambiguity pointとする．$ boldsymbol{x}$を$ boldsymbol{f}$のambiguity pointとする．補題2.1から${ lambda mid| boldsymbol{x}- boldsymbol{f}( lambda)|=d( boldsymbol{x}, boldsymbol{f})}$はコンパクト，つまり$ boldsymbol{R}$の有限閉集合である． . よって，ある$n$が存在し，${ lambda} in Lambda_n$，$ boldsymbol{x} in A_n$，$A subset bigcup_{n=1}^ infty A_n$が成立する．ここで，$ mu(A) leq mu( bigcup_{n=1}^ infty A_n) leq sum_{n=1}^ infty mu(A_i)$となる． . コンパクトな$ Lambda_n$を考えて，$ boldsymbol{f}_n$のambiguity pointの集合$A_n$について，$ mu(A_n)=0$を示せばよい． . 定義E . 写像$f: boldsymbol{R}^m to boldsymbol{R}^n$について，$ boldsymbol{y} in boldsymbol{R}^n$が正則値であるとは，$ forall boldsymbol{x} in f^{-1}( boldsymbol{y})$について，$f$の微分$f’$の階数が$p$となることである：$ text{rank}(f’( boldsymbol{x}))=p$．そうでなければ，$ boldsymbol{y}$は臨界値であると言う． . Sardの定理 . 写像$f: boldsymbol{R}^m to boldsymbol{R}^n$について$f$の微分可能性が十分高ければ，$f$の臨界値の集合$C$の測度は$0$である． . 補題3.5 . 定義Bで定義した$ boldsymbol{ chi}$の臨界値の集合$C$の測度は$0$である． . 証明 . $ boldsymbol{ chi}$は$C^ infty$級なので，Sardの定理を使う． . 定理3 . ambiguity pointの集合$A$の測度は$0$である：$ mu(A)=0$． . 証明 . 補題3.4から$ Lambda$がコンパクトの場合のみ考える．補題3.2から$ mu(A cap M^c)=0$．$ boldsymbol{ chi}$の臨界値の集合$C subset M$を考える．この時，$ mu((A cap M^c) cup C)=0$なので，補題 ref{measure_of_A_Mc}から，$ forall boldsymbol{x} in boldsymbol{R}^p backslash((A cap M^c) cup C)=M backslash C+M^c cap A^e$について，$ mu(A cap N( boldsymbol{x}))=0$なる近傍$N( boldsymbol{x})$が存在することを証明すればよい． . $A$の外部$A^e$については自明なので，以下では$ forall boldsymbol{x} in M backslash C$（正則値）について，$ mu(A cap N( boldsymbol{x}))=0$なる近傍$N( boldsymbol{x})$が存在することを証明する． . $ boldsymbol{ chi}^{-1} subset Lambda times boldsymbol{R}^{p-1}$は有限集合${( lambda_1, boldsymbol{v_1}), cdots,( lambda_k, boldsymbol{v}_k)}$である． . 仮に，無限集合であったと仮定する．この時，$ boldsymbol{ chi}( xi_i, boldsymbol{w}_i)= boldsymbol{x}$を満たす$ Lambda times boldsymbol{R}^{p-1}$の部分集合${( xi_1, boldsymbol{w}_1), ldots}$が存在する．$ Lambda$はコンパクト，$ boldsymbol{ chi}$は連続なので，${ xi_1, ldots}$の集積点$ xi_0$と対応する$ boldsymbol{w}_0$が存在し，$ boldsymbol{ chi}( xi_0, boldsymbol{w}_0)= boldsymbol{x}$となる．$ boldsymbol{x}$は正則値なので，$ text{rank}( boldsymbol{ chi}’)=p$である．よって，$( xi_0, boldsymbol{w}_0)$と$ boldsymbol{x}$の近傍で（局所）微分同相になる．よって，これは$ xi_0$が集積点であるので矛盾． . $ boldsymbol{ chi}$は正則値なので，$( lambda_i, boldsymbol{v}_i)$の近傍$L_i$と$ boldsymbol{x}$の近傍$N( boldsymbol{x})$で微分同相になる． . この時，$ tilde{N}( boldsymbol{x}) subset N( boldsymbol{x})$が存在して，$ boldsymbol{ chi}^{-1}( tilde{N}) subset bigcup_{i=1}^kL_i$が成立する． . 仮に上記のような$ tilde{N}$が存在しないと仮定する．この時，$ boldsymbol{x}$に収束する点列${ boldsymbol{x}i}$が存在し，$( xi_i, boldsymbol{w}_i) notin bigcup{i=1}^kL_i$で，$ boldsymbol{ chi}( xi_i, boldsymbol{w}i)= boldsymbol{x}_i$が成立する．点列${ xi_i}{i in boldsymbol{N}}$は集積点$ xi_0 in bigcup_{i in boldsymbol{N}}L_i$を持つ．しかし，これは$ boldsymbol{ chi}( xi_0, boldsymbol{w}_0)= boldsymbol{x}$の連続性及び，$ boldsymbol{ chi}^{-1}( boldsymbol{x})$の有限性から矛盾となる． . 以上から，$ boldsymbol{y} in tilde{N}( boldsymbol{x})$に対し，$ boldsymbol{ chi}( lambda_i( boldsymbol{y}), boldsymbol{v}_i( boldsymbol{y}))= boldsymbol{y}$を満たす$( lambda_i( boldsymbol{y}), boldsymbol{v}_i( boldsymbol{y}))$が各$L_i$に唯一存在する．ここで，$d_i( boldsymbol{y})=| boldsymbol{y}- boldsymbol{f}( lambda_i( boldsymbol{y}))|^2$とする．補題3.1から， . grad(di(y))=grad∑j=1p[yj−fj(λi(y))]2=(∂∂y1∑j=1p[yj−fj(λi(y))]2,…)=∑j=1p(2[yj−fj(λi(y))]∂∂y1[yj−fj(λi(y))],…)=(∑j=1p2[yj−fj(λi(y))]δ1j,…)−(∑j=1p2[yj−fj(λi(y))]fj′(λi(y))∂λi∂y1,…)=(2[y1−f1(λi(y))],…)−2∂λi∂y1([y−f(λi(y))]⋅f′(λi(y)))=2(y−f(λi(y))). begin{aligned} text{grad}(d_i( boldsymbol{y})) &amp;= text{grad} sum_{j=1}^p[y_j-f_j( lambda_i( boldsymbol{y}))]^2 &amp;= left( frac{ partial}{ partial y_1} sum_{j=1}^p[y_j-f_j( lambda_i( boldsymbol{y}))]^2, ldots right) &amp;= sum_{j=1}^p left(2[y_j-f_j( lambda_i( boldsymbol{y}))] frac{ partial}{ partial y_1}[y_j-f_j( lambda_i( boldsymbol{y}))], ldots right) &amp;= left( sum_{j=1}^p2[y_j-f_j( lambda_i( boldsymbol{y}))] delta_{1j}, ldots right) - left( sum_{j=1}^p2[y_j-f_j( lambda_i( boldsymbol{y}))]f^ prime_j( lambda_i( boldsymbol{y})) frac{ partial lambda_i}{ partial y_1}, ldots right) &amp;= left(2[y_1-f_1( lambda_i( boldsymbol{y}))], ldots right)-2 frac{ partial lambda_i}{ partial y_1} left([ boldsymbol{y}- boldsymbol{f}( lambda_i( boldsymbol{y}))] cdot boldsymbol{f}&amp;#x27;( lambda_i( boldsymbol{y})) right) &amp;= 2( boldsymbol{y}- boldsymbol{f}( lambda_i( boldsymbol{y}))). end{aligned}grad(di​(y))​=gradj=1∑p​[yj​−fj​(λi​(y))]2=(∂y1​∂​j=1∑p​[yj​−fj​(λi​(y))]2,…)=j=1∑p​(2[yj​−fj​(λi​(y))]∂y1​∂​[yj​−fj​(λi​(y))],…)=(j=1∑p​2[yj​−fj​(λi​(y))]δ1j​,…)−(j=1∑p​2[yj​−fj​(λi​(y))]fj′​(λi​(y))∂y1​∂λi​​,…)=(2[y1​−f1​(λi​(y))],…)−2∂y1​∂λi​​([y−f(λi​(y))]⋅f′(λi​(y)))=2(y−f(λi​(y))).​ . $ boldsymbol{y} in tilde{N}( boldsymbol{x})$がambiguity pointになるのは， . Aij={z∈N~(x)∣di(z)=dj(z),λi(z)≠λj(z)} begin{aligned} A_{ij} = { boldsymbol{z} in tilde{N}( boldsymbol{x}) mid d_i( boldsymbol{z})=d_j( boldsymbol{z}), lambda_i( boldsymbol{z}) neq lambda_j( boldsymbol{z}) } end{aligned}Aij​={z∈N~(x)∣di​(z)=dj​(z),λi​(z)​=λj​(z)}​ . として，$ boldsymbol{y} in A_{ij} ( exists i,j;i neq j)$の時のみである． . $ boldsymbol{f}$は自己交叉しない（$ eqref{no-self_crossing$}が成立する）ので，$ lambda_i( boldsymbol{z}) neq lambda_j( boldsymbol{z})$ とすると，$ eqref{grad_d}$から， . grad(di(z)−dj(z))=2(f(λj(z))−f(λi(z)))≠0. begin{aligned} text{grad}(d_i( boldsymbol{z})-d_j( boldsymbol{z})) &amp;= 2( boldsymbol{f}( lambda_j( boldsymbol{z}))- boldsymbol{f}( lambda_i( boldsymbol{z}))) &amp; neq0. end{aligned}grad(di​(z)−dj​(z))​=2(f(λj​(z))−f(λi​(z)))​=0.​ . $A_{ij}$は$p-1$次元多様体であるので，その$p$次元測度は$0$となる：$ mu(A_{ij})=0$．よって，$ mu(A cap tilde{N})= mu( bigcup_{ij}A_{ij}) leq sum_{ij} mu(A_{ij})=0$． . bendingアルゴリズム . Principal curveは次のアルゴリズムによって求めることが出来る： . . initializaiton . $ boldsymbol{a}$を第１主成分として，$ boldsymbol{f}^{0}( lambda)= bar{ boldsymbol{x}}+ boldsymbol{a} lambda$とする．$ lambda^{(0)}( boldsymbol{x})= lambda_{ boldsymbol{f}^{(0)}}( boldsymbol{x})=0$とする． . repeat . $ boldsymbol{f}^{(j)}( cdot)= boldsymbol{E}( boldsymbol{X} mid lambda_{ boldsymbol{f}^{(j-1)}}= cdot)$とする | $ boldsymbol{f}^{(j)}( lambda)$が単位速さとなるように調整する（曲線の形は変わらない）． | $ lambda^{(j)}( boldsymbol{x})= lambda_{ boldsymbol{f}^{(j)}}( boldsymbol{x}) quad forall boldsymbol{x} in h$とする． | $D^2(h, boldsymbol{f}^{(j)})=E_ lambda E(| boldsymbol{X}- boldsymbol{f}^{(j)}( lambda^{(j)}( boldsymbol{X}))|^2 mid lambda^{(j)}( boldsymbol{X})= lambda)$を計算する． | until $D^2(h, boldsymbol{f}^{(j)})$がある閾値以下になるまで繰り返す． . . これはつまり，点${ boldsymbol{X}}$と曲線の距離の２乗平均が極小値となる曲線を探している． . 有限データ点のためのbendingアルゴリズム . $p$次元の$n$個のデータ点を考える．点が有限の場合，principal curveは$( lambda_i, boldsymbol{f}_i)$の$n$個の集合になる．曲線は単位速さであるとするので，$ lambda_i$は$ boldsymbol{f}_1$から$ boldsymbol{f}_i$までの折れ線に沿った距離とする．$ lambda_1=0$としておく． . まずは，projection index $ lambda_{ boldsymbol{f}^{(j)}}( boldsymbol{x}i)$を求める．$ boldsymbol{x}_i$と，$ boldsymbol{f}_k{}^{(j)}$と$ boldsymbol{f}{k+1}{}^{(j)}$を両端とする線分の最短距離を$d_{ik}$とする．また，$ boldsymbol{f}1{}^{(j)}$から最短距離を与える線分側の点までの折れ線に沿った距離を$ lambda{ik}$とする．こうして，$1 leq k leq n-1$に対し$d_{ik}$と$ lambda_{ik}$を求める． . $ boldsymbol{x}i$のprojection index $ lambda{i}$は$d_{ik}$の最小値を与える$ lambda_{ik}$とする： . λi=λik∗,k∗=argmin⁡1≤k≤n−1dik. lambda_{i}= lambda_{ik*}, quad k*= text{arg} min_{1 leq k leq n-1}d_{ik}.λi​=λik∗​,k∗=arg1≤k≤n−1min​dik​. . 次に，projection indexが$ lambda_i$になるような点${ boldsymbol{x}}$の平均を求めて，$ boldsymbol{f}^{(j+1)}( lambda_i)$（つまり$ boldsymbol{f}_i{}^{(j+1)}$）を構成する．しかし，有限データ点の場合は$ lambda_i$に投影されるのは$ boldsymbol{x}_i$唯一という状況がほとんどである．よって，projection index $ lambda_k$が$ lambda_i$に近い点${ boldsymbol{x}_k}$を取ってきて，それらの局所平均を取る(linear weighted running-line smoother)． . $ lambda_i$に近い$wn (0&lt;w&lt;1)$個の$ lambda_j$及び点${ boldsymbol{x}_j}$に対し直線を，重み付け最小２乗法でフィッティングする．この際，各重み付けは$ lambda_i$で最大値を取って，$ lambda_i$との差が大きいほど$0$に近いもの，例えば， . wij=[1−∣λj−λiλwn th nearest−λi∣3]3w_{ij} = left[1- mid frac{ lambda_j- lambda_i}{ lambda_{wn text{ th nearest}}- lambda_i} mid ^3 right]^3wij​=[1−∣λwn th nearest​−λi​λj​−λi​​∣3]3 . などとする．${ boldsymbol{x}_j}$の平均は，この直線$ boldsymbol{l}( lambda)$の$ lambda_i$での値とする： . fi(j+1)=l(λi). boldsymbol{f}_i{}^{(j+1)} = boldsymbol{l}( lambda_i).fi​(j+1)=l(λi​). . これによって$n$個の点${ boldsymbol{f}i}$が得られる．曲線の長さに従って${ lambda{i}{}^{(j+1)}}$を定めておく． . k-segmentsアルゴリズム . 上に述べたbendingアルゴリズムの他にもPrincipal Curveを求めるアルゴリズムはいくつかあるが，データの曲率が大きかったり，自己交叉があると使い物にならなくなる．それに対処出来るのが，k-segmentsアルゴリズムである(Verbeek, Vlassis, Kr&quot;{o}se, 2001)． . 直線$s={ boldsymbol{s}(t)= boldsymbol{c}+ boldsymbol{u}t mid t in boldsymbol{R}}$を考える．点$ boldsymbol{x}$との距離は d(x,s)=inf⁡t∈R∥s(t)−x∥d( boldsymbol{x},s)= inf_{t in boldsymbol{R}} | boldsymbol{s}(t)- boldsymbol{x} |d(x,s)=inft∈R​∥s(t)−x∥ で定義される． . 定義F . $X_n$を$ boldsymbol{R}^d$から取ってきた$n$個のサンプルの集合とする時，Voronoi領域(VR) $V_1, dots,V_k$を次のように定義する： . Vi={x∈Xn∣i=argmin⁡jd(x,sj)}.V_i = { boldsymbol{x} in X_n mid i= text{arg} min_j d( boldsymbol{x},s_j) }.Vi​={x∈Xn​∣i=argminj​d(x,sj​)}. . . つまり，$V_i$は$i$番目の線$s_i$が最短となるような$X_n$の部分集合である．アルゴリズムの目標は全ての点の最短直線との距離の２乗和 . ∑i=1k∑x∈Vid(x,si)2 sum_{i=1}^k sum_{ boldsymbol{x} in V_i}d( boldsymbol{x},s_i)^2i=1∑k​x∈Vi​∑​d(x,si​)2 . を最小にするような$k$本の直線$s_1, ldots,s_k$を見つけることである． . $ eqref{kmeans_square_sum}$を最小にするような$k$本の直線を見つけるためには，ランダムな向きと位置にある$k$本の直線を用意して，次のステップを繰り返せば良い： . それぞれの直線についてVRを決定する． | 直線をそれぞれのVRの第１主成分のベクトルで置き換える． | このアルゴリズムが収束することは次のことから分かる： . $ eqref{kmeans_square_sum}$はVRの定義から，第１ステップで減少する．さらに，主成分の性質から，第２ステップでも減少する． | $X_n$は有限個なので，VRの構成方法は有限である． | . ここで，無限に長い直線ではアルゴリズムの計算量が増えるので，線分に限定する．すなわち，アルゴリズムを次のように変える： . それぞれの線分についてVRを決定する． | 線分をそれぞれのVRの第１主成分のベクトルで置き換える．そして，VRのデータ点の第１主成分の分散を$ sigma^2$として，VRの重心から$ frac{3}{2} sigma$以内の範囲に存在する部分を新しい線分とする． | $k$の決定 . $k$を決めるには，$k=1$から初めて，ある条件（線分の上限数やパフォーマンスについての条件など）が満たされるまで増やしていけば良い． . 新しい線分の挿入場所を決めるために，各データ点$ boldsymbol{x}_i$の所に長さ$0$の線分（つまり点$ boldsymbol{c}$）を追加する．この場合，点$ boldsymbol{x}$との最短距離は$d( boldsymbol{x},s)=| boldsymbol{x}- boldsymbol{c}|$で与えられる．よって，この$0$長線分も追加した$k+1$本の成分でVRを構成し，$ eqref{kmeans_square_sum}$を計算する． . 新しく追加した$0$長線分の中で，$ eqref{kmeans_square_sum}$を最も減らすような線分に関するVRを$V_{k+1}$とする．次に，$V_{k+1}$での第１主成分のベクトルから，平均からの距離$ frac{3}{2} sigma$までの線分を作る．これによって$k+1$本の線分が得られた． . 有限個の点の集合$S subset boldsymbol{R}^d$について，平均$ boldsymbol{m}$は２乗距離を最小にする： . m=argmin⁡μ∈Rd∑x∈S∥x−μ∥. boldsymbol{m}= text{arg} min_{ mu in boldsymbol{R}^d} sum_{ boldsymbol{x} in S} | boldsymbol{x}- mu |.m=argμ∈Rdmin​x∈S∑​∥x−μ∥. . よって，任意の$i$に対し . ∑x∈S∥x−m∥2≤∑x∈S∥x−xi∥2. sum_{ boldsymbol{x} in S} | boldsymbol{x}- boldsymbol{m} |^2 leq sum_{ boldsymbol{x} in S} | boldsymbol{x}- boldsymbol{x}_i |^2.x∈S∑​∥x−m∥2≤x∈S∑​∥x−xi​∥2. . 新しい線分$s$は$ boldsymbol{m}$を含むので，$S$内の点から線分への２乗距離和は，$S$内の点から$ boldsymbol{m}$への２乗距離和以下である： . ∑x∈Sd(x,s)2≤∑x∈S∥x−m∥2. sum_{ boldsymbol{x} in S}d( boldsymbol{x},s)^2 leq sum_{ boldsymbol{x} in S} | boldsymbol{x}- boldsymbol{m} |^2.x∈S∑​d(x,s)2≤x∈S∑​∥x−m∥2. . よって，線分の挿入による$ eqref{kmeans_square_sum}$の減少には下限が存在することが分かる（$V_{k+1}$を決めた時点で，$ eqref{kmeans_square_sum}$の減少は確約されており，以上の式で$S$を$V_{k+1}$とすれば，そこから更に減少することが言える）． . 新しい線分の場所の効率的な探し方 . あらかじめ$n$個のデータ間の２乗距離を表す$n times n$行列$D$を考える： . Dij=∥xi−xj∥2.D_{ij}= | boldsymbol{x}_i- boldsymbol{x}_j |^2.Dij​=∥xi​−xj​∥2. . 各点$ boldsymbol{x}_i$に対し，最も近い線分までの２乗距離$d_i^ text{VR}$を求めておく．更に . DVR=(d1VRd2VR⋮dnVR),VD=[DVR,…,DVR] begin{aligned} boldsymbol{D}^ text{VR}= begin{pmatrix} d_1^ text{VR} d_2^ text{VR} vdots d_n^ text{VR} end{pmatrix}, quad mathit{VD}=[ boldsymbol{D}^ text{VR}, ldots, boldsymbol{D}^ text{VR}] end{aligned}DVR=⎝⎜⎜⎜⎜⎛​d1VR​d2VR​⋮dnVR​​⎠⎟⎟⎟⎟⎞​,VD=[DVR,…,DVR]​ . とおく．ここで， . Gij=max⁡(VDij−Dij,0)G_{ij}= max( mathit{VD}_{ij}-D_{ij},0)Gij​=max(VDij​−Dij​,0) . とする．この時$G_{ij}$は，$0$長線分を$ boldsymbol{x}j$に挿入した時の$ boldsymbol{x}_i$に関する２乗距離の減少に等しい．よって$ eqref{kmeans_square_sum}$の減少は$ sum_iG{ij}$に等しいので， . I=(1,…,1) boldsymbol{I}=(1, ldots,1)I=(1,…,1) . を考えれば，$0$長線分を$ boldsymbol{x}_j$に挿入した時の$ eqref{kmeans_square_sum}$の減少は$ boldsymbol{I}G$の第$i$成分に等しい． . 線分の結合 . グラフ$G=(V,E)$を考える．ただし，$V$は$k$本の線分の$2k$個の端点である．更に，$k$本の線分と対応する辺全てを含むような$A subset E$を考える．全ての頂点を１度ずつつ通過する経路をハミルトニアン経路(HP)と呼ぶ．HPは辺の集合$P subset E$と考えることができる．線分を結合して多角形のPrincipal Curveを作るために，コストを最小にするようなHP $A subset P subset E$（線分に対応する辺を全て含み，かつ全頂点を一度だけ通る経路）を求めたい．経路$P$のコストは . l(P)+λa(P)l(P)+ lambda a(P)l(P)+λa(P) . とする．$l(P)$は経路の長さの総和である．辺$e=(v_i,v_j)$の長さは . l(e)=∥vi−vj∥l(e)= |v_i-v_j |l(e)=∥vi​−vj​∥ . とする．$a(P)$は隣接する角度の和である．$ lambda$を調節することによって，辺の向きが変わることに対するペナルティの重み付けを調節する． . HPを作るためgreedy algorithmで考える．sub-HP $P_i$と$P_j$をそれぞれの頂点$v_i$，$v_j$で結ぶとする．$e=(v_i,v_j)$として，新しくできるsub-HPのコストは元のsub-HPのコストの和と$l(e)+ lambda a(e)$の合計になる．全ての辺$e in(E backslash A)$に対し，コスト$c(e)=l(e)+ lambda a(e)$を計算しておく．アルゴリズムは以下のようになる： . k個のsub-HP $A$から始める． | ２個以上のsub-HPがある限り続ける． | $i neq j$として，２個のsub-HP $P_i$と$P_j$を$c(e)$が最小となるような辺$e in(E-A)$で結ぶ． これによって，折れ線が得られる． | 目的関数 . 最適な$k$を求めるため，データの対数尤度を最大とする折れ線を考える．この折れ線の長さを$l$とし，$ boldsymbol{f}:[0,l] to boldsymbol{R}^d$で表されるとする．簡単のために，$t in[0,l]$に対し，$p(t)= frac{1}{l}$，$p( boldsymbol{x} mid t)$は正規分布とする．よって，各データ点の対数尤度への寄与は負号をかけて . −log⁡p(x)=−log⁡∫t∈[0,l]p(x∣t)p(t) dt=log⁡l+c1−log⁡∫t∈[0,l]exp⁡(−∥x−f(t)∥22σ2) dt begin{aligned} &amp;- log p( boldsymbol{x}) &amp;= - log int_{t in[0,l]}p( boldsymbol{x} mid t)p(t) ,dt &amp;= log l+c_1- log int_{t in[0,l]} exp left(- frac{ | boldsymbol{x}- boldsymbol{f}(t) |^2}{2 sigma^2} right) ,dt end{aligned}​−logp(x)=−log∫t∈[0,l]​p(x∣t)p(t)dt=logl+c1​−log∫t∈[0,l]​exp(−2σ2∥x−f(t)∥2​)dt​ . となる．ここで，図のように距離を取ると， . ∥x−f(t)∥2=d⊥(f(t),x)+d∥(f(t),x)=d⊥(s,x)+d∥(f(t),x). begin{aligned} | boldsymbol{x}- boldsymbol{f}(t) |^2 &amp;= d_ perp( boldsymbol{f}(t), boldsymbol{x})+d_ parallel( boldsymbol{f}(t), boldsymbol{x}) &amp;= d_ perp( boldsymbol{s}, boldsymbol{x})+d_ parallel( boldsymbol{f}(t), boldsymbol{x}). end{aligned}∥x−f(t)∥2​=d⊥​(f(t),x)+d∥​(f(t),x)=d⊥​(s,x)+d∥​(f(t),x).​ . よって，$ eqref{k-segment_log_likelihood}$の最後の項は次のように書ける： . d⊥(s,x)22σ2−log⁡∫t∈[0,l]exp⁡(−d∥(f(t),x)22σ2) dt. frac{d_ perp( boldsymbol{s}, boldsymbol{x})^2}{2 sigma^2}- log int_{t in[0,l]} exp left(- frac{d_ parallel( boldsymbol{f}(t), boldsymbol{x})^2}{2 sigma^2} right) ,dt.2σ2d⊥​(s,x)2​−log∫t∈[0,l]​exp(−2σ2d∥​(f(t),x)2​)dt. . $d_ parallel( boldsymbol{s}, boldsymbol{x})= inf_{t in[0,l]}d_ parallel( boldsymbol{f}(t), boldsymbol{x})$とする．$ eqref{k-segment_log_likelihood}$は次のように近似できる： . log⁡l+c1+d⊥(s,x)22σ2−log⁡∫exp⁡(−d∥(f(t),x)22σ2) dt∼log⁡l+c1+d⊥(s,x)22σ2−log⁡∫exp⁡(−d∥(s,x)22σ2) dt=log⁡l+d(s,x)22σ2+c. begin{aligned} &amp; log l+c_1+ frac{d_ perp( boldsymbol{s}, boldsymbol{x})^2}{2 sigma^2}- log int exp left(- frac{d_ parallel( boldsymbol{f}(t), boldsymbol{x})^2}{2 sigma^2} right) ,dt &amp; sim log l+c_1+ frac{d_ perp( boldsymbol{s}, boldsymbol{x})^2}{2 sigma^2}- log int exp left(- frac{d_ parallel( boldsymbol{s}, boldsymbol{x})^2}{2 sigma^2} right) ,dt &amp;= log l + frac{d( boldsymbol{s}, boldsymbol{x})^2}{2 sigma^2} + c. end{aligned}​logl+c1​+2σ2d⊥​(s,x)2​−log∫exp(−2σ2d∥​(f(t),x)2​)dt∼logl+c1​+2σ2d⊥​(s,x)2​−log∫exp(−2σ2d∥​(s,x)2​)dt=logl+2σ2d(s,x)2​+c.​ . よって，全点についての和を取れば，対数尤度に負号をかけたものは . nlog⁡l+∑i=1k∑x∈Vid(si,x)22σ2n log l+ sum_{i=1}^k sum_{ boldsymbol{x} in V_i} frac{d( boldsymbol{s}_i, boldsymbol{x})^2}{2 sigma^2}nlogl+i=1∑k​x∈Vi​∑​2σ2d(si​,x)2​ . となる．$ eqref{k-segment_log_likelihood_sum}$が初めて極小となる$k$が最適な$k$となる． .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/statistics/2019/09/29/princurve.html",
            "relUrl": "/statistics/2019/09/29/princurve.html",
            "date": " • Sep 29, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "Boltzmann Generatorsの解説",
            "content": "Boltzmann Generatorsの論文についての解説，というかざっくりしたメモ．機械学習とかニューラルネットワークとかは完全にエアプなんで間違ってたらゴメンね．RealNVPとかGANの話知ってると楽かも． . Boltzmann分布と正規分布を対応させる写像を，機械学習で作る．ただし，Boltzmann分布での低エネルギー状態が正規分布で原点にあるとする． . 例えばタンパク質がopenとcloseの２形態を取ったとする．Boltzmann分布からサンプリングしようとすれば，普通はある安定な状態（今回はopenとする）から始める．openの状態から，タンパク質の側鎖の角度や長さなどに微妙な摂動を加えていって，エネルギーが低下するようなものをどんどん選んでいく．この時，openとcloseの間に準安定な領域が存在すれば，そこから抜け出せなくなって，サンプリングが詰む． . そこで，Boltzmann分布と結ばれた正規分布を考える．この正規分布では，低エネルギー状態は原点付近に存在する．正規分布の方でopenの点（原点付近）からスタートして，摂動を加えてサンプリング（原点付近）する．close（原点付近）が見つかるまでサンプリングできる． . Boltzmann Generatorとは . ある系のconfigrationが$ boldsymbol{x}$で，その時のエネルギーが$U( boldsymbol{x})$で表されるとすると，系がconfigration $ boldsymbol{x}$を取る確率はBoltzmann分布に従い， . exp⁡(−U(x)kT0)=exp⁡(−u(x)) begin{aligned} exp left(- frac{U( boldsymbol{x})}{kT_0} right)= exp(-u( boldsymbol{x})) end{aligned}exp(−kT0​U(x)​)=exp(−u(x))​ . に比例する．ただ，全ての状態を数えるのは非現実的なので，平衡状態からニューラルネットワークを使って，one-shotでサンプリングすることを目標とする．latent spaceとしては，latent variable$ boldsymbol{z}$の正規分布$p_Z( boldsymbol{z})$を使う．与えられたBoltzmann分布に近いconfigurationの確率分布$p_X( boldsymbol{x})$と，$p_Z( boldsymbol{z})$と$p_X( boldsymbol{x})$の間の全単射$F_{zx}: boldsymbol{z} mapsto boldsymbol{x}$を求めることが目標となる． . 実際にBoltzmann分布でのサンプルを得るには，$p_X( boldsymbol{x})$を重みづけすることが必要である．この場合， . w(x)=e−u(x)pX(x) begin{aligned} w( boldsymbol{x})= frac{e^{-u( boldsymbol{x})}}{p_X( boldsymbol{x})} end{aligned}w(x)=pX​(x)e−u(x)​​ . で重みづけすればよい． . 写像の構成 . configration variable $ boldsymbol{x}$とlatent variable $z$の間には，次のような関係があるとする： . z=Fxz(x;θ)x=Fzx(z;θ). begin{aligned} boldsymbol{z} &amp;= boldsymbol{F} _ {xz}( boldsymbol{x}; theta) boldsymbol{x} &amp;= boldsymbol{F} _ {zx}( boldsymbol{z}; theta). end{aligned}zx​=Fxz​(x;θ)=Fzx​(z;θ).​ . もちろん， $ boldsymbol{F} _ {xz}= boldsymbol{F} _ {zx}{}^{-1}$ ．これらの変換のJacobi行列を求めれば， . Jzx(z;θ)=∂x∂z=(∂Fzx∂z1,…,∂Fzx∂zn)(z;θ)Jxz(x;θ)=∂z∂x=(∂Fxz∂x1,…,∂Fxz∂xn)(x;θ) begin{aligned} J_{zx}( boldsymbol{z}; theta) &amp;= frac{ partial boldsymbol{x}}{ partial boldsymbol{z}} = left( frac{ partial boldsymbol{F} _ {zx}}{ partial z_1}, dots, frac{ partial boldsymbol{F} _ {zx}}{ partial z_n} right)( boldsymbol{z}; theta) J_{xz}( boldsymbol{x}; theta) &amp;= frac{ partial boldsymbol{z}}{ partial boldsymbol{x}} = left( frac{ partial boldsymbol{F} _ {xz}}{ partial x_1}, dots, frac{ partial boldsymbol{F} _ {xz}}{ partial x_n} right)( boldsymbol{x}; theta) end{aligned}Jzx​(z;θ)Jxz​(x;θ)​=∂z∂x​=(∂z1​∂Fzx​​,…,∂zn​∂Fzx​​)(z;θ)=∂x∂z​=(∂x1​∂Fxz​​,…,∂xn​∂Fxz​​)(x;θ)​ . となり，さらにJacobianを . Rxz(x;θ)=∣det⁡Jxz(x;θ)∣Rzx(x;θ)=∣det⁡Jzx(z;θ)∣ begin{aligned} R_{xz}( boldsymbol{x}; theta) &amp;= mid det J_{xz}( boldsymbol{x}; theta) mid R_{zx}( boldsymbol{x}; theta) &amp;= mid det J_{zx}( boldsymbol{z}; theta) mid . end{aligned}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;Rxz​(x;θ)Rzx​(x;θ)​=∣detJxz​(x;θ)∣=∣detJzx​(z;θ)∣​&lt;/span&gt;&lt;/span&gt; . とする． . 写像を$ boldsymbol{x}$から$ boldsymbol{z}$への体積非保存の「流れ」として考えれば，$p_X( boldsymbol{x}) ,dx=p_Z( boldsymbol{z}) ,dz$が成立するので， . pX(x;θ)=pZ(z)∣∂z∂x∣=pZ(Fxz(x;θ))Rxz(x;θ)pZ(z;θ)=pX(x)∣∂x∂z∣=pX(Fzx(z;θ))Rzx(z;θ) begin{aligned} p_X( boldsymbol{x}; theta) &amp;= p_Z( boldsymbol{z}) mid frac{ partial boldsymbol{z}}{ partial{x}} mid &amp;= p_Z( boldsymbol{F} _ {xz}( boldsymbol{x}; theta))R_{xz}( boldsymbol{x}; theta) p_Z( boldsymbol{z}; theta) &amp;= p_X( boldsymbol{x}) mid frac{ partial boldsymbol{x}}{ partial{z}} mid &amp;= p_X( boldsymbol{F} _ {zx}( boldsymbol{z}; theta))R_{zx}( boldsymbol{z}; theta) end{aligned}pX​(x;θ)pZ​(z;θ)​=pZ​(z)∣∂x∂z​∣=pZ​(Fxz​(x;θ))Rxz​(x;θ)=pX​(x)∣∂z∂x​∣=pX​(Fzx​(z;θ))Rzx​(z;θ)​ . となる．右辺の確率分布は$ theta$に依存しない（系がはじめから有している確率分布）が，左辺の計算結果は$ theta$に依存する（Boltzmann Generatorによって得られた確率分布）． . RealNVPによるニューラルネットワーク構成 . $F_{zx}$を直接求めることは困難なので，アフィンカップリングレイヤ（入出力の一部が比較的簡単な関係にある全単射）を考える．具体的には，$ boldsymbol{x}$を$( boldsymbol{x} _ 1, boldsymbol{x} _ 2)$，$ boldsymbol{z}$を$( boldsymbol{z} _ 1, boldsymbol{z} _ 2)$に分ける．これらについて，非線形変換を次のように定義する： . fxz(x1,x2):{z1=x1z2=x2⊙exp⁡(S(x1;θ))+T(x1;θ);log⁡Rxz=∑iSi(x1;θ). begin{aligned} &amp; boldsymbol{f} _ {xz}( boldsymbol{x_1}, boldsymbol{x_2}) quad: begin{cases} boldsymbol{z} _ 1 = boldsymbol{x} _ 1 boldsymbol{z} _ 2 = boldsymbol{x} _ 2 odot exp( boldsymbol{S}( boldsymbol{x} _ 1; theta))+ boldsymbol{T}( boldsymbol{x} _ 1; theta) end{cases}; &amp; log R_{xz} = sum_iS_i( boldsymbol{x} _ 1; theta). end{aligned}​fxz​(x1​,x2​):{z1​=x1​z2​=x2​⊙exp(S(x1​;θ))+T(x1​;θ)​;logRxz​=i∑​Si​(x1​;θ).​ . さらに，アフィンカップリングレイヤの逆写像は次の様になる： . fzx(z1,z2):{x1=z1x2=(z2−T(x1;θ))⊙exp⁡(−S(z1;θ));log⁡Rzx=−∑iSi(z1;θ). begin{aligned} &amp; boldsymbol{f} _ {zx}( boldsymbol{z_1}, boldsymbol{z_2}) quad: begin{cases} boldsymbol{x} _ 1 = boldsymbol{z} _ 1 boldsymbol{x} _ 2 = ( boldsymbol{z} _ 2- boldsymbol{T}( boldsymbol{x_1}; theta)) odot exp(- boldsymbol{S}( boldsymbol{z} _ 1; theta)) end{cases}; &amp; log R_{zx} = - sum_iS_i( boldsymbol{z} _ 1; theta). end{aligned}​fzx​(z1​,z2​):{x1​=z1​x2​=(z2​−T(x1​;θ))⊙exp(−S(z1​;θ))​;logRzx​=−i∑​Si​(z1​;θ).​ . このニューラルネットワークによって，合成写像$F_{zx}$が得られる． . 機械学習のtraining . Boltzmann Generatorでは，主に２つの機械学習：training by energyとtraining by exampleを使う． . training by energyでは， . latent spaceから正規分布$p_Z( boldsymbol{z})$に従って適当に$ boldsymbol{z}$を選ぶ | $ boldsymbol{F} _ {zx}: boldsymbol{z} mapsto boldsymbol{x}$を使って$p_X(x)$を計算する | 生成された確率分布$p_X(x)$と目標のBoltzmann分布$e^{-u( boldsymbol{x})}$との差をロス JKL=Ez[u(Fzx(z))−log⁡Rzx(z)] begin{aligned} J_ text{KL}=E_{ boldsymbol{z}}[u( boldsymbol{F} _ {zx}( boldsymbol{z}))- log R_{zx}( boldsymbol{z})] end{aligned}JKL​=Ez​[u(Fzx​(z))−logRzx​(z)]​ で評価する． | $J_ text{KL}$を小さくすることが目標となる． . training by exampleでは， . シミュレーションや観測結果から，実際に得られた$ boldsymbol{x}$を用意する．つまり，$e^{-u( boldsymbol{x})}$が大きい． | $F_{xz}: boldsymbol{x} mapsto boldsymbol{z}$で，これを$ boldsymbol{z}$に変換する．この$ boldsymbol{z}$が正規分布$p_Z( boldsymbol{z})$から選ばれやすければよい．それは，次のロス | JML=Ex[12∥Fxz(x)∥2−log⁡Rxz(x)](1) begin{aligned} J_ text{ML}=E_{ boldsymbol{x}} left[ frac{1}{2} | boldsymbol{F} _ {xz}( boldsymbol{x}) |^2- log R_{xz}( boldsymbol{x}) right] tag{1} end{aligned}JML​=Ex​[21​∥Fxz​(x)∥2−logRxz​(x)]​(1) . を最小化することに等しい． . $J_ text{ML}$の第１項は正規分布に対応する調和振動子のエネルギーを表す． . 一般の場合に，Boltzmann Generatorでは合計のロス . J=wMLJML+wKLJKL+wMLJML begin{aligned} J=w_ text{ML}J_ text{ML}+w_ text{KL}J_ text{KL}+w_ text{ML}J_ text{ML} end{aligned}J=wML​JML​+wKL​JKL​+wML​JML​​ . を最小とすることが目標になる． . KLロス関数 . 以下では，真の確率分布と学習によって得られた確率分布を区別する．すなわち，$ boldsymbol{x}$の真の確率分布$ mu_X( boldsymbol{x})$は分配関数を$Z_X$とするBoltzmann分布である： . μX(x)=1ZXe−u(x). begin{aligned} mu_X( boldsymbol{x})= frac{1}{Z_X}e^{-u( boldsymbol{x})}. end{aligned}μX​(x)=ZX​1​e−u(x).​ . また，$ boldsymbol{z}$の真の確率分布$ mu_Z( boldsymbol{z})$は正規分布である： . μZ(z)=1ZZexp⁡(−z22σ2). begin{aligned} mu_Z( boldsymbol{z})= frac{1}{Z_Z} exp left(- frac{ boldsymbol{z}^2}{2 sigma^2} right). end{aligned}μZ​(z)=ZZ​1​exp(−2σ2z2​).​ . ただし，１つの温度しか考えない場合は$ sigma=1$とする．また， . uZ(z)=−log⁡μZ(z)=12σ2z2+C. begin{aligned} u_Z( boldsymbol{z})=- log mu_Z( boldsymbol{z})= frac{1}{2 sigma^2} boldsymbol{z}^2+C. end{aligned}uZ​(z)=−logμZ​(z)=2σ21​z2+C.​ . とする．学習による$ boldsymbol{x}$，$ boldsymbol{z}$の確率分布をそれぞれ$q_X( boldsymbol{x})$，$q_Z( boldsymbol{z})$とする．これらについては，$ eqref{pXpZ_Jacobian}$から， . qZ(z;θ)=μX(Fzx(z;θ))Rzx(z)qX(x;θ)=μZ(Fxz(x;θ))Rxz(x) begin{aligned} q_Z( boldsymbol{z}; theta) &amp;= mu_X( boldsymbol{F} _ {zx}( boldsymbol{z}; theta))R_{zx}( boldsymbol{z}) q_X( boldsymbol{x}; theta) &amp;= mu_Z( boldsymbol{F} _ {xz}( boldsymbol{x}; theta))R_{xz}( boldsymbol{x}) end{aligned}qZ​(z;θ)qX​(x;θ)​=μX​(Fzx​(z;θ))Rzx​(z)=μZ​(Fxz​(x;θ))Rxz​(x)​ . が成り立つ． . 確率分布$p$，$q$に対して，Kullback-Leibler divergenceは次の式で定義される： . KL(q∥p)=∫q(x)[log⁡q(x)−log⁡p(x)] dx=−Hq(x)−∫q(x)log⁡p(x) dx. begin{aligned} text{KL}(q | p) &amp;= int q( boldsymbol{x})[ log q( boldsymbol{x})- log p( boldsymbol{x})] ,d boldsymbol{x} &amp;= -H_q( boldsymbol{x})- int q( boldsymbol{x}) log p( boldsymbol{x}) ,d boldsymbol{x}. end{aligned}KL(q∥p)​=∫q(x)[logq(x)−logp(x)]dx=−Hq​(x)−∫q(x)logp(x)dx.​ . よって，$ mu_Z$と$q_Z$のKullback-Leibler divergenceは . KLθ(μZ∥qZ)=−HZ−∫μZ(z)log⁡q(z) dz=−HZ−∫μZ(z)[log⁡μX(Fzx(z;θ)+log⁡Rzx(z;θ))] dz=−HZ−∫μZ(z)[log⁡exp⁡(−u(Fzx(z;θ)))ZX+log⁡Rzx(z;θ))] dz=−HZ+log⁡ZX+Ez∼μZ(Z)[u(Fzx(z;θ))−log⁡Rzx(z;θ)] begin{aligned} &amp; text{KL} _ theta( mu_Z | q_Z) = -H_Z- int mu_Z( boldsymbol{z}) log q( boldsymbol{z}) ,d boldsymbol{z} &amp;= -H_Z- int mu_Z( boldsymbol{z})[ log mu_X( boldsymbol{F} _ {zx}( boldsymbol{z}; theta) &amp; qquad qquad+ log R_{zx}( boldsymbol{z}; theta))] ,d boldsymbol{z} &amp;= -H_Z- int mu_Z( boldsymbol{z}) biggl[ log frac{ exp(-u( boldsymbol{F} _ {zx}( boldsymbol{z}; theta)))}{Z_X} &amp; qquad qquad+ log R_{zx}( boldsymbol{z}; theta)) biggr] ,d boldsymbol{z} &amp;= -H_Z+ log Z_X &amp; qquad+E_{ boldsymbol{z} sim mu_Z( boldsymbol{Z})}[u(F_{zx}( boldsymbol{z}; theta))- log R_{zx}( boldsymbol{z}; theta)] end{aligned}​KLθ​(μZ​∥qZ​)=−HZ​−∫μZ​(z)logq(z)dz=−HZ​−∫μZ​(z)[logμX​(Fzx​(z;θ)+logRzx​(z;θ))]dz=−HZ​−∫μZ​(z)[logZX​exp(−u(Fzx​(z;θ)))​+logRzx​(z;θ))]dz=−HZ​+logZX​+Ez∼μZ​(Z)​[u(Fzx​(z;θ))−logRzx​(z;θ)]​ . この第３項が，$ eqref{training_energy}$の$J_ text{KL}$である．また， . HX=−∫qX(x;θ)log⁡qX(x;θ) dx=−∫μZ(Fxz(x;θ))Rxz(x)×log⁡(μZ(Fxz(x;θ))Rxz(x)) dx=−∫μZ(Fxz(x;θ))∣∂z∂x∣×log⁡(μZ(Fxz(x;θ))Rxz(x)) dx=−∫μZ(z)log⁡(μZ(z)Rzx−1(z)) dz=−∫μZ(z)log⁡μZ(z) dz+Ez∼μZ(z) begin{aligned} H_X &amp;= - int q_X( boldsymbol{x}; theta) log q_X( boldsymbol{x}; theta) ,d boldsymbol{x} &amp;= - int mu_Z(F_{xz}( boldsymbol{x}; theta))R_{xz}( boldsymbol{x}) &amp; qquad qquad times log( mu_Z(F_{xz}( boldsymbol{x}; theta))R_{xz}( boldsymbol{x})) ,d boldsymbol{x} &amp;= - int mu_Z(F_{xz}( boldsymbol{x}; theta)) mid frac{ partial boldsymbol{z}}{ partial boldsymbol{x}} mid &amp; qquad qquad times log( mu_Z(F_{xz}( boldsymbol{x}; theta))R_{xz}( boldsymbol{x})) ,d boldsymbol{x} &amp;= - int mu_Z( boldsymbol{z}) log( mu_Z( boldsymbol{z})R_{zx}{}^{-1}( boldsymbol{z})) ,d boldsymbol{z} &amp;= - int mu_Z( boldsymbol{z}) log mu_Z( boldsymbol{z}) ,d boldsymbol{z}+E_{ boldsymbol{z} sim mu_Z( boldsymbol{z})} end{aligned}HX​​=−∫qX​(x;θ)logqX​(x;θ)dx=−∫μZ​(Fxz​(x;θ))Rxz​(x)×log(μZ​(Fxz​(x;θ))Rxz​(x))dx=−∫μZ​(Fxz​(x;θ))∣∂x∂z​∣×log(μZ​(Fxz​(x;θ))Rxz​(x))dx=−∫μZ​(z)log(μZ​(z)Rzx​−1(z))dz=−∫μZ​(z)logμZ​(z)dz+Ez∼μZ​(z)​​ . なので， . KLθ(μZ∥qZ)=−HZ+log⁡ZX+Ez∼μZ(Z)[u(Fzx(z;θ))−log⁡Rzx(z;θ)]=−HX+log⁡ZX+Ez∼μZ(z)[u(Fzx(z;θ))]=−HX+log⁡ZX+Ex∼μX(x)[u(x)]. begin{aligned} &amp; text{KL} _ theta( mu_Z | q_Z) &amp;= -H_Z+ log Z_X &amp; qquad+E_{ boldsymbol{z} sim mu_Z( boldsymbol{Z})}[u(F_{zx}( boldsymbol{z}; theta))- log R_{zx}( boldsymbol{z}; theta)] &amp;= -H_X+ log Z_X+E_{ boldsymbol{z} sim mu_Z( boldsymbol{z})}[u( boldsymbol{F} _ {zx}( boldsymbol{z}; theta))] &amp;= -H_X+ log Z_X+E_{ boldsymbol{x} sim mu_X( boldsymbol{x})}[u( boldsymbol{x})]. end{aligned}​KLθ​(μZ​∥qZ​)=−HZ​+logZX​+Ez∼μZ​(Z)​[u(Fzx​(z;θ))−logRzx​(z;θ)]=−HX​+logZX​+Ez∼μZ​(z)​[u(Fzx​(z;θ))]=−HX​+logZX​+Ex∼μX​(x)​[u(x)].​ . 同様に， . KLθ(qX∥μX)=−HX−∫qX(x;θ)log⁡μX(x) dx=−HX−∫μZ(Fxz(x;θ))Rxz(x)log⁡μX(x) dx=−HX+log⁡ZX+Ez∼μZ(z)[u(Fzx(z;θ))]=−HX+log⁡ZX+Ex∼μX(x)[u(x)]. begin{aligned} &amp; text{KL} _ theta(q_X | mu_X) &amp;= -H_X- int q_X( boldsymbol{x}; theta) log mu_X( boldsymbol{x}) ,d boldsymbol{x} &amp;= -H_X- int mu_Z( boldsymbol{F} _ {xz}( boldsymbol{x}; theta))R_{xz}( boldsymbol{x}) log mu_X( boldsymbol{x}) ,d boldsymbol{x} &amp;= -H_X+ log Z_X+E_{ boldsymbol{z} sim mu_Z( boldsymbol{z})}[u( boldsymbol{F} _ {zx}( boldsymbol{z}; theta))] &amp;= -H_X+ log Z_X+E_{ boldsymbol{x} sim mu_X( boldsymbol{x})}[u( boldsymbol{x})]. end{aligned}​KLθ​(qX​∥μX​)=−HX​−∫qX​(x;θ)logμX​(x)dx=−HX​−∫μZ​(Fxz​(x;θ))Rxz​(x)logμX​(x)dx=−HX​+logZX​+Ez∼μZ​(z)​[u(Fzx​(z;θ))]=−HX​+logZX​+Ex∼μX​(x)​[u(x)].​ . 以上から， . KLθ(μZ∥qZ)=KLθ(qX∥μX). begin{aligned} text{KL} _ theta( mu_Z | q_Z)= text{KL} _ theta(q_X | mu_X). end{aligned}KLθ​(μZ​∥qZ​)=KLθ​(qX​∥μX​).​ . さらに，$U=E_{ boldsymbol{x} sim mu_X( boldsymbol{x})}[u( boldsymbol{x})]$として， . JKL=U−HX+HZ begin{aligned} J_ text{KL}=U-H_X+H_Z end{aligned}JKL​=U−HX​+HZ​​ . が得られる．この式を参考にすれば，$ eqref{training_energy}$で定義した$J_ text{KL}$の表式において，第１項は系の内部エネルギーを，第２項は自由エネルギーに対するエントロピーの寄与を表すことが分かる． . $ eqref{re-weighting}$でも述べたように，重み付けは， . w(x)=μX(x)qX(x)=qZ(z)μZ(z)=ZXexp⁡(−u(x))μZ(z)Rxz(x)∝exp⁡(−u(Fzx(x;θ))+uZ(z)+log⁡Rzx(z)) begin{aligned} w( boldsymbol{x}) &amp;= frac{ mu_X( boldsymbol{x})}{q_X( boldsymbol{x})}= frac{q_Z( boldsymbol{z})}{ mu_Z( boldsymbol{z})} &amp;= frac{Z_X exp(-u( boldsymbol{x}))}{ mu_Z( boldsymbol{z})R_{xz}( boldsymbol{x})} &amp; propto exp(-u( boldsymbol{F} _ {zx}( boldsymbol{x}; theta))+u_Z( boldsymbol{z})+ log R_{zx}( boldsymbol{z})) end{aligned}w(x)​=qX​(x)μX​(x)​=μZ​(z)qZ​(z)​=μZ​(z)Rxz​(x)ZX​exp(−u(x))​∝exp(−u(Fzx​(x;θ))+uZ​(z)+logRzx​(z))​ . となる．これによって，平衡状態でのある量$A( boldsymbol{x})$の期待値は . E(A)≈∑i=1Nw(xA(x))∑i=1Nw(w) begin{aligned} E(A) approx frac{ sum_{i=1}^Nw( boldsymbol{x}A( boldsymbol{x}))}{ sum_{i=1}^Nw( boldsymbol{w})} end{aligned}E(A)≈∑i=1N​w(w)∑i=1N​w(xA(x))​​ . で与えられる．さらに， . min⁡KLθ(μZ∥qZ)=min⁡Ez∼μZ(Z)[u(Fzx(z;θ))−log⁡Rzx(z;θ)]=min⁡Ez∼μZ(Z)[u(Fzx(x;θ))−uZ(z)−log⁡Rzx(z)]=min⁡Ez∼μZ(Z)[log⁡μZ(z)−log⁡qZ(z;θ)]=max⁡Ez∼μZ(Z)[log⁡w(x)]. begin{aligned} &amp; min text{KL} _ theta( mu_Z | q_Z) &amp;= min E_{ boldsymbol{z} sim mu_Z( boldsymbol{Z})}[u( boldsymbol{F} _ {zx}( boldsymbol{z}; theta))- log R_{zx}( boldsymbol{z}; theta)] &amp;= min E_{ boldsymbol{z} sim mu_Z( boldsymbol{Z})}[u( boldsymbol{F} _ {zx}( boldsymbol{x}; theta))-u_Z( boldsymbol{z})- log R_{zx}( boldsymbol{z})] &amp;= min E_{ boldsymbol{z} sim mu_Z( boldsymbol{Z})}[ log mu_Z( boldsymbol{z})- log q_Z( boldsymbol{z}; theta)] &amp;= max E_{ boldsymbol{z} sim mu_Z( boldsymbol{Z})}[ log w( boldsymbol{x})]. end{aligned}​minKLθ​(μZ​∥qZ​)=minEz∼μZ​(Z)​[u(Fzx​(z;θ))−logRzx​(z;θ)]=minEz∼μZ​(Z)​[u(Fzx​(x;θ))−uZ​(z)−logRzx​(z)]=minEz∼μZ​(Z)​[logμZ​(z)−logqZ​(z;θ)]=maxEz∼μZ​(Z)​[logw(x)].​ . MLロス関数 . $ text{KL} _ theta( mu_Z | q_Z)= text{KL} _ theta(q_X| mu_X)$であったが，次のKLロスを考える： . KLθ(μX∥qX)=−HX−∫μX(x)log⁡qX(x;θ) dx=−HX−∫μX(x)[log⁡μZ(Fxz(x;θ))+log⁡Rxz(x)] dx=−HX+log⁡ZZ+Ex∼μX(x)[12σ2∥Fxz(x;θ)∥2−log⁡Rxz(x)]. begin{aligned} &amp; text{KL} _ theta( mu_X | q_X) = -H_X- int mu_X( boldsymbol{x}) log q_X( boldsymbol{x}; theta) ,d boldsymbol{x} &amp;= -H_X- int mu_X( boldsymbol{x})[ log mu_Z( boldsymbol{F} _ {xz}( boldsymbol{x}; theta))+ log R_{xz}( boldsymbol{x})] ,d boldsymbol{x} &amp;= -H_X+ log Z_Z &amp; qquad+E_{ boldsymbol{x} sim mu_X( boldsymbol{x})} left[ frac{1}{2 sigma^2} | boldsymbol{F} _ {xz}( boldsymbol{x}; theta) |^2- log R_{xz}( boldsymbol{x}) right]. end{aligned}​KLθ​(μX​∥qX​)=−HX​−∫μX​(x)logqX​(x;θ)dx=−HX​−∫μX​(x)[logμZ​(Fxz​(x;θ))+logRxz​(x)]dx=−HX​+logZZ​+Ex∼μX​(x)​[2σ21​∥Fxz​(x;θ)∥2−logRxz​(x)].​ . training by exampleを実行する場合，そのexampleが$ mu_X( boldsymbol{x})$に従ったものかは分からないので，このロスを評価することは困難である． . 代わりに，サンプルの分布$ rho( boldsymbol{x})$を使って， . JML=−Ex∼ρ[log⁡qX(x;θ)]=Ex∼ρ(x)[12σ2∥Fxz(x;θ)∥2−log⁡Rxz(x)] begin{aligned} J_ text{ML} &amp;= -E_{ boldsymbol{x} sim rho}[ log q_X( boldsymbol{x}; theta)] &amp;= E_{ boldsymbol{x} sim rho( boldsymbol{x})} left[ frac{1}{2 sigma^2} | boldsymbol{F} _ {xz}( boldsymbol{x}; theta) |^2- log R_{xz}( boldsymbol{x}) right] end{aligned}JML​​=−Ex∼ρ​[logqX​(x;θ)]=Ex∼ρ(x)​[2σ21​∥Fxz​(x;θ)∥2−logRxz​(x)]​ . を考える．これを最小にすることは，サンプル$ rho( boldsymbol{x})$が正規分布で選ばれる確率が最大になることに対応する．通常は$ sigma=1$なので，$(1)$が得られる． . RCロス関数 . configration spaceで定義されたreaction cordinate $r( boldsymbol{x})$を使う場合は，次のRCロス関数を考える： . JRC=∫p(r(x))log⁡p(r(x)) dr(x)=Ex∼qX(x)log⁡p(r(x)). begin{aligned} J_ text{RC} &amp;= int p(r( boldsymbol{x})) log p(r( boldsymbol{x})) ,dr( boldsymbol{x}) &amp;= E_{ boldsymbol{x} sim q_X( boldsymbol{x})} log p(r( boldsymbol{x})). end{aligned}JRC​​=∫p(r(x))logp(r(x))dr(x)=Ex∼qX​(x)​logp(r(x)).​ . $p(r( boldsymbol{x}))$は，上限と下限でのカーネル密度推定として計算される． . 複数の温度を扱う場合 . 基準の温度の$ tau_k$倍の温度について計算する場合，$u( boldsymbol{x})$は$1/ tau_k$倍，$ sigma^2$は$ tau_k$倍とする． .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/machine-learning/2019/09/25/boltzmanngenerators.html",
            "relUrl": "/machine-learning/2019/09/25/boltzmanngenerators.html",
            "date": " • Sep 25, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "Palantirの解説",
            "content": "Palantirの論文の解説．元論文はほとんど式の解説とかなくて，ほぼsupplementaryに数式による詳細が載っている．内容としては，コンピュータにヤバヤバな計算をさせる．サンプルの細胞の経路を複数調べたりできるけど，細胞の数を減らしたりする操作（scEpathならメタ細胞を作ったりした）が無いから，計算量はお察し． . データ幾何学（多様体） . データが２次元曲面に分布しているとする．これは３次元空間$ boldsymbol{R}^3$（多様体）に埋め込まれた部分多様体$L^2$（もう少し詳しく言えば超曲面）と考えられる．データを解析する場合は，３次元空間でやるより，これを２次元に直した方が圧倒的に効率が良い．よって，次元を落とすわけだが，この際にデータ間の距離を不変にしたい．この場合，距離は部分多様体での測地線距離とする． . diffusion mapは，データを低次元のユークリッド空間に埋め込む方法．Palantirではdiffusion mapを使う． . Nearest Neighbor Graph . nearest neighbor graphは似た細胞同士を結んだグラフ．当然のことながら，この長い経路は細胞の成長におけるトラジェクトリーに一致する．以下では，$k$-nearest neighbor graph，つまり$k$個の近い細胞を結んだグラフ$G_X in boldsymbol{R}^{N times N}$を考える． . $N$個の細胞と$M$個の遺伝子のデータとして$X in boldsymbol{R}^{N times M}$が与えられたとする．$ boldsymbol{R}^M$での距離をaffinityに変換するため，細胞$i$と細胞$j$について，次のadaptive Gaussian kernelを定義する： . K(xi,xj)=12π(σi+σj)exp⁡(−12(xi−xj)t(xi+xj)σi−σj).(1)K( boldsymbol{x} _ i, boldsymbol{x} _ j) = frac{1}{ sqrt{2 pi( sigma_i+ sigma_j)}} exp left(- frac{1}{2} frac{( boldsymbol{x} _ i- boldsymbol{x} _ j)^t( boldsymbol{x} _ i+ boldsymbol{x} _ j)}{ sigma_i- sigma_j} right). tag{1}K(xi​,xj​)=2π(σi​+σj​) . ​1​exp(−21​σi​−σj​(xi​−xj​)t(xi​+xj​)​).(1) . ただし，$ sigma_i$は，細胞$i$について，$l$番目に近い細胞までの距離である($l&lt;k$)． . $ boldsymbol{x} _ i$ は細胞$i$での遺伝子発現を表す（行）ベクトルである．これによって，affinity行列 $K in boldsymbol{R}^{N times N}$ が得られる．さらに，$K$のラプラシアン行列$T in boldsymbol{R}^{N times N}$を計算する（$X$上での$k(x,y)$の$y$についての和で$k(x,y)$を割る）．$T_{ij}$は規格化されており，細胞$i$から細胞$j$へ１回で遷移できる確率を表わしている（Markov過程での遷移確率行列）． . $T$の固有ベクトルをdiffusion componentとする．さらに，対応する固有値を$ lambda_1, lambda_2, ldots$とする．diffusion componentの順序は固有値が降順になるようにする． . 元々のグラフ$G_X$から，ノイズを減らしたグラフ$G_E in boldsymbol{R}^{N times N}$を考える．pseudotimeは，このグラフ$G_E$での最短経路の長さによって計算される．また，pseudotimeを決める際，waypointを定めておく．pseudotimeはwaypointによる重み付けスコアに基づいて順番が決まる．すなわち，細胞に最も近いwaypointが，細胞の位置を決める際に最も高い重み付けを与える．多様体全体にwaypointが存在するのが望ましい（詳しくは後述）． . 多様体$E in R^{N times L}$を考える($L&lt;M$)．つまり，diffusion componentを$L$番目まで取ってきて，$N$個の細胞について$L$種類のdiffusion componentに沿った遺伝子発現をデータにしている．細胞$i$と細胞$j$のdiffusion distanceを次のように定義する： . DDt(ei,ej)2=∑l=1Lλl2t(ei(l)−ej(l))2. begin{aligned} text{DD} _ t(e_i,e_j)^2= sum_{l=1}^L lambda_l{}^{2t}(e_i{}^{(l)}-e_j{}^{(l)})^2. end{aligned}DDt​(ei​,ej​)2=l=1∑L​λl​2t(ei​(l)−ej​(l))2.​ . $t$はグラフに沿った（ランダムウォーキングの）ステップの数，$e_i{}^{(l)}$はdiffusion component$l$に沿った細胞$i$の埋め込みである（$l$番目の固有ベクトルを基底とした場合の成分，つまり遺伝子発現）．さらに，multi-scale distanceを次のように定義する： . MS(ei,ej)2=∑t=1∞∑l=1Lλl2t(ei(l)−ej(l))2. begin{aligned} text{MS}(e_i,e_j)^2= sum_{t=1}^ infty sum_{l=1}^L lambda_l{}^{2t}(e_i{}^{(l)}-e_j{}^{(l)})^2. end{aligned}MS(ei​,ej​)2=t=1∑∞​l=1∑L​λl​2t(ei​(l)−ej​(l))2.​ . 定義から，$1&gt; lambda_1&gt; lambda_2&gt; cdots&gt; lambda_L&gt;0$なので，multi-scale distanceは次のように書くことができる： . MS(ei,ej)2=∑l=1L(λl1−λl)2(ei(l)−ej(l))2. begin{aligned} text{MS}(e_i,e_j)^2= sum_{l=1}^L left( frac{ lambda_l}{1- lambda_l} right)^2(e_i{}^{(l)}-e_j{}^{(l)})^2. end{aligned}MS(ei​,ej​)2=l=1∑L​(1−λl​λl​​)2(ei​(l)−ej​(l))2.​ . waypointの決定 . Max-min samplingは既に存在するwaypointとの最短距離の最大値を新しいwaypointとする．これによって，多様体全体にwaypointを置くことができる． . $ boldsymbol{E}^{(l)}$を$l$番目のdiffusion componentとする．まず，$ text{WS}^{(l)}$を$N$個の細胞から適当に１つ選んだ細胞とする（初期値）．$j in text{WS}^{(l)}$に対し，diffusion componentに沿った，waypointまでの距離は次のように計算される： . wdij=(ei(l)−ej(l))2. begin{aligned} text{wd} _ {ij}=(e_i{}^{(l)}-e_j{}^{(l)})^2. end{aligned}wdij​=(ei​(l)−ej​(l))2.​ . 細胞$i$に対し，waypoint distanceの最小値は次のように計算される： . mdi=min⁡(wdij). begin{aligned} text{md} _ i= min( text{wd} _ {ij}). end{aligned}mdi​=min(wdij​).​ 先述の通り，既存のwaypointからの最短距離が最大になる点がwaypointの集合に加えられる： . WS(l)=⋃(WS(l),argmax⁡({mdi})). begin{aligned} text{WS}^{(l)}= bigcup( text{WS}^{(l)}, text{arg} max( { text{md} _ i })). end{aligned}WS(l)=⋃(WS(l),argmax({mdi​})).​ . 満足な数のwaypointが得られるまでこれを繰り返し，これを全成分に関して行えば良い．得られた$L$個の集合の和集合を考えれば，最終的に$nW$個のwaypointの集合$ text{WS}$が得られる． . pseudotimeの計算 . Palantirでは，スタートの細胞は多様体の境界にある，つまりあるdiffusion componentに関する端点であるとする．ここで，境界にある細胞を . C=⋃l=1L(argmin⁡E(l),argmax⁡E(l)) begin{aligned} C= bigcup_{l=1}^L( text{arg} min boldsymbol{E}^{(l)}, text{arg} max boldsymbol{E}^{(l)}) end{aligned}C=l=1⋃L​(argminE(l),argmaxE(l))​ . と表す．ユーザーが設定したスタートの細胞$s$に最も近いような$C$の要素をpseudotimeの開始点$s’$とする： . s′=argmin⁡i∈CMS(es,ei). begin{aligned} s&amp;#x27;= text{arg} min_{i in C} text{MS}(e_s,e_i). end{aligned}s′=argi∈Cmin​MS(es​,ei​).​ . pseudotimeの初期値$ tau_i{}^{(0)}$は細胞$s’$から細胞$i$への最短経路の距離とする．これを全細胞に対して考える：${ tau_i{}^{(0)}}$． . waypoint $w$を通って，より離れた（pseudotimeが大きい）細胞$i$に行く場合，その経路の長さは，$w$までのpseudotimeと$w,i$間の距離の和になる．もし，$i$より離れたwaypointを通って$i$に行く経路という場合は，$V_{wi}$は$ tau_w$から$D_{wi}$を引けばスタートから$i$までの適切な距離が求まる．以上のことを踏まえると，$D_{wi}$を細胞$i$からのwaypoint $w$への最短経路の距離として，細胞$i$のwaypoint $w$に対するperspective（上で言った「長さ」）は次のように計算される： . Vwi={τw(0)+Dwi (τi(0)&gt;τw(0))τw(0)−Dwi (otherwise). begin{aligned} V_{wi}= begin{cases} tau_w{}^{(0)}+D_{wi} &amp;( tau_i{}^{(0)}&gt; tau_w{}^{(0)}) tau_w{}^{(0)}-D_{wi} &amp;( text{otherwise}) end{cases}. end{aligned}Vwi​={τw​(0)+Dwi​ τw​(0)−Dwi​ ​(τi​(0)&gt;τw​(0))(otherwise)​.​ . あるwaypointを経由して細胞$i$に到達する経路のperspective（長さ）を計算した．よって，細胞$i$に至る経路の長さを求めるには全てのwaypointについて平均しなければならない．$i$に近いようなwaypointを通る経路は尤もらしいので，平均を取る時に重みを大きくする．逆に，$i$から離れたwaypointを通るような経路は，平均を取る時に重みを小さくする．具体的には，waypointのperspectiveの重み付け平均は，waypointと細胞の距離に反比例するような，指数関数による重み付けを使う．そのために，重み付け行列$W in R^{nW times N}$を次のように定める： . Wwi=exp⁡(−Dwi2/σ)∑k=1Nexp⁡(−Dwk2/σ). begin{aligned} W_{wi}= frac{ exp(-{D_{wi}{}^2}/ sigma)}{ sum_{k=1}^{N} exp(-{D_{wk}{}^2}/ sigma)}. end{aligned}Wwi​=∑k=1N​exp(−Dwk​2/σ)exp(−Dwi​2/σ)​.​ . $ sigma$は距離行列$D$の標準偏差である．重み付け平均は . τi(1)=∑w∈WSVwiWwi begin{aligned} tau_i{}^{(1)}= sum_{w in text{WS}}V_{wi}W_{wi} end{aligned}τi​(1)=w∈WS∑​Vwi​Wwi​​ . と計算される．これによって，順次$ boldsymbol{ tau^{(0)}}, boldsymbol{ tau^{(1)}}, ldots$が得られ，最終的にpseudotime $ boldsymbol{ tau}$に収束する． . 最終状態 . waypointを結んだようなグラフ$G’_E in G_E$ を考える．$G’_E$ は無向グラフであるが，pseudotime $ boldsymbol{ tau}$を使って有向グラフ$G_D in boldsymbol{R}^{N times N}$を定義する： . GDij={GE′ijτi&lt;τj;τi&gt;τj, τi−τj&lt;σi0τi&gt;τj, τi−τj&gt;σi. begin{aligned} G_D{} _ {ij}= begin{cases} G&amp;#x27;_E{} _ {ij} quad &amp; tau_i&lt; tau_j; &amp; tau_i&gt; tau_j, , tau_i- tau_j&lt; sigma_i 0 quad &amp; tau_i&gt; tau_j, , tau_i- tau_j&gt; sigma_i end{cases}. end{aligned}GD​ij​=⎩⎪⎪⎨⎪⎪⎧​GE′​ij​0​τi​&lt;τj​;τi​&gt;τj​,τi​−τj​&lt;σi​τi​&gt;τj​,τi​−τj​&gt;σi​​.​ . 次に，$(1)$で使ったkernelを使って，affinity行列$Z in boldsymbol{R}^{nW times nW}$を作る．さらに，これを . Pij=Zij∑kZik begin{aligned} P_{ij}= frac{Z_{ij}}{ sum_kZ_{ik}} end{aligned}Pij​=∑k​Zik​Zij​​​ . によってMarkov過程の遷移確率行列$P$にする．$P_{ij}$は細胞が１回の遷移で状態$i$から状態$j$に到達する確率である． . 最終状態は，それ以上分化しない状態なので，最終状態の集合$ text{TS}$が与えられれば，Markov過程$P$から， . Aij=0 (i∈TS,j=1,…,nW) begin{aligned} A_{ij}=0 (i in text{TS},j=1, ldots,nW) end{aligned}Aij​=0 (i∈TS,j=1,…,nW)​ . によって吸収的Markov過程$A$を定義できる． . Markov過程の定常分布$ boldsymbol{ pi}$があるとする： $ boldsymbol{ pi}=P* boldsymbol{ pi}$．さらに，平均絶対偏差は次のように計算される： . sc=Median(∣πi−Median(π)∣). begin{aligned} text{sc}= text{Median}( mid pi_i- text{Median}( boldsymbol{ pi}) mid). end{aligned}sc=Median(∣πi​−Median(π)∣).​ . これを使って，この分布における外れ値を次のように計算する： . TScands={i∣πi&gt;Gppf(0.9999,Median(π),sc)}. begin{aligned} text{TS}^ text{cands}= {i mid pi_i&gt; text{Gppf}(0.9999, text{Median}( boldsymbol{ pi}), text{sc}) }. end{aligned}TScands={i∣πi​&gt;Gppf(0.9999,Median(π),sc)}.​ . ただし，$ text{Gppf}(p, mu, sigma^2)$は平均$ mu$，分散$ sigma^2$の正規分布関数での$p$パーセント点である．この$ text{TS}^ text{cands}$のうち，diffusion componentの端点であるものを最終状態とする： . TS=⋂(TScands,C). begin{aligned} text{TS}= bigcap( text{TS}^ text{cands},C). end{aligned}TS=⋂(TScands,C).​ . 分化ポテンシャル . Markov過程でのランダムウォークにより，細胞が辿る経路が分かる．それぞれの細胞について，吸収的最終状態$b$に到達する確率を表す分岐確率ベクトル$ boldsymbol{B} _ i$を求める．吸収的Markov過程Aは次のように表すことができる： . A=(QR0E) begin{aligned} A= begin{pmatrix} Q &amp; R 0 &amp; E end{pmatrix} end{aligned}A=(Q0​RE​)​ . $Q$は中間状態での遷移確率を表す$(nW-b) times(nW-b)$行列，$R$は中間状態から最終状態への遷移確率を表す$(nW-b) times b$行列，$E$は単位行列である．fundamental行列$F$を次のように定義する： . F∗(E−Q)=E,F=(E−Q)−1. begin{aligned} F*(E-Q)=E, quad F=(E-Q)^{-1}. end{aligned}F∗(E−Q)=E,F=(E−Q)−1.​ . $Q_{ij}$は中間状態$i$から中間状態$j$に遷移する確率を表すので，$F_{ij}$は中間状態$j$から（いつか）中間状態$i$に到達する確率を表す．次に，分化確率を . B=F∗R begin{aligned} B=F*R end{aligned}B=F∗R​ . によって計算することができる．$R _ {ij} $ は中間状態$i$から最終状態$j$（$nW-b+j$番目の状態）を表すので，$B_{ij}= sum_kF_{ki}R_{kj}$は中間状態$i$から（いつか）最終状態$j$に到達する確率を表わしている． . $ boldsymbol{B} _ {i}$ は $ sum _ j B _ {ij}=1$ となる多項分布である．また，最終状態から最終状態への分岐確率は，明らかに . Bij={1i=j0i≠j begin{aligned} B_{ij}= begin{cases} 1 quad&amp; i=j 0 quad&amp; i neq j end{cases} end{aligned}Bij​={10​i=ji​=j​​ . となる．これはwaypointの分岐確率なので，$W$がwaypointによる全細胞への投影の重み付けであったことを思い出せば，全細胞に関する遷移確率 . Bij=∑w∈WSBwjWwi begin{aligned} B_{ij}= sum_{w in text{WS}}B_{wj}W_{wi} end{aligned}Bij​=w∈WS∑​Bwj​Wwi​​ . が得られる． . 分岐確率ベクトル$ boldsymbol{B} _ i$のエントロピー . Si=−∑jBijlog⁡Bij begin{aligned} S_i=- sum_jB_{ij} log B_{ij} end{aligned}Si​=−j∑​Bij​logBij​​ . を各状態での分化ポテンシャルとする． .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/single-cell-analysis/2019/09/19/palantir.html",
            "relUrl": "/single-cell-analysis/2019/09/19/palantir.html",
            "date": " • Sep 19, 2019"
        }
        
    
  
    
        ,"post11": {
            "title": "scEpathの解説",
            "content": "scEpathの論文のメソッドのざっくりした解説，というかメモ．エネルギー最高のクラスタから単純にエネルギーが下がっていくって方針を取っているので，複数の経路を再現するとかはできない．悲しみ． . scEnergyの導入 . 遺伝子遺伝子間相互作用gene-gene interactionのネットワークを作る．つまり，遺伝子が$n$あるとして，そのグラフを考える．$i$番目の遺伝子と$k$番目の遺伝子が繋がっていれば$a_{ik}=1$，そうでなければ$a_{ik}=0$とする（隣接行列adjacency matrix）．詳しくは，$m$個の細胞での$i$番目の遺伝子と$k$番目の遺伝子の発現をそれぞれ，$x_i=(x_{i1}, ldots,x_{im}),x_k=(x_{k1}, ldots,x_{km})$とする．この時， . aik={1(∣cor(xi,xk)∣&gt;τ)0(∣cor(xi,xk)∣≤τ).(1) begin{aligned} a_{ik}= begin{cases} 1 &amp; ( mid text{cor}(x_i,x_k) mid&gt; tau) 0 &amp; ( mid text{cor}(x_i,x_k) mid leq tau) end{cases}. tag{1} end{aligned}aik​={10​(∣cor(xi​,xk​)∣&gt;τ)(∣cor(xi​,xk​)∣≤τ)​.​(1) . カノニカル分布（正準分布）にいついて．系がエネルギー$E_i (i=0,1, ldots)$を取るとき，系のエネルギーが$E_j$である確率$p_j$は . pj=exp⁡(−βEj)∑jexp⁡(−βEj) begin{aligned} p_j= frac{ exp(- beta E_j)}{ sum_{j} exp(- beta E_j)} end{aligned}pj​=∑j​exp(−βEj​)exp(−βEj​)​​ . で与えられる．特に，分母の$ sum_j exp(- beta E_j)$を分配関数$Z$と呼ぶ． . scEpathでは$ beta=1$としている 1．つまり，遺伝子発現が$y$である細胞が状態$j$にある確率は，細胞が取るステージの総数，もっと言えば，分化していくときに異種とみなす細胞の数を$m$とすれば， . pj(y)=exp⁡(Ej(y))∑j=1mexp⁡(−Ej(y))(2) begin{aligned} p_j(y)= frac{ exp(E_j(y))}{ sum_{j=1}^{m} exp(-E_j(y))} tag{2} end{aligned}pj​(y)=∑j=1m​exp(−Ej​(y))exp(Ej​(y))​​(2) . となる．ステージは適当なunsupervised clusteringでのclusterなどを使う．この場合，$m$はクラスターの数． . $E_j(y)$（遺伝子発現が$y$である$j$番目の細胞のエネルギー）については，状態$j$の細胞での$i$番目の遺伝子の発現レベルを$x_{ij}$として，標準化発現レベル$y_{ij}$を . yij=xij−xjminxjmax−xjmin begin{aligned} y_{ij}= frac{x_{ij}-x_j^ text{min}}{x_j^ text{max}-x_j^ text{min}} end{aligned}yij​=xjmax​−xjmin​xij​−xjmin​​​ . で定義する．ただし，$x_j^ text{max},x_j^ text{min}$は$j$番目の細胞の中にある全遺伝子の発現レベルのうちの最大値，最小値である． . $E_j(y)$（遺伝子発現が$y$である$j$番目の細胞のエネルギー）については，$j$番目の細胞での$i$番目の遺伝子の標準化発現レベルを$y_{ij} (0 leq y_{ij} leq1)$を使って， . Ej(y)=∑inEij(y)=−∑inyijln⁡yij∑k∈N(i)ykj. begin{aligned} E_j(y)= sum_i^nE_{ij}(y)=- sum_i^ny_{ij} ln frac{y_{ij}}{ sum_{k in N(i)}y_{kj}}. end{aligned}Ej​(y)=i∑n​Eij​(y)=−i∑n​yij​ln∑k∈N(i)​ykj​yij​​.​ . $N(i)$は，$(1)$で考えたネットワークで，$i$番目の遺伝子の隣のある遺伝子の集合．さらに，これを標準化した . Ej^(y)=(Ej(y)/Eˉ(y))21+(Ej(y)/Eˉ(y))2 begin{aligned} hat{E_j}(y)= frac{(E_j(y)/ bar{E}(y))^2}{1+(E_j(y)/ bar{E}(y))^2} end{aligned}Ej​^​(y)=1+(Ej​(y)/Eˉ(y))2(Ej​(y)/Eˉ(y))2​​ . を使う．ただし，$ bar{E}(y)$は，全細胞（仮に$m’$個とする）についての$E_j(y)$の平均である： . Eˉ(y)=1m′∑j=1m′Ej(y). begin{aligned} bar{E}(y)= frac{1}{m&amp;#x27;} sum_{j=1}^{m&amp;#x27;}E_j(y). end{aligned}Eˉ(y)=m′1​j=1∑m′​Ej​(y).​ . メタ細胞 . 各クラスタ（主成分分析を行い，２成分からなっている）2でクラスタのエネルギーの$ theta_1=80$%を占める細胞たちをメタ細胞metacellとして定義する．クラスタの数を$N$個とすれば，それぞれのメタ細胞のエネルギー$E_k^ text{M}$は，トリム平均trimeanを取る（外れ値を弾くことができる）．すなわち，$k$番目のメタ細胞のエネルギーの四部位数を下から$Q_1,Q_2,Q_3$として， . EkM=12(Q2+Q1+Q32). begin{aligned} E_k^ text{M}= frac{1}{2} left(Q_2+ frac{Q_1+Q_3}{2} right). end{aligned}EkM​=21​(Q2​+2Q1​+Q3​​).​ . $(2)$と同様に3すれば，ある細胞が$k$番目のメタ細胞にある確率 $p_k^ text{M}$は， . pkM=exp⁡(−EkM)∑j=1Nexp⁡(−EjM) begin{aligned} p_k^ text{M}= frac{ exp(-E_k^ text{M})}{ sum_{j=1}^N exp(-E_j^ text{M})} end{aligned}pkM​=∑j=1N​exp(−EjM​)exp(−EkM​)​​ で与えられる．ここで，$k$番目のメタ細胞に属する細胞のエネルギーは大体，$E_k^ text{M}$だと仮定している4．もちろん，メタ細胞，つまり，ある細胞が取りうる状態は$N$個あるので分配関数は$N$個の和になっている．また，ある細胞が$k$番目のメタ細胞に残る確率は$p_k^ text{M}$，それ以外のメタ細胞へ遷移する確率は$1-p_k^ text{M}$である． . Markov過程 . 時間的に定常なMarkov過程において，時刻が$1$経過して，$i$から$j$になる確率は， . pij=Pr⁡(Xn=j∣Xn−1=i) begin{aligned} p_{ij}= Pr(X_n=j mid X_{n-1}=i) end{aligned}pij​=Pr(Xn​=j∣Xn−1​=i)​ . で与えられる．ここで，あるベクトル$ boldsymbol{ pi}$を考える．$ boldsymbol{ pi}$が . πj=∑iπipij,∑jπj=1 begin{aligned} pi_j= sum_i pi_ip_{ij}, quad sum_j pi_j=1 end{aligned}πj​=i∑​πi​pij​,j∑​πj​=1​ . を満たす時，$ boldsymbol{ pi}$を定常分布stationary distributionと呼ぶ． . 主成分分析で得られた２次元空間で，$k$番目のメタ細胞から$l$番目のメタ細胞に遷移する確率は，これらの距離に反比例すると仮定する．メタ細胞間の隣接行列を，距離によって次のように定義する： . Gkl=exp⁡(−∥zk−zl∥24ε2). begin{aligned} G_{kl}= exp left(- frac{ |z_k-z_l |^2}{4 varepsilon^2} right). end{aligned}Gkl​=exp(−4ε2∥zk​−zl​∥2​).​ . ただし，$z_k$は２次元空間におけるメタ細胞の中心，$ varepsilon$はメタ細胞間の距離の最大値である．$G_{kl}$を規格化すれば遷移行列が得られる： . G~klasym=Gkl∑j=1NGkj. begin{aligned} tilde{G}_{kl}^ text{asym}= frac{G_{kl}}{ sum_{j=1}^NG_{kj}}. end{aligned}G~klasym​=∑j=1N​Gkj​Gkl​​.​ . 遷移行列$ tilde{G}_{kl}^ text{asym}$は非対称であるが，次に示す定常状態$ boldsymbol{ pi}_k (1 leq k leq N)$を持つ： . πk=∑i=1NGkj∑k=1N∑j=1NGkj. begin{aligned} boldsymbol{ pi}_k= frac{ sum_{i=1}^NG_{kj}}{ sum_{k=1}^N sum_{j=1}^NG_{kj}}. end{aligned}πk​=∑k=1N​∑j=1N​Gkj​∑i=1N​Gkj​​.​ . また，$G_{kl}$が対称であることに注意すれば，容易に分かるように， . πkG~klasym=πlG~lkasym begin{aligned} pi_k tilde{G}_{kl}^ text{asym}= pi_l tilde{G}_{lk}^ text{asym} end{aligned}πk​G~klasym​=πl​G~lkasym​​ . が成立する．よって， . G~klsym=πkπlG~klasym begin{aligned} tilde{G}_{kl}^ text{sym}= sqrt{ frac{ pi_k}{ pi_l}} tilde{G}_{kl}^ text{asym} end{aligned}G~klsym​=πl​πk​​ . ​G~klasym​​ . を定義すれば，これは対称な遷移行列となる． . 遷移確率 . メタ細胞の項の最後にも述べたことに注意すると，$k$番目のメタ細胞から$l$番目のメタ細胞に遷移する確率行列$T$は， . Tkl={(1−pkM)G~klsymk≠lpkMk=l (no transition) begin{aligned} T_{kl}= begin{cases} (1-p_k^ text{M}) tilde{G}_{kl}^ text{sym} &amp; k neq l p_k^ text{M} &amp; k=l ( text{no transition}) end{cases} end{aligned}Tkl​={(1−pkM​)G~klsym​pkM​​k​=lk=l (no transition)​​ . となる． . 遷移確率行列$T$は２つのメタ細胞間がお互いに遷移する確率($T_{kl},T_{lk}$)を含む．つまり，この状態ではメタ細胞の集合は双方向性のあるグラフになる．しかし，実際に細胞が時間発展していく場合はエネルギーが減少する経路のみを取るので，グラフは一方向性なはずである．よって，Wilcoxonの順位和検定によって，２つのメタ細胞を比較する．この場合の帰無仮説$ text{H}_0$は２つのメタ細胞間のエネルギー（のトリム平均）に差がないことである．scEpathでは，p値が$ alpha=0.01$以下の場合は帰無仮説が棄却される．これらのことを考慮すれば，向き付けを考慮したメタ細胞のグラフの遷移確率行列$W$は . Wkl={(1−pkM)G~klsymk≠l,p&lt;α,EkM&gt;ElM;k≠l,p≥αpkMk=l (no transition) begin{aligned} W_{kl}= begin{cases} (1-p_k^ text{M}) tilde{G}_{kl}^ text{sym} &amp; k neq l, p&lt; alpha,E_k^ text{M}&gt;E_l^ text{M}; &amp; k neq l,p geq alpha p_k^ text{M} &amp; k=l ( text{no transition}) end{cases} end{aligned}Wkl​=⎩⎪⎪⎨⎪⎪⎧​(1−pkM​)G~klsym​pkM​​k​=l,p&lt;α,EkM​&gt;ElM​;k​=l,p≥αk=l (no transition)​​ . となる．$p$は$k$番目と$l$番目のメタ細胞を検定した時の$p$値．帰無仮説が採用された場合は，$k$番目と$l$番目のメタ細胞のエッジは双方向性となる．これによって，メタ細胞をノードとするグラフはエネルギーが増加しない遷移のみを許す有向グラフとなる．その遷移確率行列は$W$である． . scEpathを求める . 細胞の実際の経路を求める場合は，最もエネルギーが高いメタ細胞から，遷移確率が最も高いような経路を辿ることになる．これは最もエネルギーが高いメタ細胞を根とし，各エッジの重みを$1-W$として構成した最小全域木minimum directed spanning tree (MDST)を見つけるのに等価である． . メタ細胞の中心をあらかじめ求めておき，その中心から$ theta_2=0.75$分位数より内側にある細胞をコア細胞とする．そして，先程求めた経路をコア細胞に限定してprincipal curveでフィッティングし，それを$[0,1]$にスケール変換する．そして，psudotime reconstruction score (PRS)を，他のデータを使って次のように定める： . PRS=c−c′c+c′. begin{aligned} text{PRS}= frac{c-c&amp;#x27;}{c+c&amp;#x27;}. end{aligned}PRS=c+c′c−c′​.​ . ただし，$c$は一致した細胞の数，$c^ prime$は一致しなかった細胞の数である． . pseudotime依存的なTF . pseudotime依存的な遺伝子を見つけるため，pseudotimeを10個に分割する．さらに，各部分での遺伝子発現のトリム平均を取る．各遺伝子の発現の平均を３次スプライン曲線で滑らかにする．細胞の順番をランダムに並び替えたものと比べて，標準偏差が$0.5$以上，Bonferroni調節p値が$0.01$以下のものをpseudotime依存的な遺伝子とする． . 既存の転写因子(TF)の中から，pseudotime依存的な遺伝子を探し，細胞系列で発言が有意に変化している（系列に含まれるクラスター間で比べた際，Bonferroni調節p値が$0.01$以下，少なくとも閾値倍以上の変化がある）ものを，pseudotime依存的なTFとする． . さらに，TFのネットワークを構成し，Spearmanの順位相関係数を使ってp値が$10^{-5}$以下のものは相関がないとし，そうでなければ，相関があるとして結ぶ．この操作を$90$%の細胞で1000回繰り返し，どの操作でも結ばれたTFは，真に関係があると結論づける． . . 逆温度$ beta$を必ずしも$1$にする必要はない？ &#8617; . | これはunsupervised clusteringとかで得られた結果から行う？ &#8617; . | 正準分布の考え方．つまり，ある細胞が$k$番目のメタ細胞に属する，と言う状態になる確率を議論している． &#8617; . | この妥当性を検証するのは面白いかも &#8617; . |",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/single-cell-analysis/2019/09/14/scepath.html",
            "relUrl": "/single-cell-analysis/2019/09/14/scepath.html",
            "date": " • Sep 14, 2019"
        }
        
    
  
    
        ,"post12": {
            "title": "グリッド細胞の発火パターンをPythonで可視化する",
            "content": "&#27010;&#35201; . Edvard Moser博士の研究室が公開している、グリッド細胞の活動をPythonで可視化してみました。データはhttps://www.ntnu.edu/kavli/research/grid-cell-dataからダウンロードできます。 . コードを書く上でhttp://felix11h.github.io/blog/grid-cell-rate-mapsを参考にしました。一部の関数はこのブログから引用しています。今回は上記のサイトで実装されていない、Gaussian kernelを用いたSmoothed rate mapとAutocorrelation mapの実装をしてみます。 . Important: 著者はGrid cellsの研究をしていません。実際の研究で用いられるコードと異なる可能性があります。 . &#12464;&#12522;&#12483;&#12489;&#32048;&#32990;(Grid Cells)&#12395;&#12388;&#12356;&#12390; . 実装とは関係ないですが、グリッド細胞についてまとめておきます。 . &#31354;&#38291;&#22522;&#24213;&#12392;&#12375;&#12390;&#12398;&#12464;&#12522;&#12483;&#12489;&#32048;&#32990; . 詳しくは場所細胞 - 脳科学辞典や2014年のノーベル生理学・医学賞の解説（神経科学学会）、Grid cells (Scholarpedia)などをお読みいただければと思います。簡単にまとめると、海馬には場所特異的に発火する場所細胞(place cell)があり、これはO&#39;keefe博士によって発見されました。次にMay-Britt Moser博士とEdvard Moser博士は六角形格子状の場所受容野を持つグリッド細胞(格子細胞, grid cell)を内側嗅内皮質(medial entorhinal cortex; MEC)で発見しました。この3人は2014年のノーベル生理学・医学賞を受賞しています。 . . http://www.scholarpedia.org/article/Grid_cellsより。左図の黒線はラットの経路、赤は発火が生じた位置。右図は発火率マップ(rate map)。 . 最近、外側膝状体背側核(dorsal lateral geniculate nucleus)で場所細胞が見つかったそうです（V Hok, et al., 2018, bioRxiv）。 . &#12487;&#12540;&#12479;&#12395;&#12388;&#12356;&#12390; . 公開されているデータはMatLabのmatファイル形式です。しかし、scipy.io.loadmatを用いることでpythonでデータの中身を取得することができます。 . 使用するデータは以下の通りです。 . 10704-07070407_POS.mat | 10704-07070407_T2C3.mat | . これらのファイルはhttps://archive.norstore.no/pages/public/datasetDetail.jsf?id=8F6BE356-3277-475C-87B1-C7A977632DA7からダウンロードできるファイルの一部です。ただし全体で2.23GBあるので、簡単に試したい場合は上記のリンクからダウンロードしてください。以下では./data/grid_cells_data/ディレクトリの下にファイルを置いています。 . データの末尾の&quot;POS&quot;と&quot;T2C3&quot;の意味について説明しておきます。まず、&quot;POS&quot;はpost, posx, posyを含む構造体でそれぞれ試行の経過時間、x座標, y座標です。座標は-50~50で記録されています。恐らく1m四方の正方形の部屋で、原点を部屋の中心としているのだと思います。&quot;T2C3&quot;はtがtetrode（テトロード電極）でcがcell（細胞）を意味します。後ろの数字は番号付けたものと思われます。 . Smoothed Rate Map&#12395;&#12388;&#12356;&#12390; . 発火率$ lambda( boldsymbol{x})$は、場所$ boldsymbol{x}=(x,y)$で記録されたスパイクの回数を、場所$ boldsymbol{x}$における滞在時間(s)で割ることで得られます。 $$ lambda( boldsymbol{x})= frac{ displaystyle sum_{i=1}^n g left( frac{ boldsymbol{s}_i- boldsymbol{x}}{h} right)}{ displaystyle int_0^T g left( frac{ boldsymbol{y}(t)- boldsymbol{x}}{h} right)dt} $$ ただし、$n$はスパイクの回数、$T$は計測時間、$g( cdot)$はGaussain Kernel（中身の分子が平均、分母が標準偏差）、$ boldsymbol{s}_i$は$i$番目のスパイクの発生した位置、$ boldsymbol{y}(t)$は時刻$t$でのラットの位置です。分母は積分になっていますが、実際には離散的に記録をするので、累積和に変更し、$dt$を時間のステップ幅(今回は0.02s)とします。 . Gaussian Kernelを用いて平滑化することで「10cm四方での発火を同じ位置での発火とする」などとした場合よりも、得られるマップは滑らかになります。 . &#23455;&#35013; . まず、ライブラリをインポートしてデータを読み込みます。 . import numpy as np import matplotlib.pyplot as plt from scipy import io as io from tqdm import tqdm # from http://www.ntnu.edu/kavli/research/grid-cell-data pos = io.loadmat(&#39;./data/grid_cells_data/10704-07070407_POS.mat&#39;) spk = io.loadmat(&#39;./data/grid_cells_data/10704-07070407_T2C3.mat&#39;) . posファイル内の構造は次のようになっています。 . pos[&quot;post&quot;]: times at which positions were recorded | pos[&quot;posx&quot;]: x positions | pos[&quot;posy&quot;]: y positions | spk[&quot;cellTS&quot;]: spike times | . 次に種々の関数を実装します。 . def nearest_pos(array, value): k = (np.abs(array - value)).argmin() return k . def GaussianKernel(sizex, sizey, sigma=0.5, center=None): &quot;&quot;&quot; sizex : kernel width sizey : kernel height sigma : gaussian Sd center : gaussian mean return gaussian kernel &quot;&quot;&quot; x = np.arange(0, sizex, 1, float) y = np.arange(0, sizey, 1, float) x, y = np.meshgrid(x,y) if center is None: x0 = sizex // 2 y0 = sizey // 2 else: if np.isnan(center[0])==False and np.isnan(center[1])==False: x0 = center[0] y0 = center[1] else: return np.zeros((sizey,sizex)) return np.exp(-((x-x0)**2 + (y-y0)**2) / 2*sigma**2) . def smoothed_rate_map(pos, spk, kernel_sigma=0.1, W=100, H=100): # load datas posx = pos[&quot;posx&quot;].flatten() posy = pos[&quot;posy&quot;].flatten() spkt = spk[&quot;cellTS&quot;].flatten() #change positions range: -50 ~ 50 -&gt; 0 ~ H or W posx = (posx + 50) / 100 * W posy = (posy + 50) / 100 * H # find nearest positions when spikes occur indx = [nearest_pos(pos[&quot;post&quot;],t) for t in spkt] indy = [nearest_pos(pos[&quot;post&quot;],t) for t in spkt] # occup position while trajectory occup_m_list = [] for i in tqdm(range(len(posx))): occup_m_list.append(GaussianKernel(W, H, kernel_sigma, (posx[i], posy[i]))) occup_m = sum(occup_m_list) occup_m *= 0.02 # one time step is 0.02s occup_m[occup_m==0] = 1 # avoid devide by zero # activation activ_m_list = [] for i in tqdm(range(len(spkt))): activ_m_list.append(GaussianKernel(W, H, kernel_sigma, (posx[indx][i] ,posy[indy][i]))) activ_m = sum(activ_m_list) rate_map = activ_m / occup_m return rate_map . 最後に実行します。 . rm = smoothed_rate_map(pos, spk, 0.2, 100, 100) plt.figure(figsize=(6,4)) plt.imshow(rm, cmap=&quot;jet&quot;) plt.colorbar(label=&quot;Hz&quot;) plt.gca().invert_yaxis() plt.tight_layout() # plt.savefig(&quot;smoothed_rate_map.png&quot;) plt.show() . 100%|██████████████████████████████████████████████████████████████████████████| 30000/30000 [00:09&lt;00:00, 3306.34it/s] 100%|█████████████████████████████████████████████████████████████████████████████| 2326/2326 [00:02&lt;00:00, 959.91it/s] . Autocorrelation Map&#12395;&#12388;&#12356;&#12390; . https://core.ac.uk/download/pdf/30859910.pdfのSupporting Online Materialに書いてある式通りに実装してみましたが、遅い＆論文と見た目が全く異なるので、scipy.signal.correlate2dを使いました。 . from scipy.signal import correlate2d rm = smoothed_rate_map(pos, spk, 0.5, 100, 100) a_corr = correlate2d(rm, rm, fillvalue=5) plt.figure(figsize=(6,4)) plt.imshow(a_corr, cmap=&quot;jet&quot;) plt.colorbar(label=&quot;Autocorrelation&quot;) plt.tight_layout() # plt.savefig(&quot;autocorr.png&quot;) plt.show() . 100%|██████████████████████████████████████████████████████████████████████████| 30000/30000 [00:16&lt;00:00, 1795.03it/s] 100%|█████████████████████████████████████████████████████████████████████████████| 2326/2326 [00:02&lt;00:00, 929.87it/s] . 若干論文と図が異なる上、cross-correlationが-1~1の範囲でないのはおかしい気がするのですが、六角形格子が見えているので良しとします。 . &#21442;&#32771;&#12395;&#12375;&#12383;&#25991;&#29486;&#12539;&#12469;&#12452;&#12488; . https://github.com/Felix11H/grid_cell_rate_map | https://www.ntnu.edu/kavli/research/grid-cell-data | https://core.ac.uk/download/pdf/30859910.pdfのSupporting Online Material | https://github.com/MattNolanLab/gridcells | https://arxiv.org/pdf/1810.07429.pdf | .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/neuroscience/2018/11/23/grid_cells.html",
            "relUrl": "/neuroscience/2018/11/23/grid_cells.html",
            "date": " • Nov 23, 2018"
        }
        
    
  

  
  
      ,"page0": {
          "title": "About Us",
          "content": "山拓：神経科学の研究をしています。 Github : https://github.com/takyamamoto | 凪：理学部（）と工学部（）がいるので人間科学部（）になってみる今日このごろ | ざっきぃ：ライフワークはクラシック音楽鑑賞。しばらく統計から離れて医学に集中します。 | 優曇華院：数理物理学者． | みこ：音楽バカ。医者になるのはフルートを吹くため | .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "神経科学",
          "content": "神経表現の可視化 . グリッド細胞の発火パターンをPythonで可視化する | スパイクトリガー平均・共分散の計算法 (Python) | ネコLGN細胞のスパイクトリガー平均による受容野解析 | Sorted plotの注意点 | . 解剖・形態学 . NibabelとMayaviによるMR画像の可視化 | 脳が対側支配をする進化的な利点は何か | . 色覚 . RGBからXYZ, LMS色空間への変換 | CIELUV色相環をPythonで描画する | . 強化学習 . Distributional Reinforcement Learningの仕組み | . 計算神経科学 . 古典的モデル . FitzHugh-Nagumoモデルをアニメーションで見る | Hodgkin-Huxleyモデルをアニメーションで見る | 心臓刺激伝導系の数理モデルのPythonでのシミュレーション | . ニューラルネットワークと神経科学 . 予測符号化 (predictive coding) とは何か | 掛け算のニューラルネットワークとベイズ推定 | 『人工神経回路で脳を理解する』論文まとめ | ニューラルネットワークにおける意味発達の数学理論 | Spiking Neural UnitをChainerで実装してみた | 物体認識のためのRecurrent CNNまとめ | FORCE法によるRecurrent Spiking Neural Networksの教師あり学習 | 人工神経回路による脳の理解はどこまで進んだか | . Computational neuroscience (Coursera) . Computational neuroscience : Week0 | Computational neuroscience : Week1 | Computational neuroscience : Week2 | Computational neuroscience : Week3 | .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/neuroscience/",
          "relUrl": "/neuroscience/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "機械学習",
          "content": "最適化 . [線形計画法（シンプレックス法）入門　PDFファイル] | Hamiltonian Descent Methodsの実装についての解説 | Boltzmann Generatorsの解説 | . Deep Learning . Chainer . Chainerで学習率のスケジューリングをする方法 | ChainerでPredNetの実装をしてみた | . TensorFlow . tensorflow-gpuとCUDAのバージョン | . Keras . Kerasにおける中間層の出力の可視化 | KerasによるGraph Convolutional Networks | Kerasにおいてforループでmodelを定義する | Keras examples directoryで実装例を見る | KerasのconvLSTM2Dの使用例を見る | . 強化学習 . OpenAI Gymでオリジナルの環境を作る | Kerasを用いたDQNでFlappyBirdを学習させる | .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/machine-learning/",
          "relUrl": "/machine-learning/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "統計学",
          "content": "Pythonで統計処理 . 分散共分散行列を相関行列/偏相関係数に変換 | Graph Lassoによる変数間の関係のグラフ化 | Pythonで線形回帰モデルの計算（１） | Pythonで線形回帰モデルの計算（２） | Pythonで正規分布の区間推定 | PythonでGaussian Fitting | scipyによる1次元混合ガウス回帰 | matplotlibのみで線形回帰の信頼区間を描画する | matplotlibで棒グラフ間の有意差の描画をする | Pythonによる分位点回帰 (Quantile regression) | . 確率論 . 確率モデル . 確率過程とランダムウォーク | マルコフ連鎖 (Markov chain) | . 確率分布 . q分位数と中央値 | 確率変数の変換 | デルタ法 (Delta method) | . 統計的推定・検定 . 比率の区間推定 | ノンパラメトリック検定 | ケンドールの一致係数(Kendall’s coefficient of concordance) | . 統計解析 . 回帰分析 . 重回帰分析① | 重回帰分析② | 正規方程式 | ロジスティック回帰分析 | 対数オッズ比の分散 | . データ分析 . Principle Curve 入門 | PythonによるPrincipal Curveの実装 (bendingアルゴリズム) | . 稲垣宣生 「数理統計学」演習問題の解答 . 書いたものをあげます。 . 演習問題1 | 演習問題2 | 演習問題3 | 演習問題4 | 演習問題5 | 演習問題6 | 演習問題7 | 演習問題8 | 演習問題9 | 演習問題10 | 演習問題11 | 演習問題12 | 演習問題13 | 演習問題14 | . 統計検定 . 統計検定2級のフィードバック（前編） | 統計検定2級のフィードバック（後編） | .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/statistics/",
          "relUrl": "/statistics/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "医学",
          "content": "生理学 . 心臓震盪とリミットサイクル | . 薬理学 . コンパートメントモデル(薬物動態)のシミュレーション | . 環境医学・公衆衛生学 . 疫学基礎 | . 医用工学 . PET検査の仕組み（第1回）「PETの基礎理論」 | PET検査の仕組み（第2回）「画像再構成①：Radon変換」 | PET検査の仕組み（第3回）「画像再構成②：逆Radon変換」 | PET検査の仕組み（第4回）「画像再構成③：FBP法」 | PET検査の仕組み（第5回）「誤差要因の補正(1)：偶発同時計数と散乱同時計数」 | PET検査の仕組み（第6回）「誤差要因の補正(2)：吸収と感度の補正」 | PET検査の仕組み（第7回）「PETシミュレーション」 | 脳波と脳磁図 (PDFファイル) | . シングルセル解析 . scEpathの解説 | Palantirの解説 | scGenの解説 | . 医学知識の整理 . 大学の試験対策用に作成したデータを、学問別に公開しようと思う。初学者が、各学問の全貌を大雑把に把握するのに適していると思う。 . メモ帳というアプリで毎回作っているため、基本的には図がない。ビジュアル面は、自身の参考書などで補ってほしい。 . 組織学 . 骨 | 神経 | 血液 | 心血管系 | 呼吸器系 | 食道・胃 | 肝・胆・膵 | 小腸・大腸 | 口腔（歯・舌・唾液腺） | 内分泌系１ | 内分泌系２ | 泌尿器系 | 眼・耳 | . 生理学 . 細胞の一般生理 | 膜電位と興奮 | シナプス | 筋生理 | 血液生理 | 呼吸 | 循環１ | 循環２ | 循環３ | 末梢神経・自律神経 | . 生物科学（Essential細胞生物学11～16章） . 遺伝学（Essential細胞生物学5～10章） . 解剖学 . 神経解剖学 .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/medicine/",
          "relUrl": "/medicine/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "化学",
          "content": "化学 .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/chemistry/",
          "relUrl": "/chemistry/",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "物理学",
          "content": "物理学 .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/physics/",
          "relUrl": "/physics/",
          "date": ""
      }
      
  

  
      ,"page7": {
          "title": "数学",
          "content": "数学 .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/mathematics/",
          "relUrl": "/mathematics/",
          "date": ""
      }
      
  

  
      ,"page8": {
          "title": "Programming",
          "content": "Programming .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/programming/",
          "relUrl": "/programming/",
          "date": ""
      }
      
  

  
      ,"page9": {
          "title": "Computer",
          "content": "Computer .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/computer/",
          "relUrl": "/computer/",
          "date": ""
      }
      
  

  
      ,"page10": {
          "title": "読書",
          "content": "読書 .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/reading/",
          "relUrl": "/reading/",
          "date": ""
      }
      
  

  
      ,"page11": {
          "title": "エッセイ",
          "content": "エッセイ .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/essay/",
          "relUrl": "/essay/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  
  

  
  

}