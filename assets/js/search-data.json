{
  
    
        "post0": {
            "title": "Predictive coding (Rao & Ballard, 1999) モデルの実装",
            "content": "この記事ではPredictive codingの初めの数理的モデルとなる (Rao &amp; Ballard, Nat. Neurosci. 1999)を解説し、Pythonによる実装を行います。コードを実行したい場合はhttps://github.com/takyamamoto/PredictiveCoding-RaoBallard-ModelをCloneしてpredictive-coding.ipynbか、train.pyを実行してください。 . &#35251;&#28204;&#19990;&#30028;&#12398;&#38542;&#23652;&#30340;&#20104;&#28204; . 構築するネットワークは入力層を含め、3層のネットワークとします。網膜への入力として画像 $ boldsymbol{I} in mathbb{R}^{n_0}$を考えます。画像 $ boldsymbol{I}$ の観測世界における隠れ変数、すなわち潜在変数 (latent variable)を$ boldsymbol{r} in mathbb{R}^{n_1}$とし、ニューロン群によって発火率で表現されているとします (真の変数と $ boldsymbol{r}$は異なるので文字を分けるべきでしょうが簡単のためにこう表します)。このとき、 . $$ boldsymbol{I} = f(U boldsymbol{r}) + boldsymbol{n} tag{1} $$が成立しているとします。ただし、$f( cdot)$は活性化関数 (activation function)、$U in mathbb{R}^{n_0 times n_1}$は重み行列です。$ boldsymbol{n} in mathbb{R}^{n_0} $は平均0, 分散 $ sigma^2$ のGaussian ノイズ項とします。 . 潜在変数 $ boldsymbol{r}$はさらに高次 (higher-level)の潜在変数 $ boldsymbol{r}^h$により、次式で表現されます。 . $$ boldsymbol{r} = boldsymbol{r}^{td}+ boldsymbol{n}^{td}=f(U^h boldsymbol{r}^h)+ boldsymbol{n}^{td} tag{2} $$ただし、Top-downの予測信号を $ boldsymbol{r}^{td}:=f(U^h boldsymbol{r}^h)$としました。また、$ boldsymbol{r}^{td} in mathbb{R}^{n_1}, boldsymbol{r}^{h} in mathbb{R}^{n_2}, U^h in mathbb{R}^{n_1 times n_2} $です。$ boldsymbol{n}^{td} in mathbb{R}^{n_1} $は平均0, 分散 $ sigma_{td}^2$ のGaussian ノイズ項とします。 . 話は飛びますが、Predictive codingのネットワークは下図のように表されます。特徴は . 階層的な構造 | 高次による低次の予測 (Feedback or Top-down信号) | 低次から高次への誤差信号の伝搬 (Feedforward or Bottom-up 信号) | . です。 . . (Rao and Ballard, 1999; Fig. 1a) ここまで、高次表現による低次表現の予測、というFeedback信号について説明しました。それではPredictive codingのもう一つの要となる、低次から高次への予測誤差の伝搬というFeedforward信号はどのように導かれるのでしょうか。結論から言えば、これは復元誤差 (reconstruction error)の最小化を行う再帰的ネットワーク (recurrent network)を考慮することで自然に導かれます。 . &#29983;&#25104;&#12514;&#12487;&#12523;&#30340;&#34920;&#29694;&#12392;MAP&#25512;&#23450; . &#23588;&#24230;&#12398;&#35373;&#23450; . ネットワークの損失関数を導くために生成モデル (generative model)的な表現をしてみましょう。潜在変数 (あるいはCauses) $ boldsymbol{r}$の事前分布 (prior)を$p( boldsymbol{r})$, 入力画像 $ boldsymbol{I}$の尤度 (likelihood)を$p( boldsymbol{I}| boldsymbol{r}; U)$とします。(1)式およびノイズ項を$ boldsymbol{n} sim mathcal{N}(0, sigma^2)$としたことから、 . $$ begin{align} p( boldsymbol{I}| boldsymbol{r}; U)&amp;= mathcal{N} left( boldsymbol{I}| f(U boldsymbol{r}), sigma^2 right) &amp; varpropto exp left(- frac{ | boldsymbol{I} - f(U boldsymbol{r}) |^2}{ sigma^2} right) end{align} $$と表せます。 . &#20107;&#21069;&#20998;&#24067;&#12398;&#35373;&#23450; . 事前分布$p( boldsymbol{r})$に対してもGaussian分布を用いてもよいですが、 Sparse coding を考えると異なる分布を用いた方がよいでしょう。Sparse codingでは、$ boldsymbol{r}$の各要素$r_i$はほとんど0に等しく、ある入力に対しては大きな値を取る、というのが理想的です。この性質を考慮すると$p( boldsymbol{r})$としては、0においてピークがあり、裾の重い(heavy tail)を持つsparse distributionあるいは super-Gaussian distribution (Laplace 分布やCauchy分布などGaussian分布よりもkurtoticな分布)を用いるのが良いでしょう。ここではGaussian分布またはCauchy分布の場合について考えてみます。なお、後で用いるために$p( boldsymbol{r})$の負の対数事前分布を$g( boldsymbol{r}):=- log p( boldsymbol{r})$とします。 . Gaussian &#20998;&#24067;&#12434;&#29992;&#12356;&#12427;&#22580;&#21512; . $$ begin{align} p( boldsymbol{r})&amp;= exp(- alpha | boldsymbol{r} |^2) g( boldsymbol{r})&amp;=- ln p( boldsymbol{r})= alpha | boldsymbol{r} |^2 g&#39;( boldsymbol{r})&amp;= frac{ partial g( boldsymbol{r})}{ partial boldsymbol{r}}=2 alpha boldsymbol{r} end{align} tag{3} $$Cauchy&#20998;&#24067;&#12434;&#29992;&#12356;&#12427;&#22580;&#21512; . $$ begin{align} p( boldsymbol{r})&amp;= prod_i p(r_i)= prod_i exp left[- alpha ln(1+r_i^2) right] g( boldsymbol{r})&amp;=- ln p( boldsymbol{r})= alpha sum_i ln(1+r_i^2) g&#39;( boldsymbol{r})&amp;= frac{ partial g( boldsymbol{r})}{ partial boldsymbol{r}}= left[ frac{2 alpha r_i}{1+r_i^2} right]_i end{align} tag{4} $$いずれの分布を用いても良いですが、今回はCauchy分布を用います。 . 一方で、重み行列$U$の事前分布 $p(U)$はGaussian分布としておきます。$p(U)$の負の対数事前分布を$h(U):=- ln p(U)$とすると、次のように表されます。 . $$ begin{align} p(U)&amp;= exp(- lambda |U |^2_F) h(U)&amp;=- ln p(U)= lambda |U |^2_F h&#39;(U)&amp;= frac{ partial h(U)}{ partial U}=2 lambda U end{align} tag{5} $$ただし、$ | cdot |_F^2 $はフロベニウスノルムを意味します。 . MAP&#25512;&#23450;&#12395;&#12424;&#12427;&#12497;&#12521;&#12513;&#12540;&#12479;&#12398;&#26368;&#36969;&#21270; . MAP推定(最大事後確率推定)によりパラメータ $ boldsymbol{r}, U$を最適化することを考えます。まず、事後分布 (posterior)は . $$ p( boldsymbol{r}, U| boldsymbol{I})=k cdot p( boldsymbol{I}| boldsymbol{r}, U) p( boldsymbol{r})p(U) tag{6} $$となります。ただし$k$は定数です。次に対数事後確率$F( boldsymbol{r}, U):= ln p( boldsymbol{r}| boldsymbol{I};U)$を考えます。 . $$ begin{align} F( boldsymbol{r}, U) &amp;= ln p( boldsymbol{I}| boldsymbol{r}, U)+ ln p( boldsymbol{r})+ ln p(U)+ ln k &amp;=- frac{1}{ sigma^2} | boldsymbol{I} - f(U boldsymbol{r}) |^2-g( boldsymbol{r})-h(U)+ ln k end{align} tag{7} $$この$F$を最大化すれば$p( boldsymbol{r}, U| boldsymbol{I})$も最大となります。このためには勾配上昇法 (gradient ascent)を用いればよいです。ただ、一般のニューラルネットワークモデルと合わせるために$F$の符号を反転させて定数項を除いたものを損失関数 $E ( approx -F)$ とし、この$E$を最小化するために勾配降下法 (gradient descent)を適応することを次節で考えます。 . &#25613;&#22833;&#38306;&#25968;&#12392;&#23398;&#32722;&#21063; . &#25613;&#22833;&#38306;&#25968;&#12398;&#35373;&#23450; . 前節では2層までのパラメータを最適化することを考えましたが、高次の活動も考慮して損失関数 $E$を次のように再定義します。 . $$ begin{align} E= underbrace{ frac{1}{ sigma^{2}} | boldsymbol{I}-f(U boldsymbol{r}) |^2+ frac{1}{ sigma_{t d}^{2}} left | boldsymbol{r}-f(U^h boldsymbol{r}^h) right |^2}_{ text{reconstruction error}}+ underbrace{g( boldsymbol{r})+g( boldsymbol{r}^{h})+h(U)+h(U^h)}_{ text{sparsity penalty}} tag{8} end{align} $$&#20877;&#24112;&#12493;&#12483;&#12488;&#12527;&#12540;&#12463;&#12398;&#26356;&#26032;&#21063; . 簡単のために$ boldsymbol{x}:=U boldsymbol{r}, boldsymbol{x}^h:=U^h boldsymbol{r}^h$とします。 . $$ begin{align} frac{d boldsymbol{r}}{d t}&amp;=- frac{k_{1}}{2} frac{ partial E}{ partial boldsymbol{r}}=k_{1} cdot Bigg( frac{1}{ sigma^{2}} U^{T} bigg[ frac{ partial f( boldsymbol{x})}{ partial boldsymbol{x}} odot underbrace{( boldsymbol{I}-f( boldsymbol{x}))}_{ text{bottom-up error}} bigg]- frac{1}{ sigma_{t d}^{2}} underbrace{ left( boldsymbol{r}-f( boldsymbol{x}^h) right)}_{ text{top-down error}}- frac{1}{2}g&#39;( boldsymbol{r}) Bigg) tag{9} frac{d boldsymbol{r}^h}{d t}&amp;=- frac{k_{1}}{2} frac{ partial E}{ partial boldsymbol{r}^h}=k_{1} cdot Bigg( frac{1}{ sigma_{t d}^{2}}(U^h)^T bigg[ frac{ partial f( boldsymbol{x}^h)}{ partial boldsymbol{x}^h} odot underbrace{ left( boldsymbol{r}-f( boldsymbol{x}^h) right)}_{ text{bottom-up error}} bigg]- frac{1}{2}g&#39;( boldsymbol{r}^h) Bigg) tag{10} end{align} $$ただし、$k_1$は更新率 (updating rate)です。または、発火率の時定数を$ tau:=1/k_1$として、$k_1$は発火率の時定数$ tau$の逆数であると捉えることもできます。ここで(9)式において、中間表現 $ boldsymbol{r}$ のダイナミクスはbottom-up errorとtop-down errorで記述されています。このようにbottom-up errorが $ boldsymbol{r}$ への入力となることは自然に導出されます。なお、top-down errorに関しては高次からの予測 (prediction)の項 $f( boldsymbol{x}^h)$とleaky-integratorとしての項 $- boldsymbol{r}$に分割することができます。また$U^T, (U^h)^T$は重み行列の転置となっており、bottom-upとtop-downの投射において対称な重み行列を用いることを意味しています。$-g&#39;( boldsymbol{r})$は発火率を抑制してスパースにすることを目的とする項ですが、無理やり解釈をすると自己再帰的な抑制と言えます。 . (9), (10)式の処理をまとめると下図のようになります。各ステップが１つずつ実行されるのではなく、連続時間RNNであることに注意しましょう。 . . (Rao and Ballard, 1999; Fig. 1b) なお、$ boldsymbol{r}, boldsymbol{r}^h$の初期値はそれぞれ $ boldsymbol{r}(0) = U^T boldsymbol{I}, boldsymbol{r}^h(0)=(U^h)^T boldsymbol{r}$としておきます。誤差が計算されるまでは通常のfeedforwordのネットワークと同様の処理をすると見なして良いでしょう。 . &#37325;&#12415;&#34892;&#21015;&#12398;&#23398;&#32722;&#21063; . 重み行列 $U, U^h$についても . $$ begin{align} frac{d U}{d t}&amp;=- frac{k_{2}}{2} frac{ partial E}{ partial U}=k_2 cdot Bigg( frac{1}{ sigma^{2}} bigg[ frac{ partial f( boldsymbol{x})}{ partial boldsymbol{x}} odot ( boldsymbol{I}-f( boldsymbol{x})) bigg] boldsymbol{r}^{T}- frac{1}{2}h&#39;(U) Bigg) tag{11} frac{d U^h}{d t}&amp;=- frac{k_{2}}{2} frac{ partial E}{ partial U^h}=k_2 cdot Bigg( frac{1}{ sigma_{td}^{2}} bigg[ frac{ partial f( boldsymbol{x}^h)}{ partial boldsymbol{x}^h} odot( boldsymbol{r}-f( boldsymbol{x}^h)) bigg] ( boldsymbol{r}^h)^{T}- frac{1}{2}h&#39;(U^h) Bigg) tag{12} end{align} $$更新則の中にはHebb則 (特にOja&#39;s rule)を表す項$( boldsymbol{I}-f( boldsymbol{x})) boldsymbol{r}^{T}$が含まれます。ここで、$ boldsymbol{r}$がシナプス前細胞の発火率、$( boldsymbol{I}-f( boldsymbol{x}))$がシナプス後細胞の発火率を表します。 . Predictive coding network&#12398;&#23455;&#35013; . &#12493;&#12483;&#12488;&#12527;&#12540;&#12463;&#12398;&#27083;&#36896; . (Rao &amp; Ballard, 1999)におけるネットワークを実装しましょう。ネットワークは入力層(level-0 module), 3つのlevel-1 module, 1つのlevel-2 moduleから構成されます。 . . (Rao and Ballard, 1999; Fig. 1c) まず、白色化された自然画像をDoG (difference of Gaussians)フィルタで処理します。DoGフィルタはLGNのモデルですが、今回は使用しません (ただし、関数としての実装例は示しておきます)。次に前処理した自然画像から16 × 26 pxの画像パッチをランダムに切り出し、そこからさらに3つの16 × 16 pxの画像パッチ($I_0, I_1, I_2$)を切り出して3つのlevel-1 moduleへの入力とします (ただし水平に5pxずつオーバーラップさせて切り出します)。さらに入力前に16 × 16のGaussianフィルタを乗じます。 . level-0 moduleの次元は3 × 16 × 16となりますが、平坦化 (flatten)して3 × 256とします。 . 1つのlevel-1 moduleは32ニューロンから構成され、これが3つあるので計96ニューロンとなります。ただし、重み共有 (weight sharing)を行うことで、学習効率を向上させます。さらにlevel-2 moduleは64ニューロンで構成され、96ニューロンにフィードバックを返します。 . &#12521;&#12452;&#12502;&#12521;&#12522;&#12398;import . import cv2 import numpy as np import matplotlib.pyplot as plt import matplotlib.patches as patches from tqdm.notebook import tqdm import scipy.io as sio np.random.seed(0) . &#20837;&#21147;&#30011;&#20687;&#12398;&#21069;&#20966;&#29702; . 入力画像の前処理用関数を定義します。前述したようにDoGフィルタは使用しません。 . # DoG filter as a model of LGN def DoG(img, ksize=(5,5), sigma=1.3, k=1.6): g1 = cv2.GaussianBlur(img, ksize, sigma) g2 = cv2.GaussianBlur(img, ksize, k*sigma) dog = g1 - g2 return (dog - dog.min())/(dog.max()-dog.min()) # Gaussian mask for inputs def GaussianMask(sizex=16, sizey=16, sigma=5): x = np.arange(0, sizex, 1, float) y = np.arange(0, sizey, 1, float) x, y = np.meshgrid(x,y) x0 = sizex // 2 y0 = sizey // 2 mask = np.exp(-((x-x0)**2 + (y-y0)**2) / (2*(sigma**2))) return mask / np.sum(mask) . 画像ファイル(IMAGES.matおよびIMAGES_RAW.mat)を読み込みます。この画像ファイルは(Olshausen &amp; Field. Nature. 1996)で用いられた10枚の自然画像(natural images)でhttp://www.rctn.org/bruno/sparsenet/からダウンロードできます。IMAGES.matは白色化(whitening)された画像で、IMAGES_RAW.matには元画像が保存されています。 . # Preprocess of inputs num_images = 10 num_iter = 5000 # datasets from http://www.rctn.org/bruno/sparsenet/ mat_images = sio.loadmat(&#39;datasets/IMAGES.mat&#39;) imgs = mat_images[&#39;IMAGES&#39;] mat_images_raw = sio.loadmat(&#39;datasets/IMAGES_RAW.mat&#39;) imgs_raw = mat_images_raw[&#39;IMAGESr&#39;] . どんな画像が保存されているか表示してみましょう。ここでは白色化前のデータを使用します。 . # Plot datasets fig = plt.figure(figsize=(8, 4)) for i in range(10): plt.subplot(2, 5, i+1) plt.imshow(imgs_raw[:,:,i], cmap=&quot;gray&quot;) plt.axis(&quot;off&quot;) plt.tight_layout() fig.suptitle(&quot;Natural Images&quot;, fontsize=20) plt.subplots_adjust(top=0.9) . ここで、画像を読み込み、1つのパッチを作成するコードを確認のために記述します。赤枠の部分が切り取った画像パッチです。 . # Get image from imglist img = imgs[:, :, 0] H, W = img.shape # Get the coordinates of the upper left corner of clopping image randomly. beginx = np.random.randint(0, W-27) beginy = np.random.randint(0, H-17) img_clopped = img[beginy:beginy+16, beginx:beginx+26] # Clop three inputs inputs = [img_clopped[:, 0:16], img_clopped[:, 5:21], img_clopped[:, 10:26]] # Show clopped images plt.figure(figsize=(5,10)) ax1 = plt.subplot(1,2,1) plt.title(&quot;Orignal image&quot;) plt.imshow(img, cmap=&quot;gray&quot;) ax1.add_patch(patches.Rectangle(xy=(beginx, beginy), width=26, height=16, ec=&#39;red&#39;, fill=False)) ax2 = plt.subplot(1,2,2) plt.title(&quot;Clopped image&quot;) plt.imshow(img_clopped, cmap=&quot;gray&quot;) plt.tight_layout() plt.show() . &#12493;&#12483;&#12488;&#12527;&#12540;&#12463;&#12398;&#23455;&#35013; . ネットワークを実装します。__init__関数でネットワークの初期化をし、__call__関数でネットワークの1ステップの計算を行います。また、以下の実装では$f(x)=x$としました (元のディレクトリにあるnetwork.pyには$f(x)= tanh(x)$の実装も付けていますが、結果があまり変わりませんでした)。よってこのネットワークは線形となります。 . class RaoBallard1999Model: def __init__(self, dt=1, sigma2=1, sigma2_td=10): self.dt = dt self.inv_sigma2 = 1/sigma2 # 1 / sigma^2 self.inv_sigma2_td = 1/sigma2_td # 1 / sigma_td^2 self.k1 = 0.3 # k_1: update rate self.k2 = 0.2 # k_2: learning rate self.lam = 0.02 # sparsity rate self.alpha = 1 self.alphah = 0.05 self.num_units_level0 = 256 self.num_units_level1 = 32 self.num_units_level2 = 128 self.num_level1 = 3 U = np.random.randn(self.num_units_level0, self.num_units_level1) Uh = np.random.randn(int(self.num_level1*self.num_units_level1), self.num_units_level2) # Xavier initialization self.U = U.astype(np.float32) * np.sqrt(2/(self.num_units_level0+self.num_units_level1)) self.Uh = Uh.astype(np.float32) * np.sqrt(2/(int(self.num_level1*self.num_units_level1)+self.num_units_level2)) self.r = np.zeros((self.num_level1, self.num_units_level1)) self.rh = np.zeros((self.num_units_level2)) def initialize_states(self, inputs): self.r = np.array([self.U.T @ inputs[i] for i in range(self.num_level1)]) self.rh = self.Uh.T @ np.reshape(self.r, (int(self.num_level1*self.num_units_level1))) def calculate_total_error(self, error, errorh): recon_error = self.inv_sigma2*np.sum(error**2) + self.inv_sigma2_td*np.sum(errorh**2) sparsity_r = self.alpha*np.sum(self.r**2) + self.alphah*np.sum(self.rh**2) sparsity_U = self.lam*(np.sum(self.U**2) + np.sum(self.Uh**2)) return recon_error + sparsity_r + sparsity_U def __call__(self, inputs, training=False): # inputs : (3, 256) r_reshaped = np.reshape(self.r, (int(self.num_level1*self.num_units_level1))) # (96) fx = np.array([self.U @ self.r[i] for i in range(self.num_level1)]) # (3, 256) fxh = self.Uh @ self.rh # (96, ) # Calculate errors error = inputs - fx # (3, 256) errorh = r_reshaped - fxh # (96, ) errorh_reshaped = np.reshape(errorh, (self.num_level1, self.num_units_level1)) # (3, 32) g_r = self.alpha * self.r / (1 + self.r**2) # (3, 32) g_rh = self.alphah * self.rh / (1 + self.rh**2) # (64, ) # Update r and rh dr = self.inv_sigma2 * np.array([self.U.T @ error[i] for i in range(self.num_level1)]) - self.inv_sigma2_td * errorh_reshaped - g_r dr *= self.k1 drh = self.inv_sigma2_td * self.Uh.T @ errorh - g_rh drh *= self.k1 self.r += dr self.rh += drh # Update weights if training: dU = self.inv_sigma2 * np.sum(np.array([np.outer(error[i], self.r[i]) for i in range(self.num_level1)]), axis=0) - 3*self.lam * self.U dUh = self.inv_sigma2_td * np.outer(errorh, self.rh) - self.lam * self.Uh self.U += self.k2 * dU self.Uh += self.k2 * dUh return error, errorh, dr, drh . 元画像における赤枠の範囲が切り取った画像パッチです。右の画像パッチを更に3つに分割して、Gaussianフィルタを乗じた後にネットワークに入力します。 . &#12514;&#12487;&#12523;&#12398;&#23450;&#32681; . 上で実装したネットワークをmodelとして定義します。また、シミュレーションで用いる定数などを初期化しておきます。 . # Define model model = RaoBallard1999Model() # Simulation constants H, W, num_images = imgs.shape nt_max = 1000 # Maximum number of simulation time eps = 1e-3 # small value which determines convergence input_scale = 40 # scale factor of inputs gmask = GaussianMask() # Gaussian mask error_list = [] # List to save errors . &#12471;&#12511;&#12517;&#12524;&#12540;&#12471;&#12519;&#12531;&#12398;&#23455;&#34892; . シミュレーションを実行します。外側のfor loopでは画像パッチの作成とrとrhの初期化 (すなわちfeedforward処理の計算)を行います。内側のfor loopではrおよびrhが収束するまで更新を行い、収束したときに重み行列UおよびUhを更新します。また、model.k2は40画像パッチごとに1.015で除します。 . for iter_ in tqdm(range(num_iter)): # Get images randomly idx = np.random.randint(0, num_images) img = imgs[:, :, idx] # Get the coordinates of the upper left corner of clopping image randomly. beginx = np.random.randint(0, W-27) beginy = np.random.randint(0, H-17) img_clopped = img[beginy:beginy+16, beginx:beginx+26] # Clop three inputs inputs = np.array([(gmask*img_clopped[:, i*5:i*5+16]).flatten() for i in range(3)]) inputs = (inputs - np.mean(inputs)) * input_scale # Reset states model.initialize_states(inputs) # Input an image patch until latent variables are converged for i in range(nt_max): # Update r and rh without update weights error, errorh, dr, drh = model(inputs, training=False) # Compute norm of r and rh dr_norm = np.linalg.norm(dr, ord=2) drh_norm = np.linalg.norm(drh, ord=2) # Check convergence od r and rh, then update weights if dr_norm &lt; eps and drh_norm &lt; eps: error, errorh, dr, drh = model(inputs, training=True) break # If failure to convergence, break and print error if i &gt;= nt_max-2: print(&quot;Error at patch:&quot;, iter_) print(dr_norm, drh_norm) break error_list.append(model.calculate_total_error(error, errorh)) # Append errors # Decay learning rate if iter_ % 40 == 39: model.k2 /= 1.015 # Print moving average error if iter_ % 1000 == 999: print(&quot;iter: &quot;+str(iter_+1)+&quot;/&quot;+str(num_iter)+&quot;, Moving error:&quot;, np.mean(error_list[iter_-999:iter_])) . iter: 1000/5000, Moving error: 2.1590278564615817 iter: 2000/5000, Moving error: 1.692464205507875 iter: 3000/5000, Moving error: 1.6900546798398537 iter: 4000/5000, Moving error: 1.7189523951808592 iter: 5000/5000, Moving error: 1.7695244313404115 . &#35347;&#32244;&#20013;&#12398;&#25613;&#22833;&#12398;plot . 訓練中の損失(の移動平均)をplotする。 . def moving_average(a, n=100) : ret = np.cumsum(a, dtype=float) ret[n:] = ret[n:] - ret[:-n] return ret[n - 1:] / n moving_average_error = moving_average(np.array(error_list)) plt.figure(figsize=(5, 3)) plt.ylabel(&quot;Error&quot;) plt.xlabel(&quot;Iterations&quot;) plt.plot(np.arange(len(moving_average_error)), moving_average_error) plt.show() . Level-1 module&#12398;&#37325;&#12415;&#34892;&#21015;&#12398;plot . 学習後の重み行列を可視化してみましょう。中央部にしか受容野が無いように見えますが、これは画像パッチの入力時にGaussianフィルタを乗じているためです。このため、端に受容野が見えるということはこのネットワークではまずありません。 . # Plot Receptive fields of level 1 fig = plt.figure(figsize=(8, 4)) for i in range(32): plt.subplot(4, 8, i+1) plt.imshow(np.reshape(model.U[:, i], (16, 16)), cmap=&quot;gray&quot;) plt.axis(&quot;off&quot;) plt.tight_layout() fig.suptitle(&quot;Receptive fields of level 1&quot;, fontsize=20) plt.subplots_adjust(top=0.9) plt.show() . 白色がON領域(興奮)、黒色がOFF領域(抑制)を表します。これは一次視覚野(V1)における単純型細胞(simple cell)の受容野に類似しています。 . なお、受容野について詳しくない場合は受容野 - 脳科学辞典を参照してください。 . Level-2 module&#12398;&#37325;&#12415;&#34892;&#21015;&#12398;plot . Level-2 moduleはそのままではplotできないので、Level-1 moduleの重み行列をかけて統合します。こうすることで入力パッチの全体 (16 × 26)のサイズの受容野が得られます。 . # Plot Receptive fields of level 2 zero_padding = np.zeros((80, 32)) U0 = np.concatenate((model.U, zero_padding, zero_padding)) U1 = np.concatenate((zero_padding, model.U, zero_padding)) U2 = np.concatenate((zero_padding, zero_padding, model.U)) U_ = np.concatenate((U0, U1, U2), axis = 1) Uh_ = U_ @ model.Uh fig = plt.figure(figsize=(8, 4)) for i in range(24): plt.subplot(4, 6, i+1) plt.imshow(np.reshape(Uh_[:, i], (16, 26), order=&#39;F&#39;), cmap=&quot;gray&quot;) plt.axis(&quot;off&quot;) plt.tight_layout() fig.suptitle(&quot;Receptive fields of level 2&quot;, fontsize=20) plt.subplots_adjust(top=0.9) plt.savefig(&quot;RF_level2.png&quot;) plt.show() . Level 1 moduleが単純型細胞のような受容野を持っていたので、その線形和であるLevel 2 moduleの受容野は複雑型細胞(complex cell)に類似していると言えます。ただ、複雑型細胞の受容野は元々可視化が困難(ON領域とOFF領域が重なり合うため)なので、この可視化から単純な比較はできません。 . それでは論文内の結果と比較をしてみましょう。下図はFig. 2b-cで、上段がlevel-1 moduleの, 下段がlevel-2 moduleの受容野です。概ね再現できまていることが分かります。 . . (Rao and Ballard, 1999; Fig. 2b-c) tanhを活性化関数に用いた場合は(Olshausen &amp; Field. Nature. 1996)のようなGaborフィルタが得られるようです (今回は再現できませんでしたが)。 . . (Rao and Ballard, 1999; Fig. 2d) なお、論文はこれだけでは終わらず、長方形(棒)を入力した時の端点(endstopping)に対する応答を実際のニューロンの出力と比較する、などを行っています。 . ICA&#12392;PCA&#12395;&#12424;&#12427;&#33258;&#28982;&#30011;&#20687;&#12398;&#20998;&#26512; . 参考までに自然画像に対してICAとPCAを実行してみます。結論から言えば、ICAを実行すればGaborフィルタ (局所的)が得られます。一方で、PCAを実行しても全体的なフィルタしか得られません (Sanger TD, 1989)。 . &#12521;&#12452;&#12502;&#12521;&#12522;&#12398;import . from sklearn.decomposition import FastICA, PCA . &#30011;&#20687;&#12497;&#12483;&#12481;&#12398;&#20316;&#25104; . W, H, num_images = imgs_raw.shape n_patchs = 20000 patchs_list = [] w, h = 16, 16 for i in tqdm(range(n_patchs)): i = np.random.randint(0, num_images) # Get the coordinates of the upper left corner of clopping image randomly. beginx = np.random.randint(0, W-w-1) beginy = np.random.randint(0, H-h-1) img_clopped = imgs_raw[beginy:beginy+h, beginx:beginx+w, i] patchs_list.append(img_clopped.flatten()) patchs = np.array(patchs_list) . . ICA&#12398;&#23455;&#34892; . # perform ICA n_comp = 64 ica = FastICA(n_components=n_comp) ica.fit(patchs) ica_filters = ica.components_ . # plot filters plt.figure(figsize=(5,5)) for i in tqdm(range(n_comp)): plt.subplot(8, 8, i+1) plt.imshow(np.reshape(ica_filters[i], (w, h)), cmap=&quot;gray&quot;) plt.axis(&quot;off&quot;) plt.tight_layout() plt.suptitle(&quot;ICA&quot;, fontsize=20) plt.subplots_adjust(top=0.9) plt.show() . . 他にはK-means法を用いることでもGaborフィルタを得ることはできます。 . Gabors / Primary Visual Cortex “Simple Cells” from an Image — skimage v0.13.1 docs | . PCA&#12398;&#23455;&#34892; . # perform PCA pca = PCA(n_components=n_comp) pca.fit(patchs) pca_filters = pca.components_ . # plot filters plt.figure(figsize=(5,5)) for i in tqdm(range(n_comp)): plt.subplot(8, 8, i+1) plt.imshow(np.reshape(pca_filters[i], (w, h)), cmap=&quot;gray&quot;) plt.axis(&quot;off&quot;) plt.tight_layout() plt.suptitle(&quot;PCA&quot;, fontsize=20) plt.subplots_adjust(top=0.9) plt.show() . . &#12414;&#12392;&#12417; . Predictive codingの初めの数理モデルを実装した | 誤差の順方向伝搬と予測の逆伝搬という構造は予測誤差(=自由エネルギー)を最小化するRNNを考慮することによって生まれた | . &#21442;&#32771;&#25991;&#29486; . &#35542;&#25991; . Rao RP, Ballard DH. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat Neurosci. 1999;2(1):79–87. | Olshausen BA, Field DJ. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature. 1996;381(6583):607–609. Data and Code | . GitHub&#12391;&#30906;&#35469;&#12375;&#12383;&#20182;&#12398;&#23455;&#35013;&#20363; . https://github.com/lorenzohonegger/PredictiveCodingModel | https://github.com/miyosuda/predictive_coding | . Rao&#20808;&#29983;&#12398;&#35611;&#32681; . Why Consider Probabilistic Models? Computational Reasons - PowerPoint PPT | 7.3 Sparse Coding and Predictive Coding - Networks that Learn: Plasticity in the Brain &amp; Learning (Rajesh Rao) | Coursera | .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/neuroscience/2020/04/25/predictive-coding.html",
            "relUrl": "/neuroscience/2020/04/25/predictive-coding.html",
            "date": " • Apr 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "PowerPointによるVOICEROID音声付きプレゼン動画の作成法",
            "content": "タイトルの通り、PowerPointでVOICEROID音声付きプレゼン動画を作成しました。VOICEROIDは読み上げ用音声合成ソフトのシリーズ製品です。動画編集ソフトを使おうかと思いましたが、PowerPointで完結させられたので備忘録として残します。なお、環境はWindows10です。また、VOICEROIDを使わずに自分の声を入れたい場合は「スライドショー」&gt;「スライドショーの記録」または「録音」を使いましょう。 . 作成した動画 . 阪医Python会の新入生歓迎用動画として作成しました。 . スライド版はこちら . 作成方法 . ① スライドを作成する . 普通にプレゼンするときのようにスライドを作成しました。ただし、 . キャラの立ち絵 | 読み上げ文の字幕 | . の2つのスペースは確保しました。立ち絵は無くても良いですが、発表者の姿が見えないという違和感を減少させられます。また、字幕に関してはVOICEROIDは結構発音の調整が難しいので何を言っているか理解するには必須だと感じました。動画を止めた時に理解もしやすいです。 . その他、調整したのはフォントです。普段の発表とは異なり、柔らかい印象を持たせたかったのでフォントはBIZ UDPゴシックを選びました。 . ② 各スライドの読み上げ文章を作成する . スライドごとに読み上げてもらう文章を作成しました。字幕を入れることを考えると1つのスライドで長々と話せないので、約100～130字程度 (1tweet程度)が適切でしょうか。長いと感じれば次のスライドに文章を移せば良いです。作成したら、字幕に貼り付けと個別のtxtファイルに保存を行います。 . ③ txtファイルをVOICEROIDに読み上げてもらう . ②で作成した音声をVOICEROIDに読み上げてもらいます (自分は東北きりたんを購入しました)。この時、難しい熟語や英単語、数式等はひらがな・カタカナに直す必要があります。また、音声を読み上げてもらうと文章のおかしさに気づくこともしばしばありました。調声を行ってよいと思ったら音声をwavファイルで保存します。 . ④ wavファイルを各スライドに貼り付ける . wavファイルを各スライドに1つずつ付けて行きます。苦行ですね。自動化したいところですが。このとき、将来的にスライドを使うなら音声無し版も別ファイルで保存しておきましょう。音声を貼り付ける際には、同時に「再生」&gt;「音声の開始タイミング」を自動に設定し、「スライドショーを実行中にサウンドのアイコンを隠す」にチェックを入れておきます。 . ⑤ スライド切替時間の設定 . ④で貼り付けた音声の長さに合わせて「画面切り替え」&gt;「画面切り替えのタイミング」&gt;「自動」にチェックを入れて、切り替え時間を音声の長さ + 1～2秒としました。気になる部分は個別で修正しましょう。 . ⑥ アニメーションの追加 . 画像を動かしたい場合などにはアニメーションを追加します。これは画面を切り替えるまでの時間で開始タイミングなどを設定します。 . ⑦ スライドショーで確認 . 完成したスライドをスライドショーで通してチェックします。この時違和感がある部分は修正しましょう。 . ⑧ 動画に出力 . 「ファイル」&gt;「名前を付けて保存」で動画形式で保存すれば良いです。このとき、mp4形式ではなくwmv形式にすることをお勧めします (Windowsの場合ですが)。自分だけではないようですが、mp4形式で保存すると、動画中にノイズ音がランダムに発生する、ということが起こりました。 . 感想 . 動画作成は初めてだったのでかなり時間がかかりましたが、見返せる良い資料ができたと個人的に思っています（大変だったのでしばらく次の動画は作りたくないと思いましたが）。 .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/presentation/2020/04/13/voiceroid_powerpoint.html",
            "relUrl": "/presentation/2020/04/13/voiceroid_powerpoint.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "脳が対側支配をする進化的な利点は何か",
            "content": "神経解剖を学ぶと、次のような疑問は自然と生じると思います。それは「なぜ脳では対側支配(contralateral innervation)があるのか」というものです。ここでの対側とは、身体の正中線に対し反対側のことを指します。例としては左運動野が右側の筋群を、右運動野 が 左側の筋群 を制御するといった対側制御(contralateral control)が挙げられます。何ともややこしい配線ですが、なぜ同側支配ではダメだったのでしょうか？対側支配をする利点はあるのでしょうか？ . かなり基本的な内容に対する疑問ではありますが、この問題はRamón y Cajalの1898年の論文に端を発します (Ramón y Cajal. 1898)。古くはHippocratesも、左頭部負傷が右の筋群の運動に影響を与えることを知っていたようです (Vulliemoz et al., Lancet Neurol. 2005)。しかしながら、神経科学の著名な教科書にもこれに対する明確な回答はありません (カンデル神経科学や神経科学-脳の探求-を一通りざっと見ただけですが)。幸い、英語でGoogle検索すると当然ながら同様の疑問を抱く人は多数いるようで、Quoraにおいていくつかの解答を見つけました (Why does our left hemisphere of brain control our right side of our body and the right our left? - Quora)。 . 【追記(2020/03/10)】WikipediaのContralateral brainのページとも内容が被っていることに気づきました。 . Quoraの記事で紹介されている論文を足掛かりに文献を調べた結果ですが、この問題に関する優れた総説として(Vulliemoz et al., Lancet Neurol. 2005; Mora et al., Neurosurg. Focus. 2019)があります。 . 本記事では、対側支配の神経解剖を復習しつつ、脳が対側支配をする進化的な利点の仮説を紹介していきます。なお、仮説ということを強調しているのはこれが進化的な話を含み、実験的証明が難しいためです。また、筆者は神経解剖学・神経発生学の教室でも研究をしていますが、本記事における内容に関しては全く研究していないのでご了承ください。 . 正中を横切る神経線維 . まず、正中を横切る神経線維は交叉(decussation)や交連(commissure)と言った名称がつけられています。ここではいくつかの経路を簡単にまとめましたが、詳しくは何処のご家庭にもある神経解剖の教科書を読みましょう (今回は寺島先生の神経解剖学講義ノートを参考にしました)。 . 視交叉(optic chiasma) . 視神経の交叉のことです。両生類、魚類、鳥類では視神経が完全に交叉します(完全交叉)が、ネコなどの食肉類や霊長類では50%の視神経のみが交叉します(半交叉)。 . 運動路における交叉 . 運動路で代表的なのが皮質脊髄路(corticospinal tract, CST; または錐体路とも)です。皮質脊髄路は主に運動性皮質L5の錐体細胞を起始とし、脊髄に終わる経路です。皮質脊髄路の途中で延髄腹側の錐体において神経線維が交叉しますが、これを錐体交叉(または運動交叉) (pyramidal decussation, motor decussation)と言います。錐体交叉では全ての神経が交叉するわけではないですが、交叉しなかった線維も脊髄の白交連(anterior white commissure)で最終的には交叉します。なので、皮質脊髄路線維は全て対側の運動ニューロンを支配することになります。他には赤核脊髄路 (rubrospinal tract)における腹側被蓋交叉(tegmental decussation)や、視蓋脊髄路 (tectospinal tract)における背側被蓋交叉(dorsal tegmental decussation)があります。 . 感覚路における交叉 . 感覚路も色々ありますが、頭部以外の体性感覚の伝導路であれば後索・内側毛帯系と脊髄視床路系があります。後索・内側毛帯系では毛帯交叉(sensory decussation, decussation of the lemniscus)、脊髄視床路系では脊髄の白交連を経由して対側の体性感覚野に至ります。 . 小脳の経路における交叉 . 念のため触れておきます。まず、小脳は同側支配です。例えば右小脳半球外側部を損傷すると、右側 (患側)に小脳性運動失調が現れます。この理由は、小脳からの出力線維が交叉しない(例えば非交叉性室頂核前庭線維)か、2回交叉するためです。2回交叉は裏の裏が表になるのと同じことで、例えば上記の症例における経路は、小脳半球外側部から視床VL核までで上小脳脚交叉があり、視床から大脳皮質は交叉無し、大脳皮質から皮質脊髄路の錐体交叉でもう一度交叉する、となっています。 . 神経線維の交叉に関連した疾患 . 神経系の発生においては軸索誘導分子(axon guidance molecules)の寄与が必要ですが、それらをコードする遺伝子に異常が生じると神経発生が正常に行われなくなります。そのような疾患の例として、クリッペル・ファイル症候群(Klippel-Feil syndrome)、 X連鎖性カルマン症候群(X-linked Kallmann’s syndrome)などでは非交叉性の皮質遠心性線維が形成されることで鏡像運動(mirror movements; 随意的な運動を行うときに、対側にも不随意的に運動が生じる症状)が生じます (Comer et al., Neural Dev. 2019)。しかしながら病態機序はいくつか考えられるようなので、詳細は書きません (cf. 脳科学辞典の鏡像運動の項目)。 . Ramón y Cajalの仮説 . この節は、Ramón y Cajalの仮説についての説明をします。なお、Cajalの発表の後となりますが英国の医師であったFrancis DixonもCajalと類似の説を提唱しています(Dixon, The Dublin Journal of Medical Science. 1907; Dixon, The Dublin Journal of Medical Science. 1918)。 . さて、Cajalの視交叉についての仮説は、脳内での外界の連続性を維持するためには交叉が必要である、ということです。次図は前方に眼があり視交叉が無い生物が矢を見た場合の図です。網膜で外界が反転されるため、交叉が無いと矢尻と矢羽が繋がった像となります。そのため、2つの視野像から脳内で外界を復元するには像を交叉させる必要があります。 . . (Ramón y Cajal. 1898; 図の間接的な引用元は Ramón Y Cajal. Histología del sistema nervioso del hombre y de los vertebrados. Tomo II. Segunda parte. 2012) . 次図の上部は、前述の通り視交叉により外界が正しく復元されることを示しています。また、完全な交叉ではない半交叉であることが立体視に重要であることも指摘していました。Cajalはさらに説を広げ、運動路と感覚路に交叉がある理由も視交叉と関連付けました。これが対側支配におけるCajalの仮説です。次図中央部では連続した外界が復元されていますが、このままでは像が反転しています。Cajalは視野の右側を体の右側と、視野の左側を体の左側と一致させるために、運動路と体性感覚路を交叉させる必要があると主張しました。 . . (Ramón y Cajal. 1898) しかし、Cajalの仮説は完全に誤っているわけではないものの、いくつかの問題点があることが指摘されています (de Lussanet &amp; Osse, Animal Biol. 2012)。まず、視交叉の説について、眼球は動くため、網膜像も動きます。そのため、位置合わせを完全に正確にすることはできません (予測により補間はしていると思われるので、Cajalが間違っているとは言えませんが)。次に、運動路と感覚路を関連させた説については、同側の視覚情報・感覚情報のみが運動野に至るというのは最適ではないということです。運動することを考えれば、両側の情報を統合して座標系を生み出した方がよいでしょうし、実際に脳はそうしています。 . ということで、Cajalの仮説は完全に誤りではないにせよ、間違っている点も見られるということになります。ただ、Cajalが偉大だったことに変わりはないなと、今回スケッチを見直して思いました。 . ねじれ仮説 (twist hypothesis) . 無脊椎動物から脊椎動物へ至る間に体のねじれが生じた、というのがねじれ仮説 (twist hypothesis) です。これは対側支配が生じた目的ではなく、その過程についての仮説となっています。 . ねじれ仮説には(Kinsbourne, Neuropsychology. 2013)の体性ねじれ説 (somatic twist hypothesis) と、(de Lussanet &amp; Osse, Animal Biol. 2012)の軸ねじれ説 (axial twist hypothesis) の2つが提案されています。まず、Kinsbourneの体性ねじれ説は、180度の回転が一回起こった、というものです。一方で、de LussanetとOsseの軸ねじれ説は2つの90度の回転が起こった、というものです。両者の差異は分かりづらいですが、de LussanetとOsseによって解説されています (de Lussanet &amp; Osse, Neuropsychology. 2015)。 . . (de Lussanet &amp; Osse, Neuropsychology. 2015, Fig.1) AはKinsbourneの体性ねじれ説を示しており、前口動物(Protostome)が脊椎動物(vertebrate)へと進化するに伴い、前脳と眼の領域が180度回転したことを表します。Bはde LussanetとOsseの軸ねじれ説で、黒線が背側、白線が腹側、点は眼となる領域、ovはoptic vesicleを意味します。重要なのはB2で、前脳と眼の領域が90度の時計回り、中脳より下が90度の反時計回りをすることで合計で180度のねじれが生まれます。 . どちらの仮説も視交叉や、交叉しない嗅覚経路、および小脳の同側支配などを説明します。しかし、de LussanetとOsseは軸ねじれ説であれば、心臓や腸管などの臓器が左右対称ではないこと、などが説明できると主張しています。また、両者の説の弱いところとして視交叉などでの全交叉は説明しても、一部の神経が同側に向かう半交叉は説明できない、といった点を挙げています。 . いずれにせよ、進化の過程で何らかのねじれが生じたことは誤りではないでしょうが、何故そのねじれが保存されてきたのでしょうか？やはり、そこには対側支配の利点があったのでしょう。次節からは対側支配(主に皮質脊髄路や赤核脊髄路の運動制御、脊髄視床路の体性感覚)の進化的利点についての仮説を見ていきます。 . 回避行動仮説 (avoidance behavior hypothesis) . 危険な刺激を知覚し回避行動を取るために体の構造の進化に伴って交叉が必要となった、というのが回避行動仮説 (avoidance behavior hypothesis) です (Vulliemoz et al., Lancet Neurol. 2005)。 . . (Vulliemoz et al., Lancet Neuro. 2005, Fig.2) まず、交叉が無く、手足も無い原始的な種の場合を考えます(図上部; なお図では魚のような見た目ですが、魚類の錐体路も交叉が存在するようです)。このような生物の場合、危険な刺激は右脳半球によって知覚され、右半身の筋肉の収縮を引き起こし、屈曲させることで危険を回避します。このとき、交叉していない原始的な経路 (網様体脊髄路や前庭脊髄路)を介して筋肉は制御されます。 . 一方で、四肢を持つ脊椎動物(図下部)は、左肢を伸ばすことにより、左側の刺激を回避しようとします。このとき、交叉のある(系統発生的に)新しい経路 (皮質脊髄路や赤核脊髄路)を介して筋肉は制御されます。 . 特に根拠についての記載はありませんでしたが、１つの説と言えます。 . 保護機構仮説 (protective mechanism hypothesis) . 対側支配であることは身体保護の観点で優れている、という説が保護機構仮説 (protective mechanism hypothesis) です (Whitehead &amp; Banihani, Laterality. 2014)。前節の回避行動仮説と体を守るという観点では似ていますが、少し異なります。これまでに紹介してきた論文と異なり、WhiteheadとBanihaniは中程度の損傷を受けた場合、対側支配の方が生存しやすいことを簡単な数理モデルを用いて説明しています。ここでは数理モデルの紹介は省略し、簡単なお気持ちだけ説明します。 . ここでは、生物の体が左右の脳半球(と神経)及び左右の筋群の4つから成ると考えます。両半身を損傷すると致命的となることが多いですが、片半身だけの損傷であれば生存することも可能です。このとき、例えば右脳半球と左筋群の組み合わせが損傷を受けることはあまりなく、どちらかというと転倒や衝突により片側の脳半球と筋群が傷害されやすいでしょう。 . 同側支配の場合、片側の脳半球と筋群が同時に損傷を受けると、損傷を受けた半球と損傷を受けた筋群という組み合わせにより片側は完全に動かすことができなくなります。一方で対側支配の場合、各脳半球は反対側を制御するため、例えば右半身を損傷した場合は、損傷した右半身を制御する無傷の左脳半球と、無傷の左側を制御する損傷した右脳半球という状況になります。無傷の左半球は損傷しているとは言え、一部の右側の筋群を動かすことができるでしょう。また、損傷した右半球であっても左側の筋群は無事であるためにある程度は制御できるでしょう。 . 片側が完全に機能しない場合に比べ、両側に影響はあるものの何とか動かすことができる場合の方が生存しやすいと仮定すると、交叉が保存されてきたことにも納得できます。 . まとめ . 本記事では主に運動野と感覚野の対側支配がどのように生じたのか、また何故対側支配が保存されてきたのか、ということについての仮説を見てきました (重ねて言いますが仮説止まりです)。現在提案されている説をまとめると、体がねじれることで対側支配が生まれ、それが生存にとって有利であったために保存されてきた、と言えます。冒頭に述べたように、進化が絡むと直接的な証拠というものを見つけづらいとは思いますが (他の研究例を知らないだけかもしれませんが)、将来は何らかの説に落ち着くのではないか、と思っています。 . 参考文献 . Carson RG. Neural pathways mediating bilateral interactions between the upper limbs. Brain Res Rev. 2005;49(3):641–662. doi:10.1016/j.brainresrev.2005.03.005 | Comer JD, Alvarez S, Butler SJ, Kaltschmidt JA. Commissural axon guidance in the developing spinal cord: from Cajal to the present day. Neural Dev. 2019;14(1):9. doi:10.1186/s13064-019-0133-1 | de Lussanet MH, Osse JW. Decussation as an axial twist: A comment on Kinsbourne (2013). Neuropsychology. 2015;29(5):713–714. doi:10.1037/neu0000163 | de Lussanet, MH, Osse, JW. An ancestral axial twist explains the contralateral forebrain and the optic chiasm in vertebrates. Animal Biol. 2012; 62: 193-216. doi:10.1163/157075611X617102. | Dixon AF. Why are the great motor and sensory tracts of the central nervous system crossed?. The Dublin Journal of Medical Science. 1907; 124, 1–4. doi:10.1007/BF02972358 | Dixon AF. Why are the cerebral motor and sensory cortical areas arranged in an inverted order?. The Dublin Journal of Medical Science. 1918; 145, 154–160. doi:10.1007/BF02958527 | Kinsbourne M. Somatic twist: a model for the evolution of decussation. Neuropsychology. 2013;27(5):511–515. doi:10.1037/a0033662 | Loosemore RG. The inversion hypothesis: A novel explanation for the contralaterality of the human brain. Biosci Hypotheses. 2009;2:375–382 | Mora C, Velásquez C, Martino J. The neural pathway midline crossing theory: a historical analysis of Santiago Rámon y Cajal’s contribution on cerebral localization and on contralateral forebrain organization. Neurosurg Focus. 2019;47(3):E10. doi:10.3171/2019.6.FOCUS19341 | Ramón y Cajal, S. Estructura del kiasma óptico y teoría general de los entrecruzamientos de las vías nerviosas. 1898. [german 1899, english 2004]. Rev. Trim. Microgràfica 3, 15–65. | Ramón Y Cajal, S. Histología del sistema nervioso del hombre y de los vertebrados. Tomo II. Segunda parte. Boletín Oficial del Estado. 2012. | Shinbrot T, Young W. Why decussate? Topological constraints on 3D wiring. Anat Rec (Hoboken). 2008;291(10):1278–1292. doi:10.1002/ar.20731 | Vulliemoz S, Raineteau O, Jabaudon D. Reaching beyond the midline: why are human brains cross wired?. Lancet Neurol. 2005;4(2):87–99. doi:10.1016/S1474-4422(05)00990-7 | Welniarz Q, Dusart I, Roze E. The corticospinal tract: Evolution, development, and human disorders. Dev Neurobiol. 2017;77(7):810–829. doi:10.1002/dneu.22455 | Whitehead L, Banihani S. The evolution of contralateral control of the body by the brain: is it a protective mechanism?. Laterality. 2014;19(3):325–339. doi:10.1080/1357650X.2013.824461 | .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/neuroscience/2020/03/07/contralateral_brain.html",
            "relUrl": "/neuroscience/2020/03/07/contralateral_brain.html",
            "date": " • Mar 7, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Pythonによる分位点回帰 (Quantile regression)",
            "content": "Distributional Reinforcement Learningの理解のために必要だったのでメモとして残しておきます (本当はexpectile regressionも書かなければならないですが)。内容としてはNumpyで勾配法により分位点回帰をする、というものになっています。 . &#20998;&#20301;&#28857;&#22238;&#24112; (Quantile Regression)&#12392;&#12399; . 通常の最小二乗法による線形回帰(Ordinary least squares regression)は、誤差が正規分布と仮定したときのX(説明変数)に対するY(目的変数)の期待値E[Y]を求めます。これに対して分位点回帰(quantile regression)では、Xに対するYの分布における分位点を通るような直線を引きます。 . 分位点(または分位数)についてですが、簡単なのが四分位数です。箱ひげ図などで出てきますが、例えば第一四分位数は分布を25:75に分ける数、第二四分位数(中央値)は分布を50:50に分ける数です。同様に$q$分位数($q$-quantile)というと分布を$q:1-q$に分ける数となっています。 . さて、分位点回帰に戻りましょう。下図は$x sim U(0, 5), quad y=3x+x cdot xi, quad xi sim N(0,1)$とした500個の点に対する分位点回帰です(コードは図の下にあります)。青い領域はX=1,2,3,4でのYの分布を示しています。紫、緑、黄色の直線はそれぞれ10, 50, 90%tile回帰の結果です。例えば50%tile回帰の結果は、Xが与えられたときのYの中央値(50%tile点)を通るような直線となっています。同様に90%tile回帰の結果は90%tile点を通るような直線となっています。 . #collapse-hide import numpy as np np.random.seed(0) from matplotlib import pyplot as plt from tqdm import tqdm def QuantileGradientDescent(X, y, init_theta, tau, lr=1e-4, num_iters=10000): theta = init_theta for i in range(num_iters): y_hat = X @ theta # predictions delta = y - y_hat # error indic = np.array(delta &lt;= 0., dtype=np.float32) # indicator grad = np.abs(tau - indic) * np.sign(delta) # gradient theta += lr * X.T @ grad # Update return theta def gaussian_func(x, mu, sigma): return (0.8/sigma)*np.exp( - (x - mu)**2 / (2 * sigma**2)) cmap = plt.cm.viridis(np.linspace(0., 1., 3)) # Generate Toy datas N = 500 # sample size x = np.random.rand(N)*5 y = 3*x + x*np.random.randn(N) X = np.ones((N, 2)) # design matrix X[:, 1] = x taus = np.array([0.1, 0.5, 0.9]) m = len(taus) Y = np.zeros((m, N)) # memory array for i in tqdm(range(m)): init_theta = np.zeros(2) # init variables theta = QuantileGradientDescent(X, y, init_theta, tau=taus[i]) y_hat = X @ theta Y[i] = y_hat # Results plot plt.figure(figsize=(5,4)) plt.title(&quot;Quantile Regression&quot;) plt.scatter(x, y, color=&quot;gray&quot;, s=5) # samples for i in range(m): plt.plot([min(x), max(x)], [min(Y[i]), max(Y[i])], linewidth=2, color=cmap[i], label=str(int(taus[i]*100))+&quot;%tile&quot;) # regression line for loc in range(1,5): noise_y = np.arange(0, 6*loc, 1e-3) noise_x = loc + gaussian_func(noise_y, 3*loc, loc) plt.fill_between(noise_x, -1, noise_y, color=&#39;#539ecd&#39;, linewidth=2, alpha=0.5) plt.plot(noise_x, noise_y, color=&#39;#539ecd&#39;, linewidth=2) plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;y&quot;) plt.ylim(0, 25) plt.legend() plt.tight_layout() plt.show() . . 100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00&lt;00:00, 7.67it/s] . 分位点回帰の利点としては、外れ値に対して堅牢(ロバスト)である、Yの分布が非対称である場合にも適応できる、などがあります (Das et al., Nat Methods. 2019)。 . それでは分位点回帰をPythonで行う方法を見ていきましょう。 . &#21246;&#37197;&#27861;&#12395;&#12424;&#12427;&#32218;&#24418;&#22238;&#24112; (&#26368;&#23567;&#20108;&#20055;&#27861;) . 最小二乗法による線形回帰と異なり、分位点回帰は解析的に求めることができません。そのため、数値的に勾配法で求めるのですが、一先ずは最小二乗法による回帰直線を勾配法で求めてみましょう。 . 簡単のために単回帰の場合を考えます。パラメータを$ theta in mathbb{R}^2$, サンプルサイズを$n$, 説明変数の計画行列を$n times 2$の行列$X$, 目的変数を$y in mathbb{R}^n$とします。ただし$X$と$y$は観測値です。$y$の予測値は$X theta$なので、誤差 $ delta in mathbb{R}^n$は $$ delta = y-X theta$$ と表せます (符号は逆のことが多いですが)。最小二乗法において最適化したい目的関数は$$L( delta)= sum_{i=1}^n delta_i^2 = | delta |^2= delta^T delta$$ であり、$$ frac{ partial L}{ partial theta}=- frac{1}{n} delta X$$ と表せるので、$ theta$の更新式は$ theta leftarrow theta + alpha cdot dfrac{1}{n} delta X$と書けます ($ alpha$は学習率です)。 . さて、これを実装したコードと結果は次のようになります。 . #collapse-hide # Ordinary least squares regression def OLSRegGradientDescent(X, y, init_theta, lr=1e-4, num_iters=10000): theta = init_theta for i in range(num_iters): y_hat = X @ theta # predictions delta = y - y_hat # error theta += lr * delta @ X # Update return theta # Generate Toy datas N = 500 # sample size x = np.linspace(0, 10, N) y = 3*x + x*np.random.randn(N) X = np.ones((N, 2)) # design matrix X[:, 1] = x # Gradient descent init_theta = np.zeros(2) # init variables lr = 1e-4 # learning rate num_iters = 1000 # training iterations theta = OLSRegGradientDescent(X, y, init_theta) y_hat = X @ theta # predictions # Results plot plt.figure(figsize=(5,4)) plt.title(&quot;Least Squares Regression&quot;) plt.scatter(x, y, color=&quot;gray&quot;, s=5) # samples plt.plot([min(x), max(x)], [min(y_hat), max(y_hat)], color=&#39;red&#39;) # regression line plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;y&quot;) plt.tight_layout() plt.show() . . 上図において赤色の線が回帰直線です。今回は誤差を正規分布としているので綺麗な結果ですが、実際には外れ値の影響を受けたりします。 . &#21246;&#37197;&#27861;&#12395;&#12424;&#12427;&#20998;&#20301;&#28857;&#22238;&#24112; . 本題の分位点回帰です。前節と同様の設定とします。ここで $ delta$の関数を $$ rho_{ tau}( delta)= left| tau- mathbb{I}_{ delta leq 0} right| cdot | delta|= left( tau- mathbb{I}_{ delta leq 0} right) cdot delta$$ とします。ただし、$ tau$は関心のある分位点(quantile)、$ mathbb{I}$は指示関数(indicator function)です。この場合、$ mathbb{I}_{ delta leq 0}$は$ delta gt 0$なら0, $ delta leq 0$なら1となります。このとき、分位点回帰の目的関数は $$L_{ tau}( delta) = sum_{i=1}^n rho_{ tau}( delta_i)$$ です。なぜこの目的関数の最適化が$ tau$-分位点の回帰となるかについてはQuantile regressionのWikipediaに詳細に書いてあります。また、$ rho_{ tau}( delta)$を色々な $ tau$についてplotすると次図のようになります。 . #collapse-hide delta = np.arange(-5, 5, 0.1) tau= np.arange(0.2, 1.0, 0.2) cmap = plt.cm.brg(np.linspace(0, 0.5, len(tau))) plt.figure(figsize=(4,3)) for i in range(len(tau)): indic = delta &lt;= 0 y = (tau[i]-indic)*delta plt.plot(delta, y, label=r&quot;$ tau=$&quot;+&quot;{0:.1f}&quot;.format(tau[i]), color=cmap[i]) plt.xlabel(&quot;Error&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend() plt.tight_layout() plt.show() . . それでは$L_ tau$を最小化するような$ theta$の更新式について考えていきましょう。まず、$$ frac{ partial rho_{ tau}( delta)}{ partial delta}= rho_{ tau}^{ prime}( delta)= left| tau- mathbb{I}_{ delta leq 0} right| cdot operatorname{sign}( delta)$$ です (ただしsignは符号関数)。さらに$$ frac{ partial L_{ tau}}{ partial theta}= frac{ partial L_{ tau}}{ partial delta} frac{ partial delta( theta)}{ partial theta}=- frac{1}{n} rho_{ tau}^{ prime}( delta) X$$ が成り立つので、$ theta$の更新式は$ theta leftarrow theta + alpha cdot dfrac{1}{n} rho_{ tau}^{ prime}( delta) X$と書けます ($ alpha$は学習率です)。ゆえに実装には前節のコードを少し修正すればよいです。 . #collapse-hide def QuantileRegGradientDescent(X, y, init_theta, tau, lr=1e-4, num_iters=10000): theta = init_theta for i in range(num_iters): y_hat = X @ theta # predictions delta = y - y_hat # error indic = np.array(delta &lt;= 0., dtype=np.float32) # indicator grad = np.abs(tau - indic) * np.sign(delta) # gradient theta += lr * grad @ X # Update return theta cmap = plt.cm.viridis(np.linspace(0., 1., 5)) # Generate Toy datas N = 500 # sample size x = np.random.rand(N)*5 y = 3*x + x*np.random.randn(N) X = np.ones((N, 2)) # design matrix X[:, 1] = x taus = np.array([0.01, 0.1, 0.5, 0.9, 0.99]) m = len(taus) Y = np.zeros((m, N)) # memory array for i in tqdm(range(m)): init_theta = np.zeros(2) # init variables theta = QuantileRegGradientDescent(X, y, init_theta, tau=taus[i]) y_hat = X @ theta # prediction Y[i] = y_hat # memory # Results plot plt.figure(figsize=(5,4)) plt.title(&quot;Quantile Regression&quot;) plt.scatter(x, y, color=&quot;gray&quot;, s=5) # samples for i in range(m): plt.plot([min(x), max(x)], [min(Y[i]), max(Y[i])], linewidth=2, color=cmap[i], label=r&quot;$ tau=$&quot;+str(taus[i])) # regression line plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;y&quot;) plt.legend() plt.tight_layout() plt.show() . . 100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00&lt;00:00, 7.95it/s] . ただし、分位点回帰を単純な勾配法で求める場合、勾配が0となって解が求まらない可能性があるので避けた方が良いという話はあります。そのために、目的関数を滑らかにするという研究もあります (Zheng. IJMLC. 2011)。 . Statsmodels&#12395;&#12424;&#12427;&#20998;&#20301;&#28857;&#22238;&#24112; . 前節のように分位点回帰は実装できますが、より簡単かつ高速に行うにはライブラリを用いるのがよいです。Statsmodelには分位点回帰のモデルがあり、それを最適化するだけで回帰直線が得られます。 . #collapse-hide import statsmodels.api as sm from statsmodels.regression.quantile_regression import QuantReg cmap = plt.cm.viridis(np.linspace(0., 1., 5)) # Generate Toy datas N = 500 # sample size x = np.random.rand(N)*5 y = 3*x + x*np.random.randn(N) X = sm.add_constant(x) model = QuantReg(y, X) taus = np.array([0.01, 0.1, 0.5, 0.9, 0.99]) m = len(taus) Y = np.zeros((m, N)) # memory array for i in range(m): results = model.fit(q=taus[i]) y_hat = X @ results.params Y[i] = y_hat plt.figure(figsize=(5,4)) plt.scatter(x, y, color=&quot;gray&quot;, s=5) # samples for i in range(m): plt.plot([min(x), max(x)], [min(Y[i]), max(Y[i])], linewidth=2, color=cmap[i], label=r&quot;$ tau=$&quot;+str(taus[i])) # regression line plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;y&quot;) plt.legend() plt.tight_layout() plt.show() . . &#21442;&#32771;&#25991;&#29486; . https://en.wikipedia.org/wiki/Quantile_regression | Das, K., Krzywinski, M. &amp; Altman, N. Quantile regression. Nat Methods 16, 451–452 (2019) doi:10.1038/s41592-019-0406-y | Quantile and Expectile Regressions (pdf) | .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/statistics/2020/01/21/quantile_regression.html",
            "relUrl": "/statistics/2020/01/21/quantile_regression.html",
            "date": " • Jan 21, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Distributional Reinforcement Learningの仕組み",
            "content": "この記事は(Dabney, et al., Nature. 2020)におけるDistributional Reinforcement Learningを実装しながら理解しようという内容です。解説とか言うのは恐れ多いので自分用のメモだと思ってください…。また、どちらかというと神経科学寄りの内容です(深層強化学習への応用については触れません)。 . この研究はDeepMindとHarvardの内田先生のラボの共同研究で、アニメーション付きの解説記事をDeepMindが書いています (DeepMindのブログ)。Botvinick氏と内田先生の講演をCiNetで聞いたにも関わらず理解が疎かだったのですが、論文が公開されたので、ちゃんと理解しておこうという次第です。また、コード(MATLAB, Python)も公開されており(https://doi.org/10.17605/OSF.IO/UX5RG) 、この記事ではこのコードをかなり参考にしています。 . Classical TD learning vs Distributional TD learning . Classical TD learning . TD (Temporal difference) learningにおいて、報酬予測誤差(reward prediction error, RPE) $ delta_{i}$は次のように計算されます (この式はDistributional TD learningでも共通です)。 $$ delta_{i}=r+ gamma V_{j} left(x^{ prime} right)-V_{i}(x) $$ ただし、現在の状態を$x$, 次の状態を$x&#39;$, 予測価値分布を$V(x)$, 報酬信号を$r$, 時間割引率(time discount)を$ gamma$としました。 また、$V_{j} left(x^{ prime} right)$は予測価値分布$V left(x^{ prime} right)$からのサンプルです。 このRPEは脳内において主に中脳のVTA(腹側被蓋野)やSNc(黒質緻密部)におけるドパミン(dopamine)ニューロンの発火率として表現されています。 . ただし、VTAとSNcのドパミンニューロンの役割は同一ではありません。ドパミンニューロンへの入力が異なっています (Watabe-Uchida et al., Neuron. 2012)00281-4)。 また、細かいですがドパミンニューロンの発火は報酬量に対して線形ではなく、やや飽和する非線形な応答関数 (Hill functionで近似可能)を持ちます(Eshel et al., Nat. Neurosci. 2016)。このため著者実装では報酬 $r$に非線形関数がかかっているものもあります。 . 先ほどRPEはドパミンニューロンの発火率で表現されている、といいました。RPEが正の場合はドパミンニューロンの発火で表現できますが、単純に考えると負の発火率というものはないため、負のRPEは表現できないように思います。ではどうしているかというと、RPEが0（予想通りの報酬が得られた場合）でもドパミンニューロンは発火しており、RPEが正の場合にはベースラインよりも発火率が上がるようになっています。逆にRPEが負の場合にはベースラインよりも発火率が減少する(抑制される)ようになっています (Schultz et al., Science. 1997; Chang et al., Nat Neurosci. 2016)。発火率というのを言い換えればISI (inter-spike interval, 発火間隔)の長さによってPREが符号化されている(ISIが短いと正のRPE, ISIが長いと負のRPEを表現)ともいえます (Bayer et al., J. Neurophysiol. 2007)。 . 予測価値(分布) $V(x)$ですが、これは線条体(striatum)のパッチ (SNcに抑制性の投射をする)やVTAのGABAニューロン (VTAのドパミンニューロンに投射して減算抑制をする, (Eshel, et al., Nature. 2015))などにおいて表現されています。 この予測価値は通常のTD learningでは次式により更新されます。 $$ V_{i}(x) leftarrow V_{i}(x)+ alpha_{i} f left( delta_{i} right) $$ ただし、$ alpha_{i}$は学習率(learning rate), $f( cdot)$はRPEに対する応答関数です。生理学的には$f( delta)= delta$を使うのが妥当ですが、後の分位数(quantile)モデルでは$f( delta)= text{sign}( delta)$を用います。 . Distributional TD learning . Distributional TD learningではRPEの正負に応じて、予測報酬の更新を異なる学習率($ alpha_{i}^{+}, alpha_{i}^{-}$)を用いて行います。 $$ begin{cases} V_{i}(x) leftarrow V_{i}(x)+ alpha_{i}^{+} f left( delta_{i} right) &amp; text{for } delta_{i} gt 0 V_{i}(x) leftarrow V_{i}(x)+ alpha_{i}^{-} f left( delta_{i} right) &amp; text{for } delta_{i} leq 0 end{cases} $$ ここで、シミュレーションにおいては$ alpha_{i}^{+}, alpha_{i}^{-} sim U(0, 1)$とします($U$は一様分布)。さらにasymmetric scaling factor $ tau_i$を次式により定義します。 $$ tau_i= frac{ alpha_{i}^{+}}{ alpha_{i}^{+}+ alpha_{i}^{-}} $$ なお、$ alpha_{i}^{+}, alpha_{i}^{-} in [0, 1]$より$ tau_i in [0,1]$です。 . Classical TD learningとDistributional TD learningにおける各ニューロンのRPEに対する発火率を表現したのが次図となります。 . #collapse-hide import numpy as np from matplotlib import pyplot as plt # Classical TD learning N = 10 cmap = plt.cm.brg(np.linspace(0, 0.5, N)) x = np.arange(-1, 1, 1e-2)[:, None] theta = np.linspace(np.pi/6, np.pi/3, N) alpha = np.tan(theta) y = alpha * x # Plot plt.figure(figsize=(8, 4)) def hide_ticks(): #上と右の軸を表示しないための関数 plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().yaxis.set_ticks_position(&#39;left&#39;) plt.gca().xaxis.set_ticks_position(&#39;bottom&#39;) plt.subplot(1,2,1) plt.axvline(x=0, color=&quot;gray&quot;, linestyle=&quot;dashed&quot;, linewidth=2) plt.axhline(y=0, color=&quot;gray&quot;, linestyle=&quot;dashed&quot;, linewidth=2) for i in range(N): if i == N//2: plt.plot(x, y[:, i], color=cmap[N//2], alpha=1, linewidth=3, label=&quot;Neutral&quot;) else: plt.plot(x, y[:, i], color=cmap[N//2], alpha=0.2) hide_ticks() plt.ylim(-1,1); plt.xlim(-1,1) plt.xticks([]); plt.yticks([]) plt.legend(loc=&#39;upper left&#39;) plt.title(&quot;Classical TD learning&quot;) plt.xlabel(&quot;RPE&quot;) plt.ylabel(&quot;Firing&quot;) # Distributional TD learning N = 20 cmap = plt.cm.brg(np.linspace(0, 0.5, N)) x = np.arange(-1, 1, 1e-2)[:, None] theta = np.linspace(np.pi/16, np.pi*7/16, N) alpha_pos = np.tan(theta) alpha_neg = np.tan(theta)[::-1] y = (alpha_pos*(x&gt;0) + (alpha_neg)*(x&lt;=0))*x # Plot ax = plt.subplot(1,2,2) plt.axvline(x=0, color=&quot;gray&quot;, linestyle=&quot;dashed&quot;, linewidth=2) plt.axhline(y=0, color=&quot;gray&quot;, linestyle=&quot;dashed&quot;, linewidth=2) for i in range(N): if i == 0: plt.plot(x, y[:, i], color=cmap[i], alpha=1, linewidth=3, label=&quot;Pessimistic&quot;) elif i == N//2: plt.plot(x, y[:, i], color=cmap[i], alpha=1, linewidth=3, label=&quot;Neutral&quot;) elif i == N-1: plt.plot(x, y[:, i], color=cmap[i], alpha=1, linewidth=3, label=&quot;Optimistic&quot;) else: plt.plot(x, y[:, i], color=cmap[i], alpha=0.2) hide_ticks() handles, labels = ax.get_legend_handles_labels() ax.legend(reversed(handles), reversed(labels), loc=&#39;upper left&#39;) plt.ylim(-1,1); plt.xlim(-1,1) plt.xticks([]); plt.yticks([]) plt.title(&quot;Distributional TD learning&quot;) plt.xlabel(&quot;RPE&quot;) plt.ylabel(&quot;Firing&quot;) plt.show() . . Classical TD learningではRPEに比例して発火する細胞しかありませんが、Distributional TD learningではRPEの正負に応じて発火率応答が変化していることがわかります。 特に$ alpha_{i}^{+} gt alpha_{i}^{-}$の細胞を楽観的細胞 (optimistic cells)、$ alpha_{i}^{+} lt alpha_{i}^{-}$の細胞を悲観的細胞 (pessimistic cells)と著者らは呼んでいます。実際には2群に分かれているわけではなく、gradientに遷移しています。楽観的・悲観的の意味に関しては後でも触れますが、ここではイメージだけお伝えしておきます。まず楽観的細胞ではRPEが正なら「結構もらえるやん」、RPEが負なら「まあそういうときもあるよね」となり最終的な予測価値は通常よりも高くなります。逆に悲観的細胞ではRPEが正なら「もらえたけどいつもそうではないやろ」、RPEが負なら「やっぱあんまもらえんよな」となり最終的な予測価値は通常よりも低くなります。収束する予測価値が細胞ごとに異なることで、$V$には報酬の期待値ではなく複雑な形状の報酬分布が符号化されます。その仕組みについて、次節から見ていきます。 . &#20998;&#20301;&#25968;(Quantile)&#12514;&#12487;&#12523;&#12392;&#22577;&#37228;&#20998;&#24067;&#12398;&#31526;&#21495;&#21270; . RPE&#12395;&#23550;&#12377;&#12427;&#24540;&#31572;&#12364;sign&#38306;&#25968;&#12398;&#12514;&#12487;&#12523;&#12392;&#22577;&#37228;&#20998;&#24067;&#12398;&#20998;&#20301;&#28857;&#12408;&#12398;&#20104;&#28204;&#20385;&#20516;&#12398;&#21454;&#26463; . さて、Distributional RLモデルでどのようにして報酬分布が学習されるかについてみていきます。この節ではRPEに対する応答関数$f( cdot)$が符合関数(sign function)の場合を考えます。結論から言うと、この場合はasymmetric scaling factor $ tau_i$は分位数(quantile)となり、予測価値 $V_i$は報酬分布の$ tau_i$分位数に収束します。 . どういうことかを簡単なシミュレーションで見てみましょう。今、報酬分布を平均2, 標準偏差5の正規分布とします (すなわち$r sim N(2, 5^2)$となります)。また、$ tau_i = 0.25, 0.5, 0.75 (i=1,2,3)$とします。このとき、3つの予測価値 $V_i (i=1,2,3)$はそれぞれ$N(2, 5^2)$の0.25, 0.5, 0.75分位数に収束します。下図はシミュレーションの結果です。左が$V_i$の変化で、右が報酬分布と0.25, 0.5, 0.75分位数の位置 (黒短線)となっています。対応する分位数に見事に収束していることが分かります。 . #collapse-hide import seaborn as sns from tqdm import tqdm from matplotlib import gridspec ############ ### init ### ############ response_func = lambda r: np.sign(r) # RPEの応答関数 num_cells = 3 # ニューロン(ユニット)の数 num_steps = 5000 # 訓練回数 base_lrate = 0.02 # ベースラインの学習率 reward_mu = 5 # 報酬の平均(正規分布) reward_sigma = 2 # 報酬の標準偏差(正規分布) distribution = np.zeros(num_cells) # 価値分布を記録する配列 dist_trans = np.zeros((num_steps, num_cells)) # 価値分布を記録する配列 alpha_pos = np.array([.1, .2, .3]) # RPEが正のときの学習率 alpha_neg = np.array([.3, .2, .1]) # RPEが負のときの学習率 tau = alpha_pos / (alpha_pos + alpha_neg) # Asymmetric scaling factor ############## # simulation # ############## for step in tqdm(range(num_steps)): # 25000 steps # 報酬がrandomに選ばれる reward = np.random.normal(reward_mu, reward_sigma, size=(1,)) # 報酬誤差(step毎に更新) reward応答をlinearとする delta = reward - distribution # (3, ) # deltaが負なら1, 正なら0 valence = np.array(delta &lt;= 0., dtype=np.float32) # (3, ) # 予測価値分布の更新 alpha = valence * alpha_neg + (1. - valence) * alpha_pos distribution += alpha * response_func(delta) * base_lrate dist_trans[step] = distribution # 予測価値分布変化の記録 ################ # Results plot # ################ steps = np.arange(num_steps) ylim = (0, 10) # y軸のlim gs = gridspec.GridSpec(1, 2, width_ratios=[1, 0.25]) plt.figure(figsize=(6,4)) plt.subplot(gs[0]) # 予測価値の変化 for i in range(num_cells): plt.plot(steps, dist_trans[:, i], label=str((i+1)*25)+&quot;%tile (&quot;+r&quot;$ tau=$&quot;+str((i+1)*0.25)+&quot;)&quot;) plt.title(&quot;Convergence of value prediction to n percentile of reward distribution&quot;) plt.xlim(0, num_steps) plt.ylim(ylim) plt.xlabel(&quot;Learning steps&quot;) plt.ylabel(&quot;Learned Value&quot;) plt.legend() # 報酬のサンプリング rewards = np.random.normal(reward_mu, reward_sigma, size=(1000,)) percentile = np.percentile(rewards, q=[25, 50, 75]) # 報酬の四分位数を取得 plt.subplot(gs[1]) # 報酬分布とその分位数 sns.kdeplot(rewards, bw=1, shade=True, vertical=True) sns.rugplot(percentile, color=&#39;k&#39;, lw=2, height=0.2, vertical=True) plt.title(&quot;Reward n distribution&quot;) plt.ylim(ylim) plt.xlabel(&quot;Density&quot;) plt.tight_layout() plt.show() . . 100%|███████████████████████████████████████████████████████████████████████████| 5000/5000 [00:00&lt;00:00, 79574.72it/s] . ここでoptimisticな細胞($ tau=0.75$)は中央値よりも高い予測価値、pessimisticな細胞($ tau=0.25$)は中央値よりも低い予測価値に収束しています。 つまり細胞の楽観度というものは、細胞が期待する報酬が大きいほど上がります。 . 同様のシミュレーションを今度は200個の細胞 (ユニット)で行います。報酬は0.1, 1, 2 μLのジュースがそれぞれ確率0.3, 0.6, 0.1で出るとします (Extended Data Fig.1と同じような分布にしています)。なお、著者らはシミュレーションとマウスに対してVariable-magnitude task (異なる量の報酬(ジュース)が異なる確率で出る)とVariable-probability task (一定量の報酬がある確率で出る)を行っています。以下はVariable-magnitude taskを行う、ということです。学習結果は次図のようになります。左はGround Truthの報酬分布で、右は$V_i$に対してカーネル密度推定 (KDE)することによって得た予測価値分布です。2つの分布はほぼ一致していることが分かります。 . #collapse-hide response_func = lambda r: np.sign(r) # RPEの応答関数 juice_amounts = np.array([0.1, 1, 2]) # reward(ジュース)の量(uL) juice_probs = np.array([0.3, 0.6, 0.1]) # 各ジュースが出る確率 num_cells = 200 # ニューロン(ユニット)の数 num_steps = 25000 # 訓練回数 base_lrate = 0.02 # ベースラインの学習率 distribution = np.zeros(num_cells) # 価値分布を記録する配列 alpha_pos = np.random.random(size=(num_cells)) # RPEが正のときの学習率 alpha_neg = np.random.random(size=(num_cells)) # RPEが負のときの学習率 tau = alpha_pos / (alpha_pos + alpha_neg) # Asymmetric scaling factor ############## # simulation # ############## for step in tqdm(range(num_steps)): # 25000 steps # 報酬がrandomに選ばれる reward = (np.random.choice(juice_amounts, p=juice_probs)) #(1, ) # 報酬誤差(step毎に更新) reward応答をlinearとする delta = reward - distribution # (200, ) # deltaが負なら1, 正なら0 valence = np.array(delta &lt;= 0., dtype=np.float32) # (200, ) # 予測価値分布の更新 alpha = valence * alpha_neg + (1. - valence) * alpha_pos distribution += alpha* response_func(delta) * base_lrate # tauの大きさでソートする ind = np.argsort(tau) tau = tau[ind] alpha_pos = alpha_pos[ind] alpha_neg = alpha_neg[ind] distribution = distribution[ind] ################ # Results plot # ################ # 報酬をサンプリング rewards = (np.random.choice(juice_amounts,size=1000, p=juice_probs)) # 結果の描画(価値・報酬分布) plt.figure(figsize=(8,4)) plt.subplot(1,2,1) # Ground Truth (Reward分布) plt.title(&quot;Reward distribution&quot;) sns.rugplot(rewards, color=&#39;k&#39;, lw=2, zorder=10) sns.kdeplot(rewards, bw=.15, color=&#39;k&#39;, lw=1., shade=True) plt.xlabel(&quot;Reward&quot;) plt.ylabel(&quot;Density&quot;) plt.subplot(1,2,2) # 学習後のValue(Reward)の分布 plt.title(&quot;Learned Value distribution&quot;) sns.kdeplot(distribution, bw=.15, color=&#39;k&#39;, lw=1., shade=True) sns.rugplot(distribution, color=&#39;k&#39;, lw=2, zorder=10) plt.xlabel(&quot;Value&quot;) plt.ylabel(&quot;Density&quot;) plt.tight_layout() plt.show() . . 100%|█████████████████████████████████████████████████████████████████████████| 25000/25000 [00:00&lt;00:00, 31986.89it/s] . そして$V_i$の経験累積分布関数(CDF)は$r$のサンプリングしたCDFとほぼ同一となっています (下図左)。また、$ tau_i$の関数である$V_i$は分位点関数 (quantile function)または累積分布関数の逆関数 (inverse cumulative distribution function)となっています (下図右)。右の図を転置すると左の青い曲線とだいたい一致しそうなことが分かります。 . #collapse-hide # 結果の描画(累積分布) plt.figure(figsize=(8,4)) plt.subplot(1,2,1) # 累積分布 sns.kdeplot(distribution, cumulative=True,bw=.05, label=&quot;Learned Value&quot;) sns.kdeplot(rewards, cumulative=True, bw=.05, label=&quot;Reward (GT)&quot;) plt.xlabel(&quot;Reward (Learned Value)&quot;) plt.ylabel(&quot;Cumulative probability&quot;) plt.subplot(1,2,2) # 累積分布 plt.plot(tau, distribution) plt.xlabel(&quot;Asymmetric scaling factors (&quot;+ r&quot;$ tau$)&quot;) plt.ylabel(&quot;Learned Value&quot;) plt.tight_layout() plt.show() . . sign&#38306;&#25968;&#12434;&#29992;&#12356;&#12383;Distributional RL&#12392;&#20998;&#20301;&#28857;&#22238;&#24112; . それでは、なぜ予測価値 $V_i$は$ tau_i$ 分位点に収束するのでしょうか。Extended Data Fig.1のように平衡点で考えてもよいのですが、後のために分位点回帰との関連について説明します。分位点回帰については記事を書いたので先にそちらを読んでもらうと分かりやすいと思います (→Pythonによる分位点回帰 (Quantile regression))。 . 実はDistributional RL (かつ、RPEの応答関数にsign関数を用いた場合)における予測報酬 $V_i$の更新式は、分位点回帰(Quantile regression)を勾配法で行うときの更新式とほとんど同じです。分位点回帰では$ delta$の関数$ rho_{ tau}( delta)$を次のように定義します。 $$ rho_{ tau}( delta)= left| tau- mathbb{I}_{ delta leq 0} right| cdot | delta|= left( tau- mathbb{I}_{ delta leq 0} right) cdot delta $$ そして、この関数を最小化することで回帰を行います。ここで$ tau$は分位点です。また$ delta=r-V$としておきます。今回、どんな行動をしても未来の報酬に影響はないので$ gamma=0$としています。 ここで、 $$ frac{ partial rho_{ tau}( delta)}{ partial delta}= rho_{ tau}^{ prime}( delta)= left| tau- mathbb{I}_{ delta leq 0} right| cdot operatorname{sign}( delta) $$ なので、$r$を観測値とすると、 $$ frac{ partial rho_{ tau}( delta)}{ partial V}= frac{ partial rho_{ tau}( delta)}{ partial delta} frac{ partial delta(V)}{ partial V}=- left| tau- mathbb{I}_{ delta leq 0} right| cdot operatorname{sign}( delta) $$ となります。ゆえに$V$の更新式は $$ V leftarrow V - beta cdot frac{ partial rho_{ tau}( delta)}{ partial V}=V+ beta left| tau- mathbb{I}_{ delta leq 0} right| cdot operatorname{sign}( delta) $$ です。ただし、$ beta$はベースラインの学習率です。個々の$V_i$について考え、符号で場合分けをすると $$ begin{cases} V_{i} leftarrow V_{i}+ beta cdot | tau_i| cdot operatorname{sign} left( delta_{i} right) &amp; text { for } delta_{i}&gt;0 V_{i} leftarrow V_{i}+ beta cdot | tau_i-1| cdot operatorname{sign} left( delta_{i} right) &amp; text { for } delta_{i} leq 0 end{cases} $$ となります。$0 leq tau_i leq 1$であり、$ tau_i= alpha_{i}^{+} / left( alpha_{i}^{+} + alpha_{i}^{-} right)$であることに注意すると上式は次のように書けます。 $$ begin{cases} V_{i} leftarrow V_{i}+ beta cdot frac{ alpha_{i}^{+}}{ alpha_{i}^{+}+ alpha_{i}^{-}} cdot operatorname{sign} left( delta_{i} right) &amp; text { for } delta_{i}&gt;0 V_{i} leftarrow V_{i}+ beta cdot frac{ alpha_{i}^{-}}{ alpha_{i}^{+}+ alpha_{i}^{-}} cdot operatorname{sign} left( delta_{i} right) &amp; text { for } delta_{i} leq 0 end{cases} $$ これは前節で述べたDistributional RLの更新式とほぼ同じです。いくつか違う点もありますが、RPEが正の場合と負の場合に更新される値の比は同じとなっています。 . このようにRPEの応答関数にsign関数を用いた場合、報酬分布を上手く符号化することができます。しかし実際のドパミンニューロンはsign関数のような生理的に妥当でない応答はせず、RPEの大きさに応じた活動をします。そこで次節ではRPEの応答関数を線形にしたときの話をします。 . Expectile &#12514;&#12487;&#12523;&#12392;&#12489;&#12497;&#12511;&#12531;&#12491;&#12517;&#12540;&#12525;&#12531;&#12363;&#12425;&#12398;&#22577;&#37228;&#20998;&#24067;&#12398;Decoding . RPE&#12395;&#23550;&#12377;&#12427;&#24540;&#31572;&#12364;&#32218;&#24418;&#12394;&#12514;&#12487;&#12523;&#12392;Expectile&#22238;&#24112; . 節の最後で述べたようにドパミンニューロンの活動はsign関数ではなく線形な応答をする、とした方が生理学的に妥当です (発火率を表現するならば$f( delta)=c+ delta quad(c &gt; 0)$とした方が良いのでしょうが)。それでは予測価値の更新式を $$ begin{cases} V_{i}(x) leftarrow V_{i}(x)+ alpha_{i}^{+} delta_{i} &amp; text{for } delta_{i} gt 0 V_{i}(x) leftarrow V_{i}(x)+ alpha_{i}^{-} delta_{i} &amp; text{for } delta_{i} leq 0 end{cases} $$ とした場合は、分位点回帰ではなく何に対応するのでしょうか。結論から言えば、この場合はエクスペクタイル回帰(Expectile regression)と同じになります。そもそも、expectileというのは聞きなれないですが、expectileという用語自体はexpectationとquantileを合わせたような概念、というところから来ています。中央値(median)に対する分位数(quantile)が、平均(mean)あるいは期待値(expectation)に対するexpectileの関係と同じであると捉えると良いです。 もう少し言えば、前者は誤差のL1ノルム, 後者はL2ノルムの損失関数を最小化することにより得られます (cf. Quantile and Expectile Regressions)。 . 分位点回帰で用いた損失関数は$$ rho_{ tau}( delta)= left| tau- mathbb{I}_{ delta leq 0} right| cdot | delta|$$でしたが、最後の$| delta|$を$ delta^2$として、 $$ rho^E_{ tau}( delta)= left| tau- mathbb{I}_{ delta leq 0} right| cdot delta^2$$ とします。これを微分すれば $$ frac{ partial rho^E_{ tau}( delta)}{ partial delta}= rho_{ tau}^{E prime}( delta)=2 cdot left| tau- mathbb{I}_{ delta leq 0} right| cdot delta $$ となり、上記の予測価値の更新式がExpectile回帰の損失関数から導けることが分かります。 . &#22577;&#37228;&#20998;&#24067;&#12398;&#12487;&#12467;&#12540;&#12487;&#12451;&#12531;&#12464; (decoding) . それで、RPEの応答を線形とした場合は報酬分布を上手く学習できるのかという話ですが、実はRPEの応答をsign関数とした場合と同じように学習後の予測価値の分布を求めても報酬分布は復元されません (簡単な修正で確認できます)。そこで報酬分布をデコーディングする方法を考えます。 . デコーデイングには各細胞が学習した予測価値(またはreversal points) $V_i$, asymmetries $ tau_i$, および報酬分布(ただし報酬の下限と上限からの一様分布)からのサンプル $z_m (m=1,2, cdots, M)$を用います。$N$を推定する$V_i$の数、$M=100$を1つの報酬サンプル集合$ {z_m }$内の要素数としたとき、次の損失関数を最小にする集合$ {z_m }$を求めます。 $$ mathcal{L}(z, V, tau)= frac{1}{M} sum_{m-1}^{M} sum_{n=1}^{N} left| tau_{n}- mathbb{I}_{z_{m} leq V_{n}} right| left(z_{m}-V_{n} right)^{2} $$ ここで、集合$ {z_m }$は20000回サンプリングするとします。損失関数$ mathcal{L}$を最小化する集合の分布が推定された報酬分布となっているので、それをplotします。以下はその結果とコードです (このコードはほとんど著者実装のままです)。灰色が元の報酬分布で、紫がデコーデイングされた分布です。完全とはいきませんが、ある程度は推定できていることが分かります。 . #collapse-hide import scipy.stats import scipy.optimize def expectile_loss_fn(expectiles, taus, samples): &quot;&quot;&quot;Expectile loss function, corresponds to distributional TD model &quot;&quot;&quot; # distributional TD model: delta_t = (r + gamma V*) - V_i # expectile loss: delta = sample - expectile delta = (samples[None, :] - expectiles[:, None]) # distributional TD model: alpha^+ delta if delta &gt; 0, alpha^- delta otherwise # expectile loss: |taus - I_{delta &lt;= 0}| * delta^2 # Note: When used to decode we take the gradient of this loss, # and then evaluate the mean-squared gradient. That is because *samples* must # trade-off errors with all expectiles to zero out the gradient of the # expectile loss. indic = np.array(delta &lt;= 0., dtype=np.float32) grad = -0.5 * np.abs(taus[:, None] - indic) * delta return np.mean(np.square(np.mean(grad, axis=-1))) def run_decoding(reversal_points, taus, minv=0., maxv=1., method=None, max_samples=1000, max_epochs=10, M=100): &quot;&quot;&quot;Run decoding given reversal points and asymmetries (taus).&quot;&quot;&quot; # sort ind = list(np.argsort(reversal_points)) points = reversal_points[ind] tau = taus[ind] # Robustified optimization to infer distribution # Generate max_epochs sets of samples, # each starting the optimization at the best of max_samples initial points. sampled_dist = [] for _ in range(max_epochs): # Randomly search for good initial conditions # This significantly improves the minima found samples = np.random.uniform(minv, maxv, size=(max_samples, M)) fvalues = np.array([expectile_loss_fn(points, tau, x0) for x0 in samples]) # Perform loss minimizing on expectile loss (w.r.t samples) x0 = np.array(sorted(samples[fvalues.argmin()])) fn_to_minimize = lambda x: expectile_loss_fn(points, tau, x) result = scipy.optimize.minimize( fn_to_minimize, method=method, bounds=[(minv, maxv) for _ in x0], x0=x0)[&#39;x&#39;] sampled_dist.extend(result.tolist()) return sampled_dist, expectile_loss_fn(points, tau, np.array(sampled_dist)) # reward distribution juice_amounts = np.array([0.1, 0.3, 1.2, 2.5, 5, 10, 20]) juice_empirical_probs = np.array( [0.06612594, 0.09090909, 0.14847358, 0.15489467, 0.31159175, 0.1509519 , 0.07705306]) # samples of reward (1000, ) sampled_empirical_dist = np.random.choice( juice_amounts, p=juice_empirical_probs, size=1000) n_trials = 10 # num of simulation trial n_epochs = 20000 # num of simulation epoch num_cells = 151 # num of cells or units n_decodings = 5 # num of decodings # Global scale for learning rates beta = 0.2 # Distributional TD simulation and decoding distribution = np.zeros((n_trials, num_cells)) alpha_pos = np.random.random((num_cells))*beta alpha_neg = np.random.random((num_cells))*beta # alpha_neg = beta - alpha_pos としてもよい # Simulation for trial in tqdm(range(n_trials)): for step in range(n_epochs): # Sample reward reward = np.random.choice(juice_amounts, p=juice_empirical_probs) # Compute TD error delta = reward - distribution[trial] # Update distributional value estimate valence = np.array(delta &lt;= 0., dtype=np.float32) alpha = valence * alpha_neg + (1. - valence) * alpha_pos distribution[trial] += alpha * delta # Decoding from distributional TD (DTD) simulation dtd_samples = [] # dtd_losses = [] # decoding loss taus = alpha_pos / (alpha_pos + alpha_neg) asym_variance = 0.2 for t in tqdm(range(n_decodings)): # Add noise to the scaling, but have mean 0.5 giving symmetric updates scaling_noise = np.tanh(np.random.normal(size=len(taus))) * asym_variance noisy_tau = np.clip(taus + scaling_noise, 0., 1.) # add noise # Run decoding for distributional TD values = run_decoding( distribution.mean(0), noisy_tau, minv=juice_amounts.min(), maxv=juice_amounts.max(), max_epochs=1, M=100, max_samples=20000, method=&#39;TNC&#39;) dtd_samples.append(values[0]) dtd_losses.append(values[1]) # print(t, values[1]) # results of decoding dtd_reward_decode = np.array(dtd_samples).flatten() # plot fig = plt.figure(figsize=(8, 5)) # Ground truth sns.kdeplot(sampled_empirical_dist, bw=.75, color=&#39;k&#39;, lw=0., shade=True) sns.rugplot(sampled_empirical_dist, color=&quot;red&quot;, lw=2, zorder=10, label=&quot;Empirical&quot;) # decoded distribution sns.kdeplot(dtd_reward_decode, bw=.75, color=plt.cm.plasma(0), lw=4., zorder=5, shade=False) sns.rugplot(dtd_reward_decode, color=plt.cm.plasma(0), label=&#39;Decoded&#39;) for draw in dtd_samples: sns.kdeplot(draw, bw=.5, color=plt.cm.plasma(0.), alpha=.5, lw=1., shade=False) plt.tick_params(top=False, right=False, labelsize=14) plt.legend(loc=&#39;best&#39;, fontsize=16) plt.xlabel(&quot;Reward&quot;, fontsize=16) plt.ylabel(&quot;Density&quot;, fontsize=16) plt.title(&quot;Distributional TD Decoding&quot;, fontsize=18) plt.tight_layout() plt.show() . . 100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:05&lt;00:00, 1.81it/s] 100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:33&lt;00:00, 6.69s/it] . このようにしてRPEに対する応答が線形であるとした場合でも報酬分布を推定できました。同じことを著者らはドパミンニューロンの活動に対しても行い、報酬分布がデコーデイングされることを示しています。ただ、デコーデイングの手間が結構かかっている気がするので、学習した予測価値分布を利用するときにはどのような処理をしているのかは気になります。 . &#21442;&#32771;&#25991;&#29486; . Dabney, W., Kurth-Nelson, Z., Uchida, N. et al. A distributional code for value in dopamine-based reinforcement learning. Nature (2020). https://doi.org/10.1038/s41586-019-1924-6 | Watabe-Uchida, M. et al. Whole-Brain Mapping of Direct Inputs to Midbrain Dopamine Neurons. Neuron 74, 5, 858 - 873 (2012). https://doi.org/10.1016/j.neuron.2012.03.01700281-4) 00281-4) | Eshel, N., Tian, J., Bukwich, M. et al. Dopamine neurons share common response function for reward prediction error. Nat Neurosci 19, 479–486 (2016). https://doi.org/10.1038/nn.4239 | Schultz, W., Dayan, P., Montague, P.R. A neural substrate of prediction and reward. Science. 275, 1593-9 (1997). doi:10.1126/science.275.5306.1593 | Chang, C., Esber, G., Marrero-Garcia, Y. et al. Brief optogenetic inhibition of dopamine neurons mimics endogenous negative reward prediction errors. Nat Neurosci 19, 111–116 (2016) doi:10.1038/nn.4191 | Bayer, H.M., Lau, B., Glimcher, P.W. Statistics of midbrain dopamine neuron spike trains in the awake primate. J Neurophysiol. 98(3):1428-39 (2007). https://doi.org/10.1152/jn.01140.2006 | Eshel, N., Bukwich, M., Rao, V. et al. Arithmetic and local circuitry underlying dopamine prediction errors. Nature 525, 243–246 (2015). https://doi.org/10.1038/nature14855 | .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/neuroscience/2020/01/20/drl.html",
            "relUrl": "/neuroscience/2020/01/20/drl.html",
            "date": " • Jan 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "NibabelとMayaviによるMR画像の可視化",
            "content": "PythonでNifti (The Neuroimaging Informatics Technology Initiative) 形式のMR画像の可視化をしてみる。まずNifti (nii.gz)形式のファイルはNibabelでloadする。Nibabelはpipで入る：pip install nibabel . なお、Nibabelに依存した機械学習ライブラリとしてNilearnというのもある。今回は使わない。 . Nibabel tutorial . NibabelのtutorialにはEPI法(echo planar imaging)で撮影された拡散強調像 (someones_epi.nii.gz) と、(恐らく)T1 強調像 (someones_anatomy.nii.gz) の2つのデータが用意されている。どちらでもよいが、someones_anatomy.nii.gzの方をダウンロードしておく。 . nibabelの関数(nib.load)でデータを読み込み、matplotlibで可視化する。 . import nibabel as nib import numpy as np import matplotlib.pyplot as plt from matplotlib import gridspec # Data load img = nib.load(&#39;someones_anatomy.nii.gz&#39;) img_data = img.get_fdata() print(img_data.shape) # (57, 67, 56) # Plot gs = gridspec.GridSpec(1, 3, width_ratios=[1.17, 1, 1.17]) plt.figure(figsize=(8,4)) plt.subplot(gs[0]) plt.title(&quot;Sagittal&quot;) plt.imshow(np.flipud(img_data[28].T), cmap=&quot;gray&quot;) plt.subplot(gs[1]) plt.title(&quot;Coronal&quot;) plt.imshow(np.flipud(img_data[:, 33].T), cmap=&quot;gray&quot;) plt.subplot(gs[2]) plt.title(&quot;Horizontal&quot;) plt.imshow(img_data[:, :, 28], cmap=&quot;gray&quot;) plt.tight_layout() plt.show() . . マーモセット脳MRIデータセット . マーモセット(marmoset)脳のMRIデータセット (Marmoset Brain Mapping)が公開された[1, 2]ので、それを読み込んで可視化してみる。 . まず、dataのページから150um dMRIの前処理後のデータ(DTI-Fitted)をダウンロードする。これを選んでいるのは単にデータサイズが小さいため。このデータはマーモセットから脳を取り出した後、ホルマリン固定後、gadoliniumに浸して造影し、7T MRIでスキャンすることで得られたものである。 . zipファイルを解凍後、DTIFIT_FA.nii.gzというデータ (添え字の詳細が無かった)を読み込んで可視化する。 . Nibabelによるマーモセット脳MRIの断面図の可視化 . コード自体は先ほどのtutorialのものとほぼ同じ。 . import nibabel as nib import numpy as np import matplotlib.pyplot as plt from matplotlib import gridspec # Data load img = nib.load(&#39;DTIFIT_FA.nii.gz&#39;) img_data = img.get_fdata() print(img_data.shape) # (192, 256, 192) # Plot gs = gridspec.GridSpec(1, 3, width_ratios=[1.33, 1, 1.33]) plt.figure(figsize=(10,4)) plt.subplot(gs[0]) plt.title(&quot;Sagittal&quot;) plt.imshow(np.flipud(img_data[96].T), cmap=&quot;gray_r&quot;) plt.subplot(gs[1]) plt.title(&quot;Coronal&quot;) plt.imshow(np.flipud(img_data[:, 128].T), cmap=&quot;gray_r&quot;) plt.subplot(gs[2]) plt.title(&quot;Horizontal&quot;) plt.imshow(img_data[:, :, 96], cmap=&quot;gray_r&quot;) plt.tight_layout() plt.show() . . Mayaviによるマーモセット脳の3Dでの可視化 . Pythonで3次元Plotをするのにはmatplotlibやplotlyなどもあるが、今回はMayaviを用いる。installはpip install mayaviでできるが、 . $ git clone https://github.com/enthought/mayavi.git $ cd mayavi $ pip install -r requirements.txt $ pip install PyQt5 # replace this with any supported toolkit $ python setup.py install # or develop . をすると確実。 . import nibabel as nib from mayavi import mlab img = nib.load(&#39;DTIFIT_FA.nii.gz&#39;) img_data = img.get_fdata() fig = mlab.figure(size=(400, 400), bgcolor = (1,1,1)) src = mlab.pipeline.scalar_field(img_data) mlab.pipeline.iso_surface(src, contours=[img_data.min()+0.05*img_data.ptp(), ], opacity=1, color=(.5, .5, .5)) mlab.show() . . 実際にはグリグリと3Dで脳を動かしてみることができる。なお、脳溝がほぼ見えないが、マーモセットは元々脳溝が少ない。 . 参考文献 . Liu, C., Ye, F.Q., Newman, J.D. et al. A resource for the detailed 3D mapping of white matter pathways in the marmoset brain. Nat Neurosci. (2020) doi:10.1038/s41593-019-0575-0 | Liu, C., Ye, F. Q., Yen, C. C., Newman, J. D. et al. A digital 3D atlas of the marmoset brain based on multi-modal MRI. NeuroImage. (2018). doi:10.1016/j.neuroimage.2017.12.004 | .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/neuroscience/2020/01/15/nibabel.html",
            "relUrl": "/neuroscience/2020/01/15/nibabel.html",
            "date": " • Jan 15, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "CIELUV色相環をPythonで描画する",
            "content": "CIE Luv色空間 (CIE 1976 Luv color space)の色相環(hue wheel)をOpenCVとMatplotlibで描画する。 . ぶっちゃけると色覚に関する研究もしていて、JNNS2019でポスター発表はしたが頓挫したりあっちこっちに行ったりしている。今回はその付随したメモである。色空間について、自分は今までHSV色空間を使うなどかなり適当だったが、先行研究がほとんどCIELUV色空間を使っていたので調べることとした。単なるHSVでは色の数値的差が知覚的差と一致しないが、CIELuvでは両者ができるだけ近くなるようにしている。 . 描画にはcolourまたはOpenCVを使う。Pythonのpackageであるcolourはpipで入る：pip install colour-science . なお、変換をscratchでする場合にはhttp://www.easyrgb.com/en/math.phpが参考になる。 . OpenCV&#12434;&#20351;&#12358;&#22580;&#21512; . CIE Luv色空間において値域はL [0, 100], u[-100, 100], v[-100, 100]である。Hue angle の配列を作り、cos, sinに入れて100をかけ、それぞれをu, vとする。そしてLを適当に定めることでLuvの配列ができる。 . 後の面倒な変換はcv2.cvtColor(hogehoge, cv2.COLOR_Luv2RGB)に任せる(cf. 変換できる色空間の一覧)。単なる変換ではRGBのどれかの値が負の値を取るが、それを0にclippingしている。逆にcvtColorを使うと正規化されずにclippingされるので、L=100のときはほぼ白色となる。 . 変換式は下記参照（Miscellaneous Image Transformations — OpenCV 2.4.13.7 documentationより引用）。 . . &#33394;&#30456;&#12496;&#12540; . import matplotlib.pyplot as plt import numpy as np import cv2 N_theta = 1000 luv = np.zeros((1, N_theta, 3)).astype(np.float32) theta = np.linspace(0, 2*np.pi, N_theta) luv[:, :, 0] = 80 # L luv[:, :, 1] = np.cos(theta)*100 # u luv[:, :, 2] = np.sin(theta)*100 # v rgb = cv2.cvtColor(luv, cv2.COLOR_Luv2RGB) plt.imshow(rgb, vmin=0, vmax=1, aspect=100) plt.show() . &#33394;&#30456;&#29872; . N_theta = 1000 luv = np.zeros((1, N_theta, 3)).astype(np.float32) theta = np.linspace(0, 2*np.pi, N_theta) luv[:, :, 0] = 80 # L luv[:, :, 1] = np.cos(theta)*100 # u luv[:, :, 2] = np.sin(theta)*100 # v rgb = cv2.cvtColor(luv, cv2.COLOR_Luv2RGB) # hue wheel plot ax = plt.subplot(111, polar=True) #get coordinates: theta = np.linspace(0, 2*np.pi, rgb.shape[1]+1) r = np.linspace(0.5, 1, rgb.shape[0]+1) Theta,R = np.meshgrid(theta, r) # get color color = rgb.reshape((rgb.shape[0]*rgb.shape[1], rgb.shape[2])) m = plt.pcolormesh(theta,R, rgb[:,:,0], color=color, linewidth=0) # This is necessary to let the `color` argument determine the color m.set_array(None) plt.show() . uv&#24179;&#38754; . uv平面をplotすると次のようになる。 . W = 1000 H = 1000 luv = np.zeros((W, H, 3)).astype(np.float32) u = np.linspace(-100, 100, W) # u v = np.linspace(-100, 100, H) # v U, V = np.meshgrid(u, v) luv[:, :, 0] = 80 luv[:, :, 1] = U luv[:, :, 2] = V RGB = cv2.cvtColor(luv, cv2.COLOR_Luv2RGB) plt.figure(figsize=(5,5)) plt.imshow(RGB, vmin=0, vmax=1) plt.show() . Colour&#12434;&#20351;&#12358;&#22580;&#21512; . colourは色の変換関数が充実している。そこでu&#39;, v&#39;平面上の円周上の色を、xyに変換、さらにXYZに変換し(このときY=1とする)、最後にRGBに変換したものをplotする。 . &#33394;&#30456;&#12496;&#12540; . HSVと比べると青がかなり短いことが分かる。 . import colour N_theta = 500 theta = np.linspace(0, 2*np.pi, N_theta) r = 0.2 u = np.cos(theta)*r + 0.2009 v = np.sin(theta)*r + 0.4610 uv = np.dstack((u, v)) # map -&gt; xy -&gt; XYZ -&gt; sRGB xy = colour.Luv_uv_to_xy(uv) xyz = colour.xy_to_XYZ(xy) rgb = colour.XYZ_to_sRGB(xyz) rgb = colour.utilities.normalise_maximum(rgb, axis=-1) plt.figure(figsize=(5,3)) plt.imshow(np.reshape(rgb, (1, N_theta, 3)), aspect=100) plt.show() . N_theta = 500 theta = np.linspace(0, 2*np.pi, N_theta) r = 0.2 u = np.cos(theta)*r + 0.2009 v = np.sin(theta)*r + 0.4610 uv = np.dstack((u, v)) # map -&gt; xy -&gt; XYZ -&gt; sRGB xy = colour.Luv_uv_to_xy(uv) xyz = colour.xy_to_XYZ(xy) rgb = colour.XYZ_to_sRGB(xyz) rgb = colour.utilities.normalise_maximum(rgb, axis=-1) # hue wheel plot ax = plt.subplot(111, polar=True) #get coordinates: theta = np.linspace(0, 2*np.pi, rgb.shape[1]+1) r = np.linspace(0.5, 1, rgb.shape[0]+1) Theta,R = np.meshgrid(theta, r) # get color color = rgb.reshape((rgb.shape[0]*rgb.shape[1], rgb.shape[2])) m = plt.pcolormesh(theta,R, rgb[:,:,0], color=color, linewidth=0) # This is necessary to let the `color` argument determine the color m.set_array(None) plt.show() . CIE 1976 UCS (uniform chromaticity scale) diagram . 描画方法を2つ示す。 . Method 1 . 黒線は光のスペクトルに対応する点を意味する。 . import matplotlib.pyplot as plt import numpy as np import colour samples = 258 xlim = (0, 1) ylim = (0, 1) wvl = np.arange(420, 700, 5) wvl_XYZ = colour.wavelength_to_XYZ(wvl) wvl_uv = colour.Luv_to_uv(colour.XYZ_to_Luv(wvl_XYZ)) wvl_pts = wvl_uv * samples u = np.linspace(xlim[0], xlim[1], samples) v = np.linspace(ylim[0], ylim[1], samples) uu, vv = np.meshgrid(u, v) # stack u and v for vectorized computations uuvv = np.stack((vv,uu), axis=2) # map -&gt; xy -&gt; XYZ -&gt; sRGB xy = colour.Luv_uv_to_xy(uuvv) xyz = colour.xy_to_XYZ(xy) dat = colour.XYZ_to_sRGB(xyz) dat = colour.normalise_maximum(dat, axis=-1) # now make an alpha/transparency mask to hide the background # and flip u,v axes because of column-major symantics alpha = np.ones((samples, samples)) # * wvl_mask dat = np.swapaxes(np.dstack((dat, alpha)), 0, 1) # lastly, duplicate the lowest wavelength so that the boundary line is closed wvl_uv = np.vstack((wvl_uv, wvl_uv[0,:])) fig, ax = plt.subplots(figsize=(5,5)) ax.imshow(dat, extent=[xlim[0], xlim[1], ylim[0], ylim[1]], interpolation=&#39;None&#39;, origin=&#39;lower&#39;) ax.set(xlim=(0, 0.7), xlabel=&#39;CIE u &#39;&#39;, ylim=(0, 0.7), ylabel=&#39;CIE v &#39;&#39;) ax.plot(wvl_uv[:,0], wvl_uv[:,1], c=&#39;0&#39;, lw=3) plt.show() . Method 2 . colour.plottingを用いる方法。 . import colour.plotting as cpl cpl.plot_chromaticity_diagram_CIE1976UCS(standalone=False) cpl.render( standalone=True, bounding_box=(-0.1, 0.7, -0.05, 0.7), x_tighten=True, y_tighten=True, filename=&quot;CIE1976UCS_diagram.png&quot;) plt.show() .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/python/2020/01/13/cieluv.html",
            "relUrl": "/python/2020/01/13/cieluv.html",
            "date": " • Jan 13, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "enumerateのラベルを毎週の日付にする",
            "content": "概要 . 久しぶりのLaTeXネタ．カッコつけてカウンタ類をTeXで作ったら，引き継がれなかったりして面倒だった．やっぱりLaTeXだね． . 表題の通りです．LaTeXのenumerateのラベルを１週間づつの日付にします．ソースは以下の２つのstyファイル： . enumitem_extended.sty：enumerate環境のラベルを増やすやつ | Wdate.sty：日付処理のstyファイル | . を適当なフォルダに入れれば動きます． . 使い方 . documentclass{jsarticle} usepackage{enumitem_extended} begin{document} intercalarytrue % 2020年は閏年！ % intercalaryfalse % 2021年は閏年じゃ無い！ setcounter{Wmonth}{1} setcounter{Wday}{8} begin{enumerate}[label= Wdate*] item 進捗を生む item 進捗を生む item 進捗を生む addWdate item 進捗を生む item 進捗を生む item 進捗を生む item 進捗を生む addWdate item 進捗を生む item 進捗を生む item 進捗を生む item 進捗を生む end{enumerate} setcounter{Wmonth}{6} setcounter{Wday}{12} begin{enumerate}[label= Wdate*] item 進捗を生む item 進捗を生む end{enumerate} end{document} . カウンターで開始日付を設定します．再設定しない場合は，前のenumerate環境の日付が引き継がれます． addWdate で，週を進める（つまり，２週分進む）ことができます． . もし3日づつとかのラベルを作りたければ，Wdate.sty の中の addtocounter の第２引数を変更すればオッケー． .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/latex/2020/01/03/enumerate_labels.html",
            "relUrl": "/latex/2020/01/03/enumerate_labels.html",
            "date": " • Jan 3, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "人工神経回路による脳の理解はどこまで進んだか",
            "content": "神経科学 Advent Calendar 2019の2記事目です。人工神経回路 (Artificial neural network, ANN) を用いた研究により、脳の理解はどこまで進んだか、次に何が調べられるべきなのかということについて解説します。 . 昨年の年末からhttps://github.com/takyamamoto/BNN-ANN-papersにANNと脳に関する論文リストを作成しており (これは先に研究が出てしまう悲劇が頻発したための措置ですが)、このリストがそのまま参考文献となっています。 . 本記事は特に(B.A. Richards, T.P. Lillicrap, et al. Nat. Neurosci. 2019)での議論を参考にしています(翻訳ではないです)。 この論文はANNと脳についての研究を先導してきた多くの研究者が共著者となっています（一体どうやって纏めたのやら）。論文にアクセス権の無い場合は、Kording先生がtwitterにpdfの短縮リンクを貼ってくださっているのでそこから読むことができます。 . 脳とANNを3つの観点で捉える . (B.A. Richards, T.P. Lillicrap, et al. Nat. Neurosci. 2019)で言っていることは多くありますが、ざっくり言えば、『脳もANNと同じように目的関数(objective functions), 学習則(learning rules), 構造(architectures)の3つの観点で捉えよう』という主張です (T.P. Lillicrap, K.P. Kording. arXiv. 2019でも、この前段階となる主張をしています)。ちなみにこれはDeep learningに限った話ではなく論文のタイトルを機械学習と置き換えても良いのですが、Reviewerが”Deep learning”を入れるように言ったため、タイトルに入ったそうです(ソースはtwitterですがリンクを忘却しました)。 . かなり思い切った主張ですが、この考えが生まれるまでの動向を(不正確かもしれませんが)、本記事で解説します。 . 脳はある目的(関数)に対して最適化されている . 近年の研究により(古いものは1980年代のものも含みますが)、ある目的関数に対して最適化されたANNが脳と同様の、あるいは類似した神経活動を得ることがあることが分かってきました（ただし全てを説明できるわけではありません）。これは神経活動を生み出すための機構を考えるという従来の研究と異なり、目的関数さえ仮定してしまえば（もちろん仮定するのはこれだけではないですが）自然に出現する、という簡素さと美しさがあります。 . 全部説明するのは不可能なので、一部の論文を広く浅く紹介します。 . 視覚系 . 視覚系は元々の研究が多いのと、ANNが畳み込みニューラルネットワーク(CNN)で成功をしたために論文の数が最も多いです。 . まず、先駆的な研究を紹介します。(D. Zipser, R.A. Andersen. Nature. 1988)は網膜座標から頭部中心座標への変換をするように誤差逆伝搬法で最適化した3層NNの中間層に、7a野のニューロンの活動に類似したゲインフィールドが得られるという内容です。 . (A. Krizhevsky, I. Sutskever, G. Hinton. NIPS. 2012)はご存知の通り、AlexNetの論文です。CNNにImageNetデータセットの画像分類タスクを学習させると、CNNの畳み込みカーネルに一次視覚野(V1野)の単純細胞の受容野に似たパターンが現れた、というのが関連する結果です。なお、V1野の単純細胞の受容野がガボール関数で近似できることについては(JP. Jones &amp; LA. Palmer. Neurophysiol. 1987)を参照してください。 . (D. Yamins, et al. PNAS. 2014; S. Khaligh-Razavi &amp; N. Kriegeskorte. PLoS Comput. Biol. 2014)は物体認識において、V4野やIT野などの高次視覚野においてもANNと神経活動に相関があったという論文です。これらの論文が出てからANNを用いた脳の研究に関する論文の数が急速に増えたという感じがします。 . 聴覚系 . (U. Güçlü, et al., NIPS. 2016; A.J.E. Kell, et al., Neuron. 2018)は音楽やヒトの声を分類するように学習させたANNが、階層的な情報処理をしており、ヒトfMRIのボクセル応答も予測することを示しています。 . (T. Koumura, H. Terashima, S. Furukawa. J. Neurosci. 2019)では、自然音を分類するように学習させたANNは、聴覚経路全体と最適変調周波数や上限周波数の分布が類似することを示しています(書いてて合ってるか不安になったのでNTTのプレスリリースを読んでください)。 . 運動野 . 一報だけ紹介。(D. Sussillo, et al., Nat. Neurosci. 2015)はRNN(recurrent neural network)にサルの到達運動時の筋電活動(electromyographic; EMG)を出力するよう学習させると、RNNのユニットは運動野のニューロンのダイナミクスに類似した発火パターンを生み出したという論文です。出力が筋電活動なのに中間にニューロンの活動が現れたということや、正則化を付けないと現れなかったというのも面白い点です。 . 場所受容野 . (C. Cueva, X. Wei. ICLR. 2018; A. Banino, et al. Nature. 2018)は自己運動の速度を積分して自己位置を推定させるタスクをRNNに学習させると、中間層にグリッド細胞のような発火パターンが生まれたという論文です。 . どちらも主張はほぼ同一ですが、(C. Cueva, X. Wei. ICLR. 2018)ではグリッド細胞の発達過程が実際の嗅内皮質のニューロンと類似することを示しています。 . 一方で(A. Banino, et al. Nature. 2018)はCuevaらの研究より発表は遅かったのですが、Baninoらの研究ではグリッド細胞のパターンが壁面から少しずれていることまで再現されていました。グリッド細胞の発火の方向が壁面から少しずれていることは謎でしたが、この実験からこれが最適であるということが分かりました。つまり、壁面からずれていることで格子パターンが部屋全体を覆うことができ、一様な場所受容野の形成に役立っているといえます。また、このネットワークを持った強化学習エージェントが迷路課題において高い性能を出したことも注目されました。 . いずれの研究でも正則化(Cuevaらは発火抑制損失、BaninoらはDropout)を入れなければグリッド様の発火が生まれなかったことを述べています。 . その他 . ANNを用いた研究対象となる脳の機能は様々です。特にヒトに固有の高次機能については、fMRIなどでは脳の機能部位しか分からなかったため、ANNは打開策となるかもと期待されています。関連する総説としては(N. Kriegeskorte &amp; P. Douglas. Nat. Neurosci. 2018)などを参照してください。 . 脳における目的関数とは何か . 前節のような結果が多数報告されたため、(B.A. Richards, T.P. Lillicrap, et al. Nat. Neurosci. 2019)では「神経細胞の個々の活動の意味をボトムアップ的に考えるのではなく目的関数の最適化の結果として捉えよう」といったことが言われています。それでは脳における目的関数とは何でしょうか。 . ANNでは定義しなければならない目的関数ですが、脳内では明確に存在はしていません。そもそも目的関数という言い方ですが、これは生体内のパラメータなどが不明であっても数値的に表現はできると思うので関数であることに違いはないと思います。ただし、ANNの目的関数のように微分可能かということは保証されていません。 . 一度、脳から離れて生物および生体の目的関数を考えてみましょう。生物の目的は主に種の生存と個体の生存の2つがあるでしょう(この他にもあると思いますが)。前者は子孫の数、後者は寿命の長さが目的関数であると言えます。これらの目的関数を最大化するために生物は主に、4つのF (Four Fs)であるfeeding, fighting, fleeing, および fornicating (またはmating)を行います。 . 生体内で考えると、体温や血中酸素濃度や血糖値などなど恒常性(homeostasis)の維持が至る所で行われています。この場合の目的関数は定常状態からの変位と言えます(この場合は最小化します)。 . 脳に戻れば、例えば物体認識では物体に対する神経表現の距離を最大化している、要は距離学習(metric learning)していると言えそう…だったり言えそうじゃなかったりします。ANNでは教師ラベルを仮定してCross entropy誤差を目的関数と置くのが基本(非生物的)ですが、最近は教師なしの距離学習も盛んです。あまりサーベイできていませんが、最近だと(K. He, et al., arXiv, 2019)があります。少し違うのですが、1つの物を手に取って色々な角度から見れば、1つの物体としてラベル付けされた複数の画像が得られます。これを生かした距離学習もしているのでは、と自分の中では思っています。もう少しはっきりした例を挙げれば強化学習における報酬(reward)の最大化があるでしょう。これは結構明示的で、報酬という大域的目的関数のためのサブ目的関数が複数存在するように思います。また好奇心(curiosity)や驚き(surprise)による学習も存在します。これに関して、報酬、好奇心、驚きに基づいた学習を指す言葉として、それぞれReward-based learning, curiosity-driven learning, Surprise-based learningなどがあります。 . 解剖学・発生学的な観点で言えば回路の配線最適化問題も挙げられるでしょう。できるだけ配線長が短くなるように効率的なネットワークを構築することが目的となります。また、頭蓋骨という体積の制約の中で、可能な限り回路を大きくするという形態学的な面においても目的関数が存在するでしょう。 . 少し計算論の方に話を逸らせば、予測符号化(predictive coding, R.P. N. Rao &amp; D.H. Ballard. Nat. Neurosci. 1999)では予測誤差の最小化、これをBayesの枠組みで拡張した(知覚においてBayes的考えを予測符号化ネットワークに乗っけたり行動も付けたりした)自由エネルギー原理(free energy principle, K. Friston, et al., Journal of Physiology. 2006)では自由エネルギーの最小化(あるいは変分下限(variational lower bound)の最大化、記述長(description length)の最小化)などを仮定しています。他のANNに関与しない研究もこうした目的関数を定義していることは多いです。 . 脳は学習方法を学習できたりと様々なことができます。実際には目的関数が複数並列・分散しているのでしょう。では目的関数がどこに表現されていて、外部からどのように知ることができるか、という疑問が当然でます。前者は恐らくゲノム(適応的に目的関数を生み出している感じもありますが、大概は報酬の最大化だったり)、後者は経験的に知るしかないのでは、と思います(曖昧過ぎますが)。 . 脳では如何なる最適化手法が用いられているか . 脳における学習はHebb則やSTDP(spike-timing dependent plasticity)則などが挙げられますが、これらは完全ではありません。上で議論されていた最適化は誤差逆伝搬法(back-prop)によるものです。この節では貢献度分配問題(credit assignment problem)を中心として脳における最適化について考えます。 . 貢献度分配問題 (credit assignment problem) . 貢献度分配問題(credit assignment problem)はある出力を得るためにどのニューロンまたはシナプスに功罪(credit/blame)、要は貢献度を割り当てるかという問題です。ある出力について貢献度の高いニューロンの出力(発火率)は大きくなりますし、逆は小さくなります。 . 誤差逆伝搬法の近似 . 誤差逆伝搬法が生体内で用いられていないという批判は数多く、DNAの二重螺旋構造を発見したCrickも記事を書いています(F. Crick. Nature. 1989)。 . 色々と問題点はあるのですが、逆伝搬の結合重みが順伝搬の結合重みの転置行列になっているという問題(weight transport problem)は、例えば逆伝搬時の重みをランダム行列にすることでも上手くいく(T. Lillicrap, et al., Nat. Commun. 2016)、という研究により緩和されています。この手法をFeedback alignmentと言います。誤差逆伝搬法を緩和しようという試みは多数ありますが(他にはEquilibrium Propagation(B. Scellier &amp; Y. Bengio. Front. Comput. Neurosci. 2017)など)、とりあえず総説(J. Whittington &amp; R. Bogacz. Trends. Cogn. Sci. 2019; T.P. Lillicrap &amp; A.Santoro. Curr. Opin. Neurobiol. 2019)を参照してください。 . 誤差逆伝搬法は現状ANNを訓練するのに最適ですが、それは層を縦断して貢献度分配問題を解決できるためです。以前は誤差逆伝搬法は全く生理学的ではないと切り捨てられてきましたが、緩和すれば全く不可能ではないかもしれません。また誤差逆伝搬法は無理でも、勾配法（に類する最適化）は使ってるんじゃないか、という意見もありますが決着はついていません(参考：Kording先生のtweetに対するリプライ)。また、誤差逆伝搬法に関係なく貢献度分配問題を解決する研究も出始めています(J. Aljadeff, et al., arXiv. 2019)。 . よく生物学的妥当性(biological plausibility)や生理学的妥当性(physiological plausibility)という語を用いますが、ここでの生物学や生理学には「人類が既知の」という枕詞が付きます。生体内では発見されるまで人類が予想していなかった現象も多いので、否定はしきれないでしょう（かといって自分が完全肯定しているということでもないですが）。 . 進化による獲得 . ANNが脳と違う点として学習データを大量に必要とする、という指摘があります。確かに脳は少ないサンプルで物事を見分けるFew shot learningが可能です。しかし、脳の最適化は生まれてから起こっているのではなく、進化の過程においても最適化が起こっています。 . (A. Zador. Nat. Commun. 2019)ではヒトを含めた動物の行動は未知の優れた教師あり/教師なし学習によるのではなく、大部分がゲノムに符号化された神経回路によると主張しています。同時にゲノムにはあらゆることが記述できるわけではない(genomic bottleneck)ので、優れた汎用的な回路(いわゆる canonical microcircuit)が生まれたとしています。 . 物体認識で考えれば、幼児であっても顔に対してはよく反応します。また、飼育員が顔を隠して育てたサルであってもヒトとサルの顔の識別はできます(Y. Sugita, PNAS. 2008)。ただしこれについては経験がやっぱり大事だという主張もあります(M. J. Arcaro, et al., Nat. Neurosci. 2017)。関連して、謎ですが、ANNにおいて顔認識ニューロンが未訓練でも自発的に生まれるという報告もあります (S. Baek, et al., bioRxiv. 2019)。 . 色々と議論はありますが、最適化において進化が占める割合はとても大きいです。しかし、ヒトに戻って考えてみると環境は常に変化しているにも関わらず何とか適応できています。このことを考えれば貢献度分配問題は進化による最適化だけでは説明できるとは言えなさそうです。 . in silicoからin vivoへ . 結局、ANNを用いたin silicoの研究で分かったことは『脳は目的関数を最適化している』ということです(個々の論文を読めばそれだけではないですが)。この結果をin vivoの研究に持っていくわけですが、解決すべき問題は『脳における貢献度分配問題を解決するような学習則は何か』ということです。 . かなり難しいと思いますが、研究が進めばいずれ解決できる問題だと自分は思っています。 .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/neuroscience/2019/12/06/understanding_brain_with_ann.html",
            "relUrl": "/neuroscience/2019/12/06/understanding_brain_with_ann.html",
            "date": " • Dec 6, 2019"
        }
        
    
  
    
        ,"post11": {
            "title": "Windows10が起動しない",
            "content": "症状 . Windows10をシャットダウンしてから，次に起動しようとすると，起動しない．具体的には . 電源自体は入る（ファンは回っている） | ディスプレイに信号が送られない | キーボードのNum Lockランプが点灯しない | . ただし， . 強制終了してからもう一度起動すれば正常に起動 | 再起動の場合は正常に起動 | . 解決策 . Windowsがシャットダウンする時に，低電力状態を選択するモードになっているのが原因．ということで，「コントロールパネル」から「電源オプション」→「電源ボタンの動作を選択する」，さらに表示された画面内の「現在利用可能ではない設定を変更します」をクリック．「高速スタートアップを有効にする（推奨）」のチェックを外す． .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/windows/2019/10/10/windows_boot_error.html",
            "relUrl": "/windows/2019/10/10/windows_boot_error.html",
            "date": " • Oct 10, 2019"
        }
        
    
  
    
        ,"post12": {
            "title": "PythonによるPrincipal Curveの実装 (bendingアルゴリズム)",
            "content": "この記事はPythonでPrincipal Curves (Hastie &amp; Stuetzle, 1989)を実装するという内容です。数学的な詳細は優曇華院による記事「Principal Curve 入門」を読んでください。 . Principal Curves&#12392;&#12399; . Principal curvesは簡単に言えば主成分分析 (principal component analysis, PCA) の非線形版です。非線形次元圧縮としては他にKernel PCAやLLE (Locally Linear Embedding), ISOMAP (Isometric Feature Map), LEM (Laplacian Eigenmap), tSNE (t-distributed Stochastic Neighbor Embedding), UMAP (Uniform Manifold Approximation and Projection) などがあります。 . Principal curves (高次元であれば Principal surface) の特徴としては . PCAの曲線 (曲面) への一般化 (データ点との距離を最小化する曲線 (曲面)) | グローバルな情報も保持できる | などがあります。 . 具体例を示しておきます。2次元上の点$(x_0, x_1) in mathbb{R}^2$が次のようにサンプリングされたとします。 . $$ begin{aligned} x_0 &amp; sim U(-1, 1) x_1 &amp; = x_0^5 + xi ( xi sim mathcal{N}(0, 0.15^2)) end{aligned} $$ただし、$U$は一様分布、$ mathcal{N}$は正規分布です。このような点を100個描画したのが、下図(左)です。 下図(中央)はPCAをしたときの結果であり、青線は第一主成分軸を表します。点群との間の線は点から第一主成分軸に下ろした垂線です。 下図(右)は同じ点群に対するPrincipal curveです。PCAよりもデータセットの特徴を適切に捉えることができています。 . . bending&#12450;&#12523;&#12468;&#12522;&#12474;&#12512; . Rのpackageはprincurveがあるのですが、Pythonにはありません。そこで、今回は (Hastie &amp; Stuetzle, 1989) によるPrincipal curvesの基本となるbendingアルゴリズムを実装します。 . $p$次元の$n$個のデータ$ boldsymbol{x}_i in mathbb{R}^{p} (i=1, 2, cdots, n)$があるとし、点$ boldsymbol{x}_i$の集合を$ boldsymbol{X}$とします。さらに$ boldsymbol{f}( lambda)=[f_1( lambda), cdots, f_p( lambda)]$を$ lambda in mathbb{R}$を媒介変数とする$ mathbb{R}^p$上の曲線とします。この$ boldsymbol{f}$がprincipal curveとなります。また、媒介変数$ lambda$は一方の端点を基準点としたときの曲線上の位置 (基準点からの距離)を表します。さらに $ lambda_{ boldsymbol{f}}( boldsymbol{x}_i) (i=1,2, cdots, n)$ を$ boldsymbol{x}_i$と$ boldsymbol{f}( lambda)$の距離を最小化する$ lambda$の中で最大の$ lambda$とします。数式で表すと . $$ lambda_{ boldsymbol{f}}( boldsymbol{x})= sup_ lambda left { lambda : | boldsymbol{x}- boldsymbol{f}( lambda) |= inf_ mu | boldsymbol{x}- boldsymbol{f}( mu) | right } $$となります。ただし、$ | cdot |$はユークリッドノルムを意味します。この$ lambda_f( boldsymbol{x})$をprojection indexと呼びます。 . さて、このprincipal curveを求めるためのbendingアルゴリズムは次のように表記されます(Hastie and Stuetzle, 1989; Kegl, et al., 2000)。 . [Step 0] $ boldsymbol{f}^{(0)}( lambda)$を$ boldsymbol{X}$の第一主成分線とし、$j=1$とする。 | [Step 1] $ boldsymbol{f}^{(j)}( lambda)=E( boldsymbol{X} mid lambda_{ boldsymbol{f}^{(j-1)}}= lambda)$とする。 | [Step 2] $ lambda^{(j)}( boldsymbol{X})= lambda_{ boldsymbol{f}^{(j)}}( boldsymbol{X})$とする。 | [Step 3] $D^2( boldsymbol{X}, boldsymbol{f}^{(j)})= E( | boldsymbol{X}- boldsymbol{f}^{(j)}( lambda^{(j)}( boldsymbol{X}))$を計算する。$|D^2( boldsymbol{X}, boldsymbol{f}^{(j)})-D^2( boldsymbol{X}, boldsymbol{f}^{(j-1)})|$がある閾値以下となれば (つまり収束した場合)、 $ boldsymbol{f}= boldsymbol{f}^{(j)}$として終了する。そうでない場合、$j leftarrow j+1$としてStep 1に戻る。 | . ここで、Step1はExpectation Stepと呼ばれ、$ lambda_{ boldsymbol{f}^{(j-1)}}= lambda$という条件の下でのデータ点$ boldsymbol{x}_i$の平均(条件付き期待値)を求めます。 ただし、これはデータの分布が既知、あるいは無限個のサンプルを仮定する場合であるので、データ$ boldsymbol{X}$の分布が未知の場合にはLocally weighted regressionや3次スプライン曲線(cubic smoothing splines)で置き換えます。 またStep2は Projection Stepと呼ばれ、データ点$ boldsymbol{x}_i$から最も近い曲線上の点の$ lambda$を求め、その$ lambda$を$ lambda^{(j)}( boldsymbol{x}_i)$とします。 . bending&#12450;&#12523;&#12468;&#12522;&#12474;&#12512;&#12398;&#23455;&#35013; . &#21021;&#26399;&#21270; . まず、toyデータセットを生成し、PCAを実行します。このときの第一主成分軸がPrincipal curveの初期関数です。なお、点群から第一主成分軸へ下ろした垂線の描画は「Matplotlibで複数の線分を描画する方法」を参考にしてください。 . import numpy as np import matplotlib.pyplot as plt from matplotlib import collections as mc from scipy.interpolate import UnivariateSpline . # Generate toy data np.random.seed(0) n_samples = 100 x0 = np.random.uniform(-1, 1, size=n_samples) x1 = x0**5 + 0.15*np.random.randn(n_samples) X = np.vstack((x0, x1)).T . # SVD u, s, v = np.linalg.svd(X) # 主成分上へのXの射影 expand_v = np.expand_dims(v[0], axis=1) Z = X @ expand_v @ expand_v.T # 元データから射影への直線のリスト lines = [[(X[i, 0], X[i, 1]), (Z[i, 0], Z[i, 1])] for i in range(n_samples)] lc = mc.LineCollection(lines, colors=&quot;k&quot;, linewidths=1) . # 主成分軸の描画用変数 xmax = ymax = 1.3 xmin = ymin = -1.3 zs = np.arange(Z[:, 0].min(), Z[:, 0].max(), 1e-2) # 描画 fig = plt.figure(figsize=(6, 6)) ax = fig.add_subplot(aspect=&#39;1&#39;) #こうしないと線が傾いて見える ax.scatter(X[:, 0], X[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) # 元データのplot ax.scatter(Z[:, 0], Z[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) # 射影のplot ax.add_collection(lc) ax.plot(zs, v[0, 1] / v[0, 0] * zs) # 主成分軸 plt.title(&quot;Initialization&quot;) plt.xlim(xmin, xmax); plt.ylim(ymin, ymax) plt.xlabel(&#39;x0&#39;); plt.ylabel(&#39;x1&#39;) plt.show() . ここでデータ点と曲線(今は第一主成分線)との距離を計算し、さらに$ lambda_{ boldsymbol{f}^{(j)}}( boldsymbol{X})$の初期値lamも計算します(これは主成分線の 一方の端点からの距離を意味します)。また、各種定数も定義します。 . D2 = np.sum((X-Z)**2) print(&quot;D^2:&quot;, D2) lam = u[:,0]*s[0] # lambdaの初期値 lam_plus = 0.01 # iterごとに伸長するlamの長さ n_seq_lam = int(1e4) # 離散化した曲線の点の数 . D^2: 4.45390390507273 . ここからはStep1からStep3までを実装します。実際にはループを用いるのですが、まずは一度だけStep1～3を実行する場合を考えます。 . Expectation Step . Step1のExpectation Stepでは$ lambda_{ boldsymbol{f}^{(j-1)}}( boldsymbol{x}_i)$に対する$ boldsymbol{x}_i$のスプライン曲線$( in mathbb{R}^p)$を$ boldsymbol{f}^{(j)}( lambda)$の近似とします。 スプライン曲線のために scipy.interpolate.UnivariateSpline を用い、 このインスタンスに lam と X[:, 0] またはX[:, 1] を渡します。ただし、lam は完全に増加している必要があるので、 重複を削除し、昇順に並び替えます。このとき対応するXのデータもスプライン曲線の生成から除外します。 また、seq_lam は$ lambda$を均等に取った配列です。ただし、ステップごとに曲線の長さを伸長するためにlamの値域から少し大きくしたものとしています。 . &quot;&quot;&quot; Expectation Step: 曲線上に射影されるデータ点の平均（正確には条件付期待値）を求める Xの分布が未知の場合にはLocally weighted regressionや cubic smoothing splinesで置き換えられる &quot;&quot;&quot; sorted_lam, lam_idx = np.unique(lam, return_index=True) f0 = UnivariateSpline(x=sorted_lam, y=X[lam_idx, 0], k=3, s=None) f1 = UnivariateSpline(x=sorted_lam, y=X[lam_idx, 1], k=3, s=None) seq_lam = np.linspace(lam.min()-lam_plus, lam.max()+lam_plus, n_seq_lam) f_points = np.vstack((f0(seq_lam), f1(seq_lam))).T Z = np.vstack((f0(lam), f1(lam))).T # 描画 fig = plt.figure(figsize=(6, 6)) ax = fig.add_subplot(aspect=&#39;1&#39;) ax.scatter(X[:, 0], X[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) ax.scatter(Z[:, 0], Z[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) lines = [[(X[i, 0], X[i, 1]), (Z[i, 0], Z[i, 1])] for i in range(n_samples)] lc = mc.LineCollection(lines, colors=&quot;k&quot;, linewidths=1) ax.add_collection(lc) ax.plot(f_points[:, 0], f_points[:, 1]) plt.title(&quot;Expectation Step&quot;) plt.xlim(xmin, xmax); plt.ylim(ymin, ymax) plt.xlabel(&#39;x0&#39;); plt.ylabel(&#39;x1&#39;) plt.show() . Projection Step . Projection Stepではデータ点と曲線上の点の距離を計算し、距離が最小となる点の$ lambda$である $ lambda_{ boldsymbol{f}^{(j)}}( boldsymbol{X})$を新しい$ lambda^{(j)}( boldsymbol{X})$とします。 . &quot;&quot;&quot; Projection Step: データ点から最も近い曲線上の点を求める 最適化問題を解かせてもよいが、ステップを小さく取り、距離が最小の点を取る &quot;&quot;&quot; f_ = np.repeat(np.expand_dims(f_points, 0), n_samples, axis=0) X_ = np.repeat(np.expand_dims(X, 1), n_seq_lam, axis=1) dist = np.sum((f_ - X_)**2, axis=2) # 点と関数の距離 min_dist_idx = np.argmin(dist, axis=1) # 距離が最初となるindex Z = f_points[min_dist_idx] # 射影点の更新 lam = seq_lam[min_dist_idx] # lambdaの更新 lines = [[(X[i, 0], X[i, 1]), (Z[i, 0], Z[i, 1])] for i in range(n_samples)] lc = mc.LineCollection(lines, colors=&quot;k&quot;, linewidths=1) # 描画 fig = plt.figure(figsize=(6, 6)) ax = fig.add_subplot(aspect=&#39;1&#39;) ax.scatter(X[:, 0], X[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) ax.scatter(Z[:, 0], Z[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) ax.add_collection(lc) ax.plot(f_points[:, 0], f_points[:, 1]) plt.title(&quot;Projection Step&quot;) plt.xlim(xmin, xmax); plt.ylim(ymin, ymax) plt.xlabel(&#39;x0&#39;); plt.ylabel(&#39;x1&#39;) plt.show() D2 = np.sum((X-Z)**2) print(&quot;D^2:&quot;, D2) . D^2: 1.688773139900023 . &#12467;&#12540;&#12489;&#12398;&#20840;&#20307;&#12392;&#32080;&#26524; . これまで見てきたStep0～Step3ですが、実際にはこれを繰り返します。収束を考えず、単にforループを用いた実装の全体は次のようになります。 . import numpy as np import matplotlib.pyplot as plt from matplotlib import collections as mc from scipy.interpolate import UnivariateSpline # Generate toy data np.random.seed(0) n_samples = 100 x0 = np.random.uniform(-1, 1, size=n_samples) x1 = x0**5 + 0.15*np.random.randn(n_samples) X = np.vstack((x0, x1)).T # SVD u, s, v = np.linalg.svd(X) # 主成分上へのXの射影 expand_v = np.expand_dims(v[0], axis=1) Z = X @ expand_v @ expand_v.T # 元データから射影への直線のリスト lines = [[(X[i, 0], X[i, 1]), (Z[i, 0], Z[i, 1])] for i in range(n_samples)] lc = mc.LineCollection(lines, colors=&quot;k&quot;, linewidths=1) # 主成分軸の描画用変数 xmax = ymax = 1.3 xmin = ymin = -1.3 zs = np.arange(Z[:, 0].min(), Z[:, 0].max(), 1e-2) # 描画 fig = plt.figure(figsize=(10, 5)) ax = fig.add_subplot(aspect=&#39;1&#39;) #こうしないと線が傾いて見える ax.scatter(X[:, 0], X[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) # 元データのplot ax.scatter(Z[:, 0], Z[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) # 射影のplot ax.add_collection(lc) ax.plot(zs, v[0, 1] / v[0, 0] * zs) # 主成分軸 plt.title(&quot;Initialization&quot;) plt.xlim(xmin, xmax); plt.ylim(ymin, ymax) plt.xlabel(&#39;x0&#39;); plt.ylabel(&#39;x1&#39;) plt.show() D2 = np.sum((X-Z)**2) print(&quot;D^2:&quot;, D2) . D^2: 4.45390390507273 . D2_list = [] # 点群と曲線の距離の2乗和を記録するリスト D2_list.append(D2) lam = u[:,0]*s[0] # lambdaの初期値 lam_plus = 0.01 # iterごとに伸長するlamの長さ n_iter = 5 # 探索の回数 n_seq_lam = int(1e4) # 離散化した曲線の点の数 for i in range(n_iter): &quot;&quot;&quot; Expectation Step: 曲線上に射影されるデータ点の平均（正確には条件付期待値）を求める Xの分布が未知の場合にはLocally weighted regressionや cubic smoothing splinesで置き換えられる &quot;&quot;&quot; sorted_lam, lam_idx = np.unique(lam, return_index=True) f0 = UnivariateSpline(x=sorted_lam, y=X[lam_idx, 0], k=3, s=None) f1 = UnivariateSpline(x=sorted_lam, y=X[lam_idx, 1], k=3, s=None) seq_lam = np.linspace(lam.min()-lam_plus, lam.max()+lam_plus, n_seq_lam) f_points = np.vstack((f0(seq_lam), f1(seq_lam))).T Z = np.vstack((f0(lam), f1(lam))).T # 描画 fig = plt.figure(figsize=(10, 5)) ax1 = fig.add_subplot(1, 2, 1, aspect=&#39;1&#39;) ax1.scatter(X[:, 0], X[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) ax1.scatter(Z[:, 0], Z[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) lines = [[(X[i, 0], X[i, 1]), (Z[i, 0], Z[i, 1])] for i in range(n_samples)] lc = mc.LineCollection(lines, colors=&quot;k&quot;, linewidths=1) ax1.add_collection(lc) ax1.plot(f_points[:, 0], f_points[:, 1]) plt.title(&quot;Expectation Step (&quot;+str(i+1)+&quot; iter)&quot;) plt.xlim(xmin, xmax); plt.ylim(ymin, ymax) plt.xlabel(&#39;x0&#39;); plt.ylabel(&#39;x1&#39;) &quot;&quot;&quot; Projection Step: データ点から最も近い曲線上の点を求める 最適化問題を解かせてもよいが、ステップを小さく取り、距離が最小の点を取る &quot;&quot;&quot; f_ = np.repeat(np.expand_dims(f_points, 0), n_samples, axis=0) X_ = np.repeat(np.expand_dims(X, 1), n_seq_lam, axis=1) dist = np.sum((f_ - X_)**2, axis=2) # 点と関数の距離 min_dist_idx = np.argmin(dist, axis=1) # 距離が最初となるindex Z = f_points[min_dist_idx] # 射影点の更新 lam = seq_lam[min_dist_idx] # lambdaの更新 lines = [[(X[i, 0], X[i, 1]), (Z[i, 0], Z[i, 1])] for i in range(n_samples)] lc = mc.LineCollection(lines, colors=&quot;k&quot;, linewidths=1) # 描画 ax2 = fig.add_subplot(1,2,2, aspect=&#39;1&#39;) ax2.scatter(X[:, 0], X[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) ax2.scatter(Z[:, 0], Z[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) ax2.add_collection(lc) ax2.plot(f_points[:, 0], f_points[:, 1]) plt.title(&quot;Projection Step (&quot;+str(i+1)+&quot; iter)&quot;) plt.xlim(xmin, xmax); plt.ylim(ymin, ymax) plt.xlabel(&#39;x0&#39;); plt.ylabel(&#39;x1&#39;) plt.tight_layout() plt.show() D2 = np.sum((X-Z)**2) D2_list.append(D2) print(&quot;D^2:&quot;, D2) . D^2: 1.688773139900023 . D^2: 1.6072936372827606 . D^2: 1.573855714007582 . D^2: 1.5521875833071004 . D^2: 1.5358119308800324 . このコードを実行し、結果の図をgifアニメーションにしたものは次のようになります。 . . さらにデータ点と曲線の距離の2乗和の変化は次のようになります。 . # 点群と曲線の距離の2乗和の変化 plt.figure(figsize=(4, 4)) plt.plot(np.arange(len(D2_list)), np.array(D2_list)) plt.xlabel(&quot;Iterations&quot;) plt.ylabel(&quot;$D^2$&quot;) plt.show() . ちゃんと収束していることが分かります。 . &#27096;&#12293;&#12394;&#12487;&#12540;&#12479;&#12475;&#12483;&#12488;&#12395;&#23550;&#12377;&#12427;Principal curves&#12398;&#36969;&#29992;&#20363; . ここからは他のデータセットにおけるPrincipal curveを求めてみます。 . &#20870;&#21608;&#29366;&#12395;&#25955;&#24067;&#12373;&#12428;&#12383;&#28857;&#32676; . #collapse-hide import numpy as np import matplotlib.pyplot as plt from matplotlib import collections as mc from scipy.interpolate import UnivariateSpline # Generate toy data np.random.seed(0) n_samples = 100 r = 1 theta = np.random.uniform(0, 2*np.pi, size=n_samples) x0 = r*np.cos(theta) + 0.03*np.random.randn(n_samples) x1 = r*np.sin(theta) + 0.03*np.random.randn(n_samples) X = np.vstack((x0, x1)).T # SVD u, s, v = np.linalg.svd(X) # 主成分上へのXの射影 expand_v = np.expand_dims(v[0], axis=1) Z = X @ expand_v @ expand_v.T # 元データから射影への直線のリスト lines = [[(X[i, 0], X[i, 1]), (Z[i, 0], Z[i, 1])] for i in range(n_samples)] lc = mc.LineCollection(lines, colors=&quot;k&quot;, linewidths=1) # 主成分軸の描画用変数 xmax = ymax = 1.3 xmin = ymin = -1.3 zs = np.arange(Z[:, 0].min(), Z[:, 0].max(), 1e-2) # 描画 fig = plt.figure(figsize=(6, 6)) ax = fig.add_subplot(aspect=&#39;1&#39;) #こうしないと線が傾いて見える ax.scatter(X[:, 0], X[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) # 元データのplot ax.scatter(Z[:, 0], Z[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) # 射影のplot ax.add_collection(lc) ax.plot(zs, v[0, 1] / v[0, 0] * zs) # 主成分軸 plt.title(&quot;Initialization&quot;) plt.xlim(xmin, xmax); plt.ylim(ymin, ymax) plt.xlabel(&#39;x0&#39;); plt.ylabel(&#39;x1&#39;) plt.show() D2 = np.sum((X-Z)**2) print(&quot;D^2:&quot;, D2) D2_list = [] # 点群と曲線の距離の2乗和を記録するリスト D2_list.append(D2) lam = u[:,0]*s[0] # lambdaの初期値 lam_plus = 0.05 # iterごとに伸長するlamの長さ n_iter = 8 # 探索の回数 n_seq_lam = int(1e4) # 離散化した曲線の点の数 for i in range(n_iter): &quot;&quot;&quot; Expectation Step: 曲線上に射影されるデータ点の平均（正確には条件付期待値）を求める Xの分布が未知の場合にはLocally weighted regressionや cubic smoothing splinesで置き換えられる &quot;&quot;&quot; sorted_lam, lam_idx = np.unique(lam, return_index=True) f0 = UnivariateSpline(x=sorted_lam, y=X[lam_idx, 0], k=3, s=None) f1 = UnivariateSpline(x=sorted_lam, y=X[lam_idx, 1], k=3, s=None) seq_lam = np.linspace(lam.min()-lam_plus, lam.max()+lam_plus, n_seq_lam) f_points = np.vstack((f0(seq_lam), f1(seq_lam))).T Z = np.vstack((f0(lam), f1(lam))).T # 描画 fig = plt.figure(figsize=(6, 6)) ax = fig.add_subplot(aspect=&#39;1&#39;) ax.scatter(X[:, 0], X[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) ax.scatter(Z[:, 0], Z[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) lines = [[(X[i, 0], X[i, 1]), (Z[i, 0], Z[i, 1])] for i in range(n_samples)] lc = mc.LineCollection(lines, colors=&quot;k&quot;, linewidths=1) ax.add_collection(lc) ax.plot(f_points[:, 0], f_points[:, 1]) plt.title(&quot;Expectation Step (&quot;+str(i+1)+&quot; iter)&quot;) plt.xlim(xmin, xmax); plt.ylim(ymin, ymax) plt.xlabel(&#39;x0&#39;); plt.ylabel(&#39;x1&#39;) plt.show() &quot;&quot;&quot; Projection Step: データ点から最も近い曲線上の点を求める 最適化問題を解かせてもよいが、ステップを小さく取り、距離が最小の点を取る &quot;&quot;&quot; f_ = np.repeat(np.expand_dims(f_points, 0), n_samples, axis=0) X_ = np.repeat(np.expand_dims(X, 1), n_seq_lam, axis=1) dist = np.sum((f_ - X_)**2, axis=2) # 点と関数の距離 min_dist_idx = np.argmin(dist, axis=1) # 距離が最初となるindex Z = f_points[min_dist_idx] # 射影点の更新 lam = seq_lam[min_dist_idx] # lambdaの更新 lines = [[(X[i, 0], X[i, 1]), (Z[i, 0], Z[i, 1])] for i in range(n_samples)] lc = mc.LineCollection(lines, colors=&quot;k&quot;, linewidths=1) # 描画 fig = plt.figure(figsize=(6, 6)) ax = fig.add_subplot(aspect=&#39;1&#39;) ax.scatter(X[:, 0], X[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) ax.scatter(Z[:, 0], Z[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) ax.add_collection(lc) ax.plot(f_points[:, 0], f_points[:, 1]) plt.title(&quot;Projection Step (&quot;+str(i+1)+&quot; iter)&quot;) plt.xlim(xmin, xmax); plt.ylim(ymin, ymax) plt.xlabel(&#39;x0&#39;); plt.ylabel(&#39;x1&#39;) plt.show() D2 = np.sum((X-Z)**2) D2_list.append(D2) print(&quot;D^2:&quot;, D2) # 点群と曲線の距離の2乗和の変化 plt.figure(figsize=(4, 4)) plt.plot(np.arange(len(D2_list)), np.array(D2_list)) plt.xlabel(&quot;Iterations&quot;) plt.ylabel(&quot;$D^2$&quot;) plt.show() . . 実行した結果のgifアニメーションは次のようになります。 . . この例は (Hastie &amp; Stuetzle, 1989)にある例の1つです。同様の結果が再現されています。 . なお、seed値を変えると次のようになります。 . . このように上手くいかない場合もあります。 . 2&#27425;&#20803;Swiss Roll . #collapse-hide import numpy as np import matplotlib.pyplot as plt from matplotlib import collections as mc from scipy.interpolate import UnivariateSpline # Generate toy data np.random.seed(0) n_samples = 100 r = 0.1 theta = np.random.uniform(np.pi, 4*np.pi, size=n_samples) x0 = r*theta*np.cos(theta) x1 = r*theta*np.sin(theta) X = np.vstack((x0, x1)).T # SVD u, s, v = np.linalg.svd(X) # 主成分上へのXの射影 expand_v = np.expand_dims(v[0], axis=1) Z = X @ expand_v @ expand_v.T # 元データから射影への直線のリスト lines = [[(X[i, 0], X[i, 1]), (Z[i, 0], Z[i, 1])] for i in range(n_samples)] lc = mc.LineCollection(lines, colors=&quot;k&quot;, linewidths=1) # 主成分軸の描画用変数 xmax = ymax = 1.5 xmin = ymin = -1.5 zs = np.arange(Z[:, 0].min(), Z[:, 0].max(), 1e-2) # 描画 fig = plt.figure(figsize=(6, 6)) ax = fig.add_subplot(aspect=&#39;1&#39;) #こうしないと線が傾いて見える ax.scatter(X[:, 0], X[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) # 元データのplot ax.scatter(Z[:, 0], Z[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) # 射影のplot ax.add_collection(lc) ax.plot(zs, v[0, 1] / v[0, 0] * zs) # 主成分軸 plt.title(&quot;Initialization&quot;) plt.xlim(xmin, xmax); plt.ylim(ymin, ymax) plt.xlabel(&#39;x0&#39;); plt.ylabel(&#39;x1&#39;) plt.show() D2 = np.sum((X-Z)**2) print(&quot;D^2:&quot;, D2) D2_list = [] # 点群と曲線の距離の2乗和を記録するリスト D2_list.append(D2) lam = u[:,0]*s[0] # lambdaの初期値 lam_plus = 0.01 # iterごとに伸長するlamの長さ n_iter = 10 # 探索の回数 n_seq_lam = int(1e4) # 離散化した曲線の点の数 for i in range(n_iter): &quot;&quot;&quot; Expectation Step: 曲線上に射影されるデータ点の平均（正確には条件付期待値）を求める Xの分布が未知の場合にはLocally weighted regressionや cubic smoothing splinesで置き換えられる &quot;&quot;&quot; sorted_lam, lam_idx = np.unique(lam, return_index=True) f0 = UnivariateSpline(x=sorted_lam, y=X[lam_idx, 0], k=3, s=None) f1 = UnivariateSpline(x=sorted_lam, y=X[lam_idx, 1], k=3, s=None) seq_lam = np.linspace(lam.min()-lam_plus, lam.max()+lam_plus, n_seq_lam) f_points = np.vstack((f0(seq_lam), f1(seq_lam))).T Z = np.vstack((f0(lam), f1(lam))).T # 描画 fig = plt.figure(figsize=(6, 6)) ax = fig.add_subplot(aspect=&#39;1&#39;) ax.scatter(X[:, 0], X[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) ax.scatter(Z[:, 0], Z[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) lines = [[(X[i, 0], X[i, 1]), (Z[i, 0], Z[i, 1])] for i in range(n_samples)] lc = mc.LineCollection(lines, colors=&quot;k&quot;, linewidths=1) ax.add_collection(lc) ax.plot(f_points[:, 0], f_points[:, 1]) plt.title(&quot;Expectation Step (&quot;+str(i+1)+&quot; iter)&quot;) plt.xlim(xmin, xmax); plt.ylim(ymin, ymax) plt.xlabel(&#39;x0&#39;); plt.ylabel(&#39;x1&#39;) plt.show() &quot;&quot;&quot; Projection Step: データ点から最も近い曲線上の点を求める 最適化問題を解かせてもよいが、ステップを小さく取り、距離が最小の点を取る &quot;&quot;&quot; f_ = np.repeat(np.expand_dims(f_points, 0), n_samples, axis=0) X_ = np.repeat(np.expand_dims(X, 1), n_seq_lam, axis=1) dist = np.sum((f_ - X_)**2, axis=2) # 点と関数の距離 min_dist_idx = np.argmin(dist, axis=1) # 距離が最初となるindex Z = f_points[min_dist_idx] # 射影点の更新 lam = seq_lam[min_dist_idx] # lambdaの更新 lines = [[(X[i, 0], X[i, 1]), (Z[i, 0], Z[i, 1])] for i in range(n_samples)] lc = mc.LineCollection(lines, colors=&quot;k&quot;, linewidths=1) # 描画 fig = plt.figure(figsize=(6, 6)) ax = fig.add_subplot(aspect=&#39;1&#39;) ax.scatter(X[:, 0], X[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) ax.scatter(Z[:, 0], Z[:, 1], s=10, marker=&#39;o&#39;, color=&#39;k&#39;) ax.add_collection(lc) ax.plot(f_points[:, 0], f_points[:, 1]) plt.title(&quot;Projection Step (&quot;+str(i+1)+&quot; iter)&quot;) plt.xlim(xmin, xmax); plt.ylim(ymin, ymax) plt.xlabel(&#39;x0&#39;); plt.ylabel(&#39;x1&#39;) plt.show() D2 = np.sum((X-Z)**2) D2_list.append(D2) print(&quot;D^2:&quot;, D2) # 点群と曲線の距離の2乗和の変化 plt.figure(figsize=(4, 4)) plt.plot(np.arange(len(D2_list)), np.array(D2_list)) plt.xlabel(&quot;Iterations&quot;) plt.ylabel(&quot;$D^2$&quot;) plt.show() . . 実行した結果のgifアニメーションは次のようになります。 . . 今回実装したbendingアルゴリズムでは残念ながらSwiss Rollデータセットに対して上手くprincipal curveを生成することができませんが、principal curveのアルゴリズムによっては正しく多様体学習が行えるようです (Laparra &amp; Malo, 2016). . &#21442;&#32771;&#25991;&#29486; . Guedj, B., and Li, L. (2019). Sequential Learning of Principal Curves: Summarizing Data Streams on the Fly. ffhal-01796011v2f (pdf) | Hastie, T. and Stuetzle, W. (1989). Principal Curves. JASA, 84 (406), 502-516. (pdf) | Hastie, T., Tibshirani, R., and Friedman, J. (2009). *T**he Elements of Statistical Learning: Data Mining, Inference, and Prediction***. (2nd ed.). Springer. (website) | Kegl, B.,Krzyzak, A., Linder, T., and Zeger, K. (2000). Learning and design of principal curves. IEEE. 22 (3), 281-297. (pdf) | Laparra, V., and Malo, J. (2016). Sequential principal curves analysis. arXiv [preprint], https://arxiv.org/abs/1606.00856. | Sorzano, C.O.S., Vargas, J., and Montano A. P. (2014). A survey of dimensionality reduction techniques. arXiv [preprint], https://arxiv.org/abs/1403.2877. | Verbeek, J., Vlassis, N., and Krose, B. (2002). A k-segments algorithm for finding principal curves. Pattern Recognition Letters, Elsevier. 23 (8), 1009–1017. (pdf) | Rでの実装例：https://biostatmatt.com/archives/2894 | .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/statistics/2019/10/06/princurve2.html",
            "relUrl": "/statistics/2019/10/06/princurve2.html",
            "date": " • Oct 6, 2019"
        }
        
    
  
    
        ,"post13": {
            "title": "Matplotlibで複数の線分を描画する方法",
            "content": "Matplotlibで線を描画するにはmatplotlib.lines.Line2Dなどがあるが、一度に複数の線を引くことは（forループを使う以外には）できない。 . こういう場合にはmatplotlib.collections.linecollectionを用いるといい、という話。 . &#20351;&#29992;&#20363;1 . $x in mathbb{R}$としたときの2点$(x, 0), (0, x)$を結ぶ線分を10本描く。 . import numpy as np import matplotlib.pyplot as plt import matplotlib.collections as mc import matplotlib.cm as cm . N = 10 # 線の数 x = np.linspace(-1, 1, N) # 線のリスト : [(x0, y0), (x1, y1)]が1つの線 lines = [[(0, x[i]), (x[i], 0)] for i in range(N)] colors = [cm.viridis(i/N) for i in range(N)] # 色のリスト lc = mc.LineCollection(lines, colors=colors, linewidths=2) # 描画 fig = plt.figure(figsize=(4,4)) ax = fig.add_subplot(aspect=&#39;1&#39;) ax.add_collection(lc) ax.autoscale() plt.show() . &#20351;&#29992;&#20363;2 . PCAの際に元データから第一主成分軸に下ろした垂線を描く（実際には元データと射影の点を結ぶ）。 . # Generate toy data np.random.seed(0) n_samples = 50 x0 = np.random.uniform(-3, 3, size=n_samples) x1 = x0 + np.random.randn(n_samples) X = np.vstack((x0, x1)).T . # SVD u, s, v = np.linalg.svd(X) # 主成分上へのXの射影 expand_v = np.expand_dims(v[0], axis=1) Z = X @ expand_v @ expand_v.T # 元データから射影への直線のリスト lines = [[(X[i, 0], X[i, 1]), (Z[i, 0], Z[i, 1])] for i in range(n_samples)] lc = mc.LineCollection(lines, colors=&quot;k&quot;, linewidths=1) . # 主成分軸の描画用変数 xmax = ymax = 3 xmin = ymin = -3 x = np.arange(xmin, xmax, 1e-2) # 描画 fig = plt.figure(figsize=(5, 5)) ax = fig.add_subplot(aspect=&#39;1&#39;) ax.scatter(X[:, 0], X[:, 1], s=5, marker=&#39;o&#39;, color=&#39;k&#39;) # 元データのplot ax.scatter(Z[:, 0], Z[:, 1], s=5, marker=&#39;o&#39;, color=&#39;k&#39;) # 射影のplot ax.add_collection(lc) # 主成分軸 #ax.quiver(0, 0, v[:, 0], v[:, 1], zorder=11, width=0.01, scale=6, color=&#39;orange&#39;) # ベクトル表示 for i in range(2): ax.plot(x, v[i, 1] / v[i, 0] * x) plt.hlines(0, xmin, xmax, linestyle=&quot;dashed&quot;) # x軸 plt.vlines(0, ymin, ymax, linestyle=&quot;dashed&quot;) # y軸 plt.xlim(xmin, xmax); plt.ylim(ymin, ymax) plt.xlabel(&#39;x&#39;); plt.ylabel(&#39;y&#39;) plt.show() .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/python/2019/10/04/multilines.html",
            "relUrl": "/python/2019/10/04/multilines.html",
            "date": " • Oct 4, 2019"
        }
        
    
  
    
        ,"post14": {
            "title": "scGenの解説",
            "content": "概要 . scGenの論文の解説．実際はほぼVAEの話になった．実験の詳細とかは割愛．LPSが異なる生物に与える影響とかを予測していてかなりアツい． . 細胞が外界から刺激を受けたときにどんな反応をするかを予測する．潜在空間で摂動$ delta$が加えられたときに，遺伝子発現空間での変化をニューラルネットワークを使って予測する． . VAE 　 . variational autoencoder(VAE)では確率分布$P( boldsymbol{x} _ i; boldsymbol{ theta})$に従って新しいデータ点が生成される．ただし，確率分布は$P( boldsymbol{x} _ i; boldsymbol{ theta})$の対数尤度（を各$ boldsymbol{x} _ i$について足したもの）が最大となるように取る．潜在変数を$ boldsymbol{z}$とすれば，この確率は次のようになる： . P(xi;θ)=∫P(xi∣zi;θ)P(zi;θ) dzi.(1) begin{aligned} P( boldsymbol{x} _ i; boldsymbol{ theta})= int P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta})P( boldsymbol{z} _ i; boldsymbol{ theta}) ,d boldsymbol{z} _ i. end{aligned} tag{1}P(xi​;θ)=∫P(xi​∣zi​;θ)P(zi​;θ)dzi​.​(1) . $ boldsymbol{x} _ i$を生成しそうな$ boldsymbol{z} _ i$が潜在空間から正規分布$P( boldsymbol{z} _ i; boldsymbol{ theta})$に従ってサンプリングされるような確率分布を求めることが目標になる． . ここで，$P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta})$に近い確率分布$Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ theta})$をニューラルネットワークで作る．２つの確率分布の近さの評価としてKullback Leibler divergenceを使う： . KL(Q(zi∣xi;ϕ)∥P(zi∣xi;θ))=EQ(zi∣xi;ϕ)[log⁡Q(zi∣xi;ϕ)−log⁡P(zi∣xi;θ)]=EQ(zi∣xi;ϕ)[log⁡Q(zi∣xi;ϕ)−log⁡P(zi;θ)P(xi;θ)P(xi∣zi;θ)]=EQ(zi∣xi;ϕ)[log⁡Q(zi∣xi;ϕ)−log⁡P(zi;θ)−log⁡P(xi∣zi;θ)+log⁡P(xi;θ)]=−EQ(zi∣xi;ϕ)[log⁡P(xi∣zi;θ)]+log⁡P(xi;θ)+KL(Q(zi∣xi;ϕ)∥P(zi;θ)).(2) begin{aligned} &amp; text{KL}(Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi}) |P( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ theta})) &amp;= E _ {Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})} left[ log Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})- log P( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ theta}) right] &amp;= E _ {Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})} Bigl[ log Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})- log frac{P( boldsymbol{z} _ i; boldsymbol{ theta})}{P( boldsymbol{x} _ i; boldsymbol{ theta})}P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta}) Bigr] &amp;= E _ {Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})} bigl[ log Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})- log P( boldsymbol{z} _ i; boldsymbol{ theta})- log P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta})+ log P( boldsymbol{x} _ i; boldsymbol{ theta}) bigr] &amp;= -E _ {Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})}[ log P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta})]+ log P( boldsymbol{x} _ i; boldsymbol{ theta})+ text{KL}(Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi}) | P( boldsymbol{z} _ i; boldsymbol{ theta})). end{aligned} tag{2}​KL(Q(zi​∣xi​;ϕ)∥P(zi​∣xi​;θ))=EQ(zi​∣xi​;ϕ)​[logQ(zi​∣xi​;ϕ)−logP(zi​∣xi​;θ)]=EQ(zi​∣xi​;ϕ)​[logQ(zi​∣xi​;ϕ)−logP(xi​;θ)P(zi​;θ)​P(xi​∣zi​;θ)]=EQ(zi​∣xi​;ϕ)​[logQ(zi​∣xi​;ϕ)−logP(zi​;θ)−logP(xi​∣zi​;θ)+logP(xi​;θ)]=−EQ(zi​∣xi​;ϕ)​[logP(xi​∣zi​;θ)]+logP(xi​;θ)+KL(Q(zi​∣xi​;ϕ)∥P(zi​;θ)).​(2) . 式変形の途中でBayesの定理を用いた．$Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ theta})$は$P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta})$の近似であるので，これらの量のKullback Leibler divergenceはほぼ$0$である．よって， . log⁡P(xi;θ)≃EQ(zi∣xi;ϕ)[log⁡P(xi∣zi;θ)]−KL(Q(zi∣xi;ϕ)∥P(zi;θ)).(3) begin{aligned} log P( boldsymbol{x} _ i; boldsymbol{ theta}) simeq E _ {Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})}[ log P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta})]- text{KL}(Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi}) | P( boldsymbol{z} _ i; boldsymbol{ theta})). end{aligned} tag{3}logP(xi​;θ)≃EQ(zi​∣xi​;ϕ)​[logP(xi​∣zi​;θ)]−KL(Q(zi​∣xi​;ϕ)∥P(zi​;θ)).​(3) . (3)式第１項は解析的に解くことは困難なので，Monte Carlo法によるサンプリングによって決める．ただし，$Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})$に従って$ boldsymbol{z} _ i$を選び出す： . zi∼Q(zi∣xi;ϕ)(4) boldsymbol{z} _ i sim Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi}) tag{4}zi​∼Q(zi​∣xi​;ϕ)(4) . のは不便であるので，再パラメータ化を考える．すなわち， . zi=gϕ(ϵ,xi),ϵ∼p(ϵ)(5) boldsymbol{z} _ i = g _ { boldsymbol{ phi}}( boldsymbol{ epsilon}, boldsymbol{x} _ i), quad boldsymbol{ epsilon} sim p( boldsymbol{ epsilon}) tag{5}zi​=gϕ​(ϵ,xi​),ϵ∼p(ϵ)(5) . となる関数$g _ { boldsymbol{ phi}}$とノイズ$ boldsymbol{ epsilon}$を適当に選んでやる．こうすれば， . EQ(zi∣xi;ϕ)[f(zi)]=∫Q(zi∣xi;ϕ)f(zi)dzi=∫p(ϵ)f(zi)dϵ=Ep(ϵ)[f(zi)],zi=gϕ(ϵ,xi)(6) begin{aligned} E _ {Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})}[f( boldsymbol{z} _ i)] &amp;= int Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi}) f( boldsymbol{z} _ i)d boldsymbol{z} _ i &amp;= int p( boldsymbol{ epsilon}) f( boldsymbol{z} _ i)d boldsymbol{ epsilon} &amp;= E _ {p( boldsymbol{ epsilon})}[f( boldsymbol{z} _ i)], quad boldsymbol{z} _ i = g _ { boldsymbol{ phi}}( boldsymbol{ epsilon}, boldsymbol{x} _ i) end{aligned} tag{6}EQ(zi​∣xi​;ϕ)​[f(zi​)]​=∫Q(zi​∣xi​;ϕ)f(zi​)dzi​=∫p(ϵ)f(zi​)dϵ=Ep(ϵ)​[f(zi​)],zi​=gϕ​(ϵ,xi​)​(6) . となる．よって，再パラメータ化を施せば(3)第１項は . EQ(zi∣xi;ϕ)[log⁡P(xi∣zi;θ)]=1L∑l=1Llog⁡P(xi∣zi(l);θ)(7)E _ {Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})}[ log P( boldsymbol{x} _ i mid boldsymbol{z} _ i; boldsymbol{ theta})] = frac{1}{L} sum _ {l=1} ^ L log P( boldsymbol{x} _ i mid boldsymbol{z} _ i ^ {(l)}; boldsymbol{ theta}) tag{7}EQ(zi​∣xi​;ϕ)​[logP(xi​∣zi​;θ)]=L1​l=1∑L​logP(xi​∣zi(l)​;θ)(7) . となる．ただし， . zi(l)=gϕ(ϵ(l),xi),ϵ(l)∼p(ϵ)(8) boldsymbol{z} _ i ^ {(l)}=g _ { boldsymbol{ phi}}( boldsymbol{ epsilon} ^ {(l)}, boldsymbol{x} _ i), quad boldsymbol{ epsilon} ^ {(l)} sim p( boldsymbol{ epsilon}) tag{8}zi(l)​=gϕ​(ϵ(l),xi​),ϵ(l)∼p(ϵ)(8) . である．よって，(3)は次のようになる： . log⁡P(xi;θ)=1L∑l=1Llog⁡P(xi∣zi(l);θ)−KL(Q(zi∣xi;ϕ)∥P(zi;θ)).(9) log P( boldsymbol{x} _ i; boldsymbol{ theta})= frac{1}{L} sum _ {l=1} ^ L log P( boldsymbol{x} _ i mid boldsymbol{z} _ i ^ {(l)}; boldsymbol{ theta})- text{KL}(Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi}) | P( boldsymbol{z} _ i; boldsymbol{ theta})). tag{9}logP(xi​;θ)=L1​l=1∑L​logP(xi​∣zi(l)​;θ)−KL(Q(zi​∣xi​;ϕ)∥P(zi​;θ)).(9) . ここで，$Q( boldsymbol{z} mid boldsymbol{x}; boldsymbol{ theta})$が多変量正規分布であると仮定する： . Q(z∣x;θ)=1(2π)n∣Σ∣exp⁡[−12t(z−μ)Σ−1(z−μ)].(10)Q( boldsymbol{z} mid boldsymbol{x}; boldsymbol{ theta})= frac{1}{ sqrt{(2 pi) ^ n mid Sigma mid}} exp left[- frac{1}{2} ^ t( boldsymbol{z}- boldsymbol mu) Sigma ^ {-1}( boldsymbol{z}- boldsymbol mu) right]. tag{10}Q(z∣x;θ)=(2π)n∣Σ∣ . ​1​exp[−21​t(z−μ)Σ−1(z−μ)].(10) . ただし，$ Sigma, boldsymbol mu$は$ boldsymbol{x}$によって決まり，特に$ Sigma$は対角行列であるとする： . Σ=(σ12Oσ22⋱Oσn2).(11) begin{aligned} Sigma = begin{pmatrix} sigma _ 1 ^ 2 &amp;&amp;&amp;O &amp; sigma _ 2 ^ 2 &amp;&amp; &amp;&amp; ddots&amp; O &amp;&amp;&amp; sigma _ n ^ 2 end{pmatrix}. end{aligned} tag{11}Σ=⎝⎜⎜⎜⎛​σ12​O​σ22​​⋱​Oσn2​​⎠⎟⎟⎟⎞​.​(11) . さらに，$P( boldsymbol{z}; boldsymbol{ theta})$も正規分布であるとする： . P(z;θ)=1(2π)nexp⁡(−∣z∣22).(12)P( boldsymbol{z}; boldsymbol{ theta}) = frac{1}{ sqrt{(2 pi) ^ n}} exp left(- frac{ mid boldsymbol{z} mid ^ 2}{2} right). tag{12}P(z;θ)=(2π)n . ​1​exp(−2∣z∣2​).(12) . まず， . ∫Q(z∣x;θ)log⁡P(z;θ) dz=1(2π)n∣Σ∣∫exp⁡[−12t(z−μ)Σ−1(z−μ)]×[−n2log⁡(2π)−∣z∣22] dz=−n2log⁡(2π)−12(2π)n∣Σ∣×∫∣z∣2exp⁡[−12t(z−μ)Σ−1(z−μ)] dz(13) begin{aligned} &amp; int Q( boldsymbol{z} mid boldsymbol{x}; boldsymbol{ theta}) log P( boldsymbol{z}; boldsymbol{ theta}) ,d boldsymbol{z} &amp;= frac{1}{ sqrt{(2 pi) ^ n mid Sigma mid}} int exp left[- frac{1}{2} ^ t( boldsymbol{z}- boldsymbol mu) Sigma ^ {-1}( boldsymbol{z}- boldsymbol mu) right] times left[- frac{n}{2} log(2 pi)- frac{ mid boldsymbol{z} mid ^ 2}{2} right] ,d boldsymbol{z} &amp;= - frac{n}{2} log(2 pi)- frac{1}{2 sqrt{(2 pi) ^ n mid Sigma mid}} times int mid boldsymbol{z} mid ^ 2 exp left[- frac{1}{2} ^ t( boldsymbol{z}- boldsymbol mu) Sigma ^ {-1}( boldsymbol{z}- boldsymbol mu) right] ,d boldsymbol{z} end{aligned} tag{13}​∫Q(z∣x;θ)logP(z;θ)dz=(2π)n∣Σ∣ . ​1​∫exp[−21​t(z−μ)Σ−1(z−μ)]×[−2n​log(2π)−2∣z∣2​]dz=−2n​log(2π)−2(2π)n∣Σ∣ . ​1​×∫∣z∣2exp[−21​t(z−μ)Σ−1(z−μ)]dz​(13) . 第２項の積分は . ∑i=1n∫zi2exp⁡[−12∑j=1n(zj−μj)2σj2] dz=∑i=1n∫exp⁡[−(z1−μ1)22σ12] dz1×⋯×∫zi2exp⁡[−(zj−μj)22σj2] dzi×⋯×∫exp⁡[−(zn−μn)22σn2] dzn=∑i=1n(2π)n−12∏j≠iσj∫zi2exp⁡[−(zj−μj)22σj2] dzi=∑i=1n(2π)n2∏j=1nσj(σi2+μi2)(14) begin{aligned} sum _ {i=1} ^ n int z _ i ^ 2 exp left[- frac{1}{2} sum _ {j=1} ^ n frac{(z _ j- mu _ {j}) ^ 2}{ sigma _ j ^ 2} right] ,d boldsymbol{z}&amp;= sum _ {i=1} ^ n int exp left[- frac{(z _ 1- mu _ {1}) ^ 2}{2 sigma _ 1 ^ 2} right] ,dz _ 1 times &amp; qquad dots times int z _ i ^ 2 exp left[- frac{(z _ j- mu _ {j}) ^ 2}{2 sigma _ j ^ 2} right] ,dz _ i times &amp; qquad dots times int exp left[- frac{(z _ n- mu _ {n}) ^ 2}{2 sigma _ n ^ 2} right] ,dz _ n &amp;= sum _ {i=1} ^ n(2 pi) ^ { frac{n-1}{2}} prod _ {j neq i} sigma _ j int z _ i ^ 2 exp left[- frac{(z _ j- mu _ {j}) ^ 2}{2 sigma _ j ^ 2} right] ,dz _ i &amp;= sum _ {i=1} ^ n(2 pi) ^ { frac{n}{2}} prod _ {j=1} ^ n sigma _ j( sigma _ i ^ 2+ mu _ {i} ^ 2) end{aligned} tag{14}i=1∑n​∫zi2​exp[−21​j=1∑n​σj2​(zj​−μj​)2​]dz​=i=1∑n​∫exp[−2σ12​(z1​−μ1​)2​]dz1​×⋯×∫zi2​exp[−2σj2​(zj​−μj​)2​]dzi​×⋯×∫exp[−2σn2​(zn​−μn​)2​]dzn​=i=1∑n​(2π)2n−1​j​=i∏​σj​∫zi2​exp[−2σj2​(zj​−μj​)2​]dzi​=i=1∑n​(2π)2n​j=1∏n​σj​(σi2​+μi2​)​(14) . のように変形できるので， . ∫Q(z∣x;θ)log⁡P(z;θ) dz=−n2log⁡(2π)−12∑i=1n(σi2+μi2).(15) int Q( boldsymbol{z} mid boldsymbol{x}; boldsymbol{ theta}) log P( boldsymbol{z}; boldsymbol{ theta}) ,d boldsymbol{z}= - frac{n}{2} log(2 pi)- frac{1}{2} sum _ {i=1} ^ n( sigma _ i ^ 2+ mu _ {i} ^ 2). tag{15}∫Q(z∣x;θ)logP(z;θ)dz=−2n​log(2π)−21​i=1∑n​(σi2​+μi2​).(15) . 次に， . ∫Q(z∣x;θ)log⁡Q(z∣x;θ) dz=1(2π)n∣Σ∣∫exp⁡[−12∑i=1n(zi−μi)2σi2]×[−n2log⁡2π−12log⁡∣Σ∣−12∑j=1n(zj−μj)2σj2] dz=−n2log⁡2π−12log⁡∣Σ∣−12(2π)n∣Σ∣×∑i=1n∫(zi−μi)2σi2exp⁡[−∑j=1n(zj−μj)22σj2] dz(16) begin{aligned} &amp; int Q( boldsymbol{z} mid boldsymbol{x}; boldsymbol{ theta}) log Q( boldsymbol{z} mid boldsymbol{x}; boldsymbol{ theta}) ,d boldsymbol{z} &amp;= frac{1}{ sqrt{(2 pi) ^ n mid Sigma mid}} int exp left[- frac{1}{2} sum _ {i=1} ^ n frac{(z _ i- mu _ {i}) ^ 2}{ sigma _ i ^ 2} right] times left[- frac{n}{2} log 2 pi- frac{1}{2} log mid Sigma mid- frac{1}{2} sum _ {j=1} ^ n frac{(z _ j- mu _ {j}) ^ 2}{ sigma _ j ^ 2} right] ,d boldsymbol{z} &amp;=- frac{n}{2} log 2 pi- frac{1}{2} log mid Sigma mid- frac{1}{2 sqrt{(2 pi) ^ n mid Sigma mid}} times sum _ {i=1} ^ n int frac{(z _ i- mu _ {i}) ^ 2}{ sigma _ i ^ 2} exp left[- sum _ {j=1} ^ n frac{(z _ j- mu _ {j}) ^ 2}{2 sigma _ j ^ 2} right] ,d boldsymbol{z} end{aligned} tag{16}​∫Q(z∣x;θ)logQ(z∣x;θ)dz=(2π)n∣Σ∣ . ​1​∫exp[−21​i=1∑n​σi2​(zi​−μi​)2​]×[−2n​log2π−21​log∣Σ∣−21​j=1∑n​σj2​(zj​−μj​)2​]dz=−2n​log2π−21​log∣Σ∣−2(2π)n∣Σ∣ . ​1​×i=1∑n​∫σi2​(zi​−μi​)2​exp[−j=1∑n​2σj2​(zj​−μj​)2​]dz​(16) . 第３項の積分は . ∑i=1n1σi2∫zi2∏j=1nexp⁡(−zj22σj2)dz=∑i=1n1σi2∫exp⁡(−z122σ12) dz1×⋯×∫zi2exp⁡(−zi22σi2) dzi×⋯×∫exp⁡(−zn22σn2) dzn=∑i=1n1σi2(2π)n−12∏j≠iσj2πσi3=∑i=1n(2π)n2∏j=1nσj(17) begin{aligned} sum _ {i=1} ^ n frac{1}{ sigma _ i ^ 2} int z _ i ^ 2 prod _ {j=1} ^ n exp left(- frac{z _ j ^ 2}{2 sigma _ j ^ 2} right) d boldsymbol{z}&amp;= sum _ {i=1} ^ n frac{1}{ sigma _ i ^ 2} int exp left(- frac{z _ 1 ^ 2}{2 sigma _ 1 ^ 2} right) dz _ 1 times qquad&amp; dots times int z _ i ^ 2 exp left(- frac{z _ i ^ 2}{2 sigma _ i ^ 2} right) dz _ i times qquad&amp; dots times int exp left(- frac{z _ n ^ 2}{2 sigma _ n ^ 2} right) ,dz _ n &amp;= sum _ {i=1} ^ n frac{1}{ sigma _ i ^ 2}(2 pi) ^ { frac{n-1}{2}} prod _ {j neq i} sigma _ j sqrt{2 pi} sigma _ i ^ 3 &amp;= sum _ {i=1} ^ n (2 pi) ^ { frac{n}{2}} prod _ {j=1} ^ n sigma _ j end{aligned} tag{17}i=1∑n​σi2​1​∫zi2​j=1∏n​exp(−2σj2​zj2​​)dz​=i=1∑n​σi2​1​∫exp(−2σ12​z12​​) dz1​×⋯×∫zi2​exp(−2σi2​zi2​​) dzi​×⋯×∫exp(−2σn2​zn2​​)dzn​=i=1∑n​σi2​1​(2π)2n−1​j​=i∏​σj​2π . ​σi3​=i=1∑n​(2π)2n​j=1∏n​σj​​(17) . のように変形できるので， . ∫Q(z∣x;θ)log⁡Q(z∣x;θ) dz=−n2log⁡(2π)−12∑j=1n(1+log⁡σj2).(18) int Q( boldsymbol{z} mid boldsymbol{x}; boldsymbol{ theta}) log Q( boldsymbol{z} mid boldsymbol{x}; boldsymbol{ theta}) ,d boldsymbol{z}=- frac{n}{2} log(2 pi)- frac{1}{2} sum _ {j=1} ^ n(1+ log sigma _ j ^ 2). tag{18}∫Q(z∣x;θ)logQ(z∣x;θ)dz=−2n​log(2π)−21​j=1∑n​(1+logσj2​).(18) . (15)，(18)から，(3) 第２項は， . −KL(Q(zi∣xi;ϕ)∥P(zi;θ))=−∫Q(zi∣xi;θ)log⁡Q(zi∣xi;θ) dzi+∫Q(zi∣xi;θ)log⁡P(zi;θ) dzi=12∑j=1n[(1+log⁡σj2)−μj2−σj2].(19) begin{aligned} &amp;- text{KL}(Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi}) | P( boldsymbol{z} _ i; boldsymbol{ theta})) &amp;=- int Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ theta}) log Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ theta}) ,d boldsymbol{z} _ i+ int Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ theta}) log P( boldsymbol{z} _ i; boldsymbol{ theta}) ,d boldsymbol{z} _ i &amp;= frac{1}{2} sum _ {j=1} ^ n[(1+ log{ sigma _ j} ^ 2)- mu _ j ^ 2- sigma _ j ^ 2]. end{aligned} tag{19}​−KL(Q(zi​∣xi​;ϕ)∥P(zi​;θ))=−∫Q(zi​∣xi​;θ)logQ(zi​∣xi​;θ)dzi​+∫Q(zi​∣xi​;θ)logP(zi​;θ)dzi​=21​j=1∑n​[(1+logσj​2)−μj2​−σj2​].​(19) . また，今回は$Q( boldsymbol{z} _ i mid boldsymbol{x} _ i; boldsymbol{ phi})$が正規分布$ mathcal{N}( boldsymbol{z} _ i, boldsymbol{ mu}, boldsymbol{ sigma} odot boldsymbol{ sigma}E)$であるとしたので，再パラメータ化は . gϕ(ϵ,xi)=μ(xi)+σ(xi)⊙ϵ,ϵ∼N(ϵ,0,E)(20)g _ { boldsymbol{ phi}}( boldsymbol{ epsilon}, boldsymbol{x} _ i) = boldsymbol{ mu}( boldsymbol{x} _ i)+ boldsymbol{ sigma}( boldsymbol{x} _ i) odot boldsymbol{ epsilon}, quad boldsymbol{ epsilon} sim mathcal{N}( boldsymbol{ epsilon},0,E) tag{20}gϕ​(ϵ,xi​)=μ(xi​)+σ(xi​)⊙ϵ,ϵ∼N(ϵ,0,E)(20) . とする．これは， . EN(ϵ,0,E)[μ(xi)+σ(xi)⊙ϵ]=μ(xi)(21)E _ { mathcal{N}( boldsymbol{ epsilon},0,E)}[ boldsymbol{ mu}( boldsymbol{x} _ i)+ boldsymbol{ sigma}( boldsymbol{x} _ i) odot boldsymbol{ epsilon}]= boldsymbol{ mu}( boldsymbol{x} _ i) tag{21}EN(ϵ,0,E)​[μ(xi​)+σ(xi​)⊙ϵ]=μ(xi​)(21) . および . EN(ϵ,0,E)[(μi+σiϵi−μi)(μj+σjϵj−μj)]=δijσiσj(22)E _ { mathcal{N}( boldsymbol{ epsilon},0,E)}[( mu _ i+ sigma _ i epsilon _ i- mu _ i)( mu _ j+ sigma _ j epsilon _ j- mu _ j)]= delta _ {ij} sigma _ i sigma _ j tag{22}EN(ϵ,0,E)​[(μi​+σi​ϵi​−μi​)(μj​+σj​ϵj​−μj​)]=δij​σi​σj​(22) . から分かる． . 以上から， . log⁡P(xi;θ)=12∑j=1n[(1+log⁡σj2)−μj2−σj2]+1L∑l=1Llog⁡P(xi∣zi(l);θ).(23) begin{aligned} log P( boldsymbol{x} _ i; boldsymbol{ theta}) &amp;= frac{1}{2} sum _ {j=1} ^ n[(1+ log{ sigma _ j} ^ 2)- mu _ j ^ 2- sigma _ j ^ 2]+ frac{1}{L} sum _ {l=1} ^ L log P( boldsymbol{x} _ i mid boldsymbol{z} _ i ^ {(l)}; boldsymbol{ theta}). end{aligned} tag{23}logP(xi​;θ)​=21​j=1∑n​[(1+logσj​2)−μj2​−σj2​]+L1​l=1∑L​logP(xi​∣zi(l)​;θ).​(23) . ただし， . zi(l)=μ(l)(xi)+σ(l)(xi)⊙ϵ(l),ϵ(l)∼N(ϵ,0,E).(24) begin{aligned} boldsymbol{z} _ i ^ {(l)}&amp;= boldsymbol{ mu} ^ {(l)}( boldsymbol{x} _ i)+ boldsymbol{ sigma} ^ {(l)}( boldsymbol{x} _ i) odot boldsymbol{ epsilon} ^ {(l)}, boldsymbol{ epsilon} ^ {(l)}&amp; sim mathcal{N}( boldsymbol{ epsilon},0,E). end{aligned} tag{24}zi(l)​ϵ(l)​=μ(l)(xi​)+σ(l)(xi​)⊙ϵ(l),∼N(ϵ,0,E).​(24) . これが最大になるように，NNを構成すればよい． . 摂動$ delta$の予想 . 各状態にある細胞を抽出し，細胞の数によるバイアスを無くすために，調節する．最後に . δ=z1‾−z0‾(25) delta= overline{z _ 1}- overline{z _ 0} tag{25}δ=z1​​−z0​​(25) . を計算する．$ overline{z _ 0}$は各状態の潜在変数の平均，$ overline{z _ 1}$は摂動があったときの潜在変数の平均． .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/single-cell-analysis/2019/10/02/scgen.html",
            "relUrl": "/single-cell-analysis/2019/10/02/scgen.html",
            "date": " • Oct 2, 2019"
        }
        
    
  
    
        ,"post15": {
            "title": "Principal Curve 入門",
            "content": "Principal Curve（主曲線） . Principal ComponentとPrincipal Curve . $ boldsymbol{R} ^ d$内で$N$個のデータがどのように分布しているかを，比較的簡単な式で表したい（次元を減らしたい）時がある．一つのよく知られた方法は，線形回帰である．これは，線形関数$f$を用いて，$f(x _ 1, ldots, check{x} _ i, ldots,x _ d)$の式を考え，実際のデータとの差の分散 . 1N∑k=1N[xi−f(x1,…,xˇi,…,xd)]2 frac{1}{N} sum _ {k=1} ^ N[x _ i-f(x _ 1, ldots, check{x} _ i, ldots,x _ d)] ^ 2N1​k=1∑N​[xi​−f(x1​,…,xˇi​,…,xd​)]2 . が最小となるようにする．これは，$ boldsymbol{R} ^ d$内で，$x _ i$軸に沿った，データ点と直線の距離の２乗平均を最小にするのに等しい（図）． . . これに対し，主成分分析では，同じく直線を考えるが，その直線に関する成分の２乗和 . ∑k=1Nx⋅e1 sum _ {k=1} ^ N boldsymbol{x} cdot boldsymbol{e} _ 1k=1∑N​x⋅e1​ . が最大となる単位ベクトル$ boldsymbol{e} _ 1$を方向ベクトルとする直線を第１主成分とする．ここで，各データ点と直線の距離を$d( boldsymbol{x} _ k)$とすれば， . d(xk)2=∥x∥2−x⋅e1d( boldsymbol{x} _ k) ^ 2= | boldsymbol{x} | ^ 2- boldsymbol{x} cdot boldsymbol{e} _ 1d(xk​)2=∥x∥2−x⋅e1​ . なので，主成分分析は各データ点との距離の２乗平均 . 1N∑k=1Nd(xk)2 frac{1}{N} sum _ {k=1} ^ N d( boldsymbol{x} _ k) ^ 2N1​k=1∑N​d(xk​)2 . が最小となるような直線を選ぶことに等しい（図）． . . Principal Curve Analysisはデータ点を曲線で代表する方法である．各データ点とその曲線上への射影点との距離の２乗平均が最小となるような曲線を考える（図）. . . Principal Curveの定義 . $ boldsymbol{R} ^ p$内に確率密度$h$に従って分布した点を$ boldsymbol{X}$で表す．$E( boldsymbol{X})=0$としても一般性を失わない．$C ^ infty$級曲線$ boldsymbol{f}: boldsymbol{R} supset Lambda to boldsymbol{R} ^ p$を考える．ただし$ boldsymbol{f}$は自己交叉しないものとする： . λ1≠λ2⇒f(λ1)≠f(λ2). lambda _ 1 neq lambda _ 2 Rightarrow boldsymbol{f}( lambda _ 1) neq boldsymbol{f}( lambda _ 2).λ1​​=λ2​⇒f(λ1​)​=f(λ2​). . また，パラメータ$ forall lambda in Lambda$について$ boldsymbol{f}$は単位速さであるとする： . ∥f′(λ)∥=1. | boldsymbol{f}&amp;#x27;( lambda) |=1.∥f′(λ)∥=1. . $ boldsymbol{X}$の$ boldsymbol{f}$に関するprojection index $ lambda _ { boldsymbol{f}}: boldsymbol{R} ^ p to boldsymbol{R}$を次のように定義する： . λf(x)=sup⁡λ{λ∣∥x−f(λ)∥=inf⁡μ∥x−f(μ)∥}. lambda _ { boldsymbol{f}}( boldsymbol{x})= sup _ lambda left { lambda mid | boldsymbol{x}- boldsymbol{f}( lambda) |= inf _ mu | boldsymbol{x}- boldsymbol{f}( mu) | right }.λf​(x)=λsup​{λ∣∥x−f(λ)∥=μinf​∥x−f(μ)∥}. . すなわち，$ boldsymbol{x}$の$ boldsymbol{f}$に関するprojection index $ lambda _ { boldsymbol{f}}( boldsymbol{x})$は，$ boldsymbol{x}$に最も近い$ boldsymbol{f}$の点に対応するパラメータのうち最大のものである． . 定義A . 曲線$ boldsymbol{f}$をself-consistentである，もしくはprincipal curveであるとは，全ての$ lambda in Lambda$に対し， . E(X∣λf(X)=λ)=f(λ)E( boldsymbol{X} mid lambda _ { boldsymbol{f}}( boldsymbol{X})= lambda)= boldsymbol{f}( lambda)E(X∣λf​(X)=λ)=f(λ) . が成立することである(Hastie and Stuetzle, 1989)． . . つまり，projection index $ lambda _ { boldsymbol{f}}( boldsymbol{x})$によって$ lambda$に射影される$ boldsymbol{R} ^ p$の全ての点$ boldsymbol{X}$を考える．そのような$ boldsymbol{X}$の平均がちょうど$ boldsymbol{f}( lambda)$になるのである． . Principal Curveは主成分の自然な拡張である．それは次の定理によって分かる： . 定理1 . $ boldsymbol{u} _ 0 cdot boldsymbol{v} _ 0=0$とすると，直線$l( lambda)= boldsymbol{u} _ 0+ lambda boldsymbol{v} _ 0$がself-consistentならば，それは主成分である． . 証明 . projection indexの性質（最短距離を指すパラメータが複数ある場合は最大値を取る）によって，異なる$ lambda$の原像$ lambda _ { boldsymbol{f}}{} ^ {-1}( lambda)$は交わらない．よって，${ boldsymbol{X}}$は，projection indexによって$ lambda$に写る$ boldsymbol{X}$の集合の直和で表される： . {X}=⨁λ{X∣λf(X)=λ}. { boldsymbol{X} }= bigoplus _ lambda { boldsymbol{X} mid lambda _ { boldsymbol{f}}( boldsymbol{X})= lambda }.{X}=λ⨁​{X∣λf​(X)=λ}. . . よって，$ boldsymbol{X}$の平均は，ある$ lambda$にprojection indexによって写される$ boldsymbol{X}$の平均$ boldsymbol{X} _ lambda=E( boldsymbol{X} mid lambda _ { boldsymbol{f}}( boldsymbol{X})= lambda)$の$ lambda$に関する平均に等しい： . 0=E(X)=EλE(X∣λf(X)=λ)=Eλ(u0+λv0)=u0+λˉv0. begin{aligned} 0=E( boldsymbol{X}) &amp;= E _ lambda E( boldsymbol{X} mid lambda _ { boldsymbol{f}}( boldsymbol{X})= lambda) &amp;= E _ lambda( boldsymbol{u} _ 0+ lambda boldsymbol{v} _ 0) &amp;= boldsymbol{u} _ 0+ bar{ lambda} boldsymbol{v} _ 0. end{aligned}0=E(X)​=Eλ​E(X∣λf​(X)=λ)=Eλ​(u0​+λv0​)=u0​+λˉv0​.​ . 両辺$ boldsymbol{u} _ 0$で内積を取れば，$ boldsymbol{u} _ 0 cdot boldsymbol{v} _ 0=0$なので . u0=0. boldsymbol{u} _ 0=0.u0​=0. . projection indexによって$ lambda$に写る$ boldsymbol{X}$について$ boldsymbol{u} _ 0$，つまり$l$は原点を通るので， . λf(X)=X⋅v0=Xtv0. lambda _ { boldsymbol{f}}( boldsymbol{X}) = boldsymbol{X} cdot boldsymbol{v} _ 0 = boldsymbol{X} ^ t boldsymbol{v} _ 0.λf​(X)=X⋅v0​=Xtv0​. . $ boldsymbol{X}$の共分散行列を$ Sigma$とすれば， . Σ=E[(X−E(X))(X−E(X))t]=E(XXt) Sigma=E left[( boldsymbol{X}-E( boldsymbol{X}))( boldsymbol{X}-E( boldsymbol{X})) ^ t right]=E( boldsymbol{X} boldsymbol{X} ^ t)Σ=E[(X−E(X))(X−E(X))t]=E(XXt) . となるので， . Σv0=E(XXt)v0=EλE(XXtv0∣λf(X)=λ)=EλE(XXtv0∣Xtv0=λ)=EλE(λX∣Xtv0=λ)=Eλλ2v0. begin{aligned} Sigma boldsymbol{v} _ 0 &amp;= E( boldsymbol{X} boldsymbol{X} ^ t) boldsymbol{v} _ 0 &amp;= E _ lambda E( boldsymbol{X} boldsymbol{X} ^ t boldsymbol{v} _ 0 mid lambda _ { boldsymbol{f}}( boldsymbol{X})= lambda) &amp;= E _ lambda E( boldsymbol{X} boldsymbol{X} ^ t boldsymbol{v} _ 0 mid boldsymbol{X} ^ t boldsymbol{v} _ 0= lambda) &amp;= E _ lambda E( lambda boldsymbol{X} mid boldsymbol{X} ^ t boldsymbol{v} _ 0= lambda) &amp;= E _ lambda lambda ^ 2 boldsymbol{v} _ 0. end{aligned}Σv0​​=E(XXt)v0​=Eλ​E(XXtv0​∣λf​(X)=λ)=Eλ​E(XXtv0​∣Xtv0​=λ)=Eλ​E(λX∣Xtv0​=λ)=Eλ​λ2v0​.​ . projection indexの存在 . 補題2.1 . 全ての$ boldsymbol{x} in boldsymbol{R} ^ p$と$r&gt;0$に対し，集合$Q{ lambda mid| boldsymbol{x}- boldsymbol{f}( lambda)| leq r}$はコンパクトである． . 証明 . $Q$が$ boldsymbol{R} ^ {p}$の有界な閉集合であることが言えれば良い． . $| boldsymbol{x}- boldsymbol{f}( lambda)|$は$ lambda$の連続関数で，値域が閉球体$B ^ ast(0,r)$であるので，その原像$Q$も閉集合である． . $Q$が有界でないと仮定する．$| boldsymbol{x}- boldsymbol{f}( lambda _ i)| leq r$を満たす点列$ lambda _ 1, ldots$が存在する．$B ^ ast( boldsymbol{x},2r)$を考える．$ boldsymbol{f}$は$ lambda _ i$から$ lambda _ {i+1}$の間で，ずっと$B ^ ast( boldsymbol{x},2r)$の中にあるか，一度$B ^ ast( boldsymbol{x},2r)$を出た後，再び入ってくる．何れにせよ，$ boldsymbol{f}$は単位速さであったから，$ boldsymbol{f}( lambda _ i)$から$ boldsymbol{f}( lambda _ i{i+1})$までの曲線の長さは少なくとも$ min(2r, lambda _ {i+1}- lambda _ i)$である．よって，仮定により${ lambda _ n} _ {n in boldsymbol{N}}$が$B( boldsymbol{x},2r)$内で無限長を持つことになり，矛盾である． . 補題2.2 . 全ての$ boldsymbol{x} in boldsymbol{R} ^ p$に対し，$| boldsymbol{x}- boldsymbol{f}( lambda)|= inf _ { mu in Lambda}| boldsymbol{x}- boldsymbol{f}( mu)|$なる$ lambda in Lambda$が存在する． . 証明 . $r= inf _ { mu in Lambda}| boldsymbol{x}- boldsymbol{f}( mu)|$，$B={ mu mid| boldsymbol{x}- boldsymbol{f}( mu)| leq2r}$とする．$B$は空でなく，コンパクトなので，最大値・最小値が存在する．よって，$ inf _ { mu in Lambda}| boldsymbol{x}- boldsymbol{f}( mu)|= inf _ { mu in B}| boldsymbol{x}- boldsymbol{f}( mu)|$が成立する． . . $d( boldsymbol{x}, boldsymbol{f})= inf _ { mu in Lambda}| boldsymbol{x}- boldsymbol{f}( mu)|$とする． . 定理2 . projection index $ lambda _ { boldsymbol{f}}( boldsymbol{x})= sup _ lambda{ lambda mid| boldsymbol{x}- boldsymbol{f}( lambda)|=d( boldsymbol{x}, boldsymbol{f})}$はwell-definedである． . 証明 . ${ lambda mid| boldsymbol{x}- boldsymbol{f}( lambda)|=d( boldsymbol{x}, boldsymbol{f})}$は補題 ref{existence _ of _ parameter _ set}から空でなく，補題 ref{compact _ Q}からコンパクトと分かる． . ambiguity pointの集合の測度 . 補題3.1 . $ boldsymbol{x}$の最近点が$ boldsymbol{f}( lambda _ 0) ( lambda _ 0 in Lambda _ 0)$（ただし，$ lambda _ 0$はパラメータ範囲$ Lambda _ 0$の端点ではないとする）なら，$ boldsymbol{x}$は超平面$( boldsymbol{x}- boldsymbol{f}( lambda _ 0)) cdot boldsymbol{f}’( lambda _ 0)$上にある . 証明 . $ boldsymbol{f}( lambda _ 0)$は$ boldsymbol{x}$からの距離が最短なので， . 0=ddλ∥x−f(λ)∥2∣λ0=−2(x−f(λ0))⋅f′(λ0). begin{aligned} 0 &amp;= frac{d}{d lambda} | boldsymbol{x}- boldsymbol{f}( lambda) | ^ 2 mid _ { lambda _ 0} &amp;= -2( boldsymbol{x}- boldsymbol{f}( lambda _ 0)) cdot boldsymbol{f}&amp;#x27;( lambda _ 0). end{aligned}0​=dλd​∥x−f(λ)∥2∣λ0​​=−2(x−f(λ0​))⋅f′(λ0​).​ . 定義B . 最近点が複数ある（$ text{card}{ lambda mid| boldsymbol{x}- boldsymbol{f}( lambda)|=d( boldsymbol{x}, boldsymbol{f})}&gt;1$である）点$ boldsymbol{x}$をambiguity pointと呼ぶ． . . $M _ lambda={ boldsymbol{x} mid( boldsymbol{x}- boldsymbol{f}( lambda)) cdot boldsymbol{f}’( lambda)=0}$とする．補題 ref{hyperplane}から，$ boldsymbol{f}( lambda)$が$ boldsymbol{x}$の最近点で$ lambda in Lambda _ 0$なら$ boldsymbol{x} in M _ lambda$である． . 全ての$ lambda$について，$ boldsymbol{f}’( lambda)$と直交するよう$p-1$個のベクトル場$ boldsymbol{n} _ 1( lambda), ldots, boldsymbol{n} _ {p-1}( lambda)$を考える．$ boldsymbol{ chi}: Lambda times boldsymbol{R} ^ {p-1} to boldsymbol{R} ^ p$を次の様に定める： . χ(λ,v)=f(λ)+∑i=1p−1vini(λ). boldsymbol{ chi}( lambda, boldsymbol{v})= boldsymbol{f}( lambda)+ sum _ {i=1} ^ {p-1}v _ i boldsymbol{n} _ i( lambda).χ(λ,v)=f(λ)+i=1∑p−1​vi​ni​(λ). . さらに，$M= boldsymbol{ chi}( Lambda, boldsymbol{R} ^ {p-1})= bigcup _ lambda M _ lambda$を，曲線上の点で，曲線と直交する超平面に属する点の集合とする． . 定義C . $X$の部分集合族$ mathcal{F} in2 ^ X$が$ sigma$加法族であるとは，次の性質を満たすことである： . $A in mathcal{F} Rightarrow A ^ c in mathcal{F}$； | $n in boldsymbol{N}$に対し，$A _ n in mathcal{F} Rightarrow bigcup _ {n in boldsymbol{N}}A _ n in mathcal{F}$． | . $ sigma$加法族の定義から直ちに次のことが証明される： . $ phi, X in mathcal{F}$． | $n in boldsymbol{N}$に対し，$A _ n in mathcal{F} Rightarrow bigcap _ {n in boldsymbol{N}}A _ n in mathcal{F}$． | $A,B Rightarrow A cup B in mathcal{F}$． | $A,B Rightarrow A cap B in mathcal{F}$． | 定義D . $ mathcal{F}$を$X$の$ sigma$加法族とする．$ mu: mathcal{F} to boldsymbol{R} cup{+ infty}$が測度であるとは次の性質を満たすことである： . $A in mathcal{F}$に対し$0 leq mu(A) leq+ infty$； | $A _ i,A _ j in mathcal{F} (i neq j in boldsymbol{N})$に対し，$A _ i cap A _ j= varnothing Rightarrow mu( bigcup _ {i in boldsymbol{N}}A _ i)= sum _ {i=1} ^ infty mu(A _ i)$． | . 測度の定義から直ちに次のことが証明される： . $ mu( varnothing)=0$． | $A,B in mathcal{F}, quad A subset B Rightarrow mu(A) leq mu(B)$． | $A,B in mathcal{F}, quad A subset B, quad mu(B)&lt;+ infty Rightarrow mu(B backslash A)= mu(B)- mu(A)$． | $A _ i in mathcal{F} (i in boldsymbol{N})$に対し，$ mu( bigcup _ {i in boldsymbol{N}}A _ i) leq sum _ {i=1} ^ infty mu(A _ i)$． | 補題3.2 . $M$に含まれないambiguity pointの（$p$次元）測度は$0$である：$ mu(A cap M ^ c)$． . 証明 . $ boldsymbol{x} in A cap M ^ c$とする．補題 ref{hyperplane}から，このような点$ boldsymbol{x}$が存在するのは，$ Lambda=[ lambda _ text{min}, lambda _ text{max}]$であり，$ boldsymbol{x}$が端点$ boldsymbol{f}( lambda _ text{min})$, $ boldsymbol{f}( lambda _ text{max})$から等距離にあり，かつそれが曲線$ boldsymbol{f}$との最短である時のみである．これは測度$0$の超平面を形成する． . 補題3.3 . $E$を測度$0$の集合とする．ambiguity pointの集合$A$の測度が$0$であるためには，$ forall boldsymbol{x} in boldsymbol{R} ^ p backslash E$に対し，$ mu(A cap N( boldsymbol{x}))$なる開近傍$N( boldsymbol{x})$が存在することが十分である． . 証明 . 開被覆${N( boldsymbol{x}) mid boldsymbol{x} in boldsymbol{R} ^ p backslash E}$は明らかに$ bar{A}$を被覆する．$ bar{A}$は$ boldsymbol{R} ^ p$の有界閉集合なのでコンパクトである．よって，${N( boldsymbol{x}) mid boldsymbol{x} in boldsymbol{R} ^ p backslash E}$の中から有限個の被覆${N _ i}$を選んで，$A subset bar{A} subset bigcup _ {i=1} ^ k N _ i$とできる．よって，$ mu(A) leq mu( bigcup _ {i=1} ^ kN _ i cap A) leq sum _ {i=1} ^ {k} mu(N _ i cap A)=0$． . 補題3.4 . $ Lambda$がコンパクトな場合のみ考えても一般性を失わない． . 証明 . $ Lambda _ n=[-n,n]$, $ boldsymbol{f} _ n= boldsymbol{f} mid Lambda _ n$とし，$A _ n$を$ boldsymbol{f} _ n$のambiguity pointとする．$ boldsymbol{x}$を$ boldsymbol{f}$のambiguity pointとする．補題2.1から${ lambda mid| boldsymbol{x}- boldsymbol{f}( lambda)|=d( boldsymbol{x}, boldsymbol{f})}$はコンパクト，つまり$ boldsymbol{R}$の有限閉集合である． . よって，ある$n$が存在し，${ lambda} in Lambda _ n$，$ boldsymbol{x} in A _ n$，$A subset bigcup _ {n=1} ^ infty A _ n$が成立する．ここで，$ mu(A) leq mu( bigcup _ {n=1} ^ infty A _ n) leq sum _ {n=1} ^ infty mu(A _ i)$となる． . コンパクトな$ Lambda _ n$を考えて，$ boldsymbol{f} _ n$のambiguity pointの集合$A _ n$について，$ mu(A _ n)=0$を示せばよい． . 定義E . 写像$f: boldsymbol{R} ^ m to boldsymbol{R} ^ n$について，$ boldsymbol{y} in boldsymbol{R} ^ n$が正則値であるとは，$ forall boldsymbol{x} in f ^ {-1}( boldsymbol{y})$について，$f$の微分$f’$の階数が$p$となることである：$ text{rank}(f’( boldsymbol{x}))=p$．そうでなければ，$ boldsymbol{y}$は臨界値であると言う． . Sardの定理 . 写像$f: boldsymbol{R} ^ m to boldsymbol{R} ^ n$について$f$の微分可能性が十分高ければ，$f$の臨界値の集合$C$の測度は$0$である． . 補題3.5 . 定義Bで定義した$ boldsymbol{ chi}$の臨界値の集合$C$の測度は$0$である． . 証明 . $ boldsymbol{ chi}$は$C ^ infty$級なので，Sardの定理を使う． . 定理3 . ambiguity pointの集合$A$の測度は$0$である：$ mu(A)=0$． . 証明 . 補題3.4から$ Lambda$がコンパクトの場合のみ考える．補題3.2から$ mu(A cap M ^ c)=0$．$ boldsymbol{ chi}$の臨界値の集合$C subset M$を考える．この時，$ mu((A cap M ^ c) cup C)=0$なので，補題 ref{measure _ of _ A _ Mc}から，$ forall boldsymbol{x} in boldsymbol{R} ^ p backslash((A cap M ^ c) cup C)=M backslash C+M ^ c cap A ^ e$について，$ mu(A cap N( boldsymbol{x}))=0$なる近傍$N( boldsymbol{x})$が存在することを証明すればよい． . $A$の外部$A ^ e$については自明なので，以下では$ forall boldsymbol{x} in M backslash C$（正則値）について，$ mu(A cap N( boldsymbol{x}))=0$なる近傍$N( boldsymbol{x})$が存在することを証明する． . $ boldsymbol{ chi} ^ {-1} subset Lambda times boldsymbol{R} ^ {p-1}$は有限集合${( lambda _ 1, boldsymbol{v _ 1}), cdots,( lambda _ k, boldsymbol{v} _ k)}$である． . 仮に，無限集合であったと仮定する．この時，$ boldsymbol{ chi}( xi _ i, boldsymbol{w} _ i)= boldsymbol{x}$を満たす$ Lambda times boldsymbol{R} ^ {p-1}$の部分集合${( xi _ 1, boldsymbol{w} _ 1), ldots}$が存在する．$ Lambda$はコンパクト，$ boldsymbol{ chi}$は連続なので，${ xi _ 1, ldots}$の集積点$ xi _ 0$と対応する$ boldsymbol{w} _ 0$が存在し，$ boldsymbol{ chi}( xi _ 0, boldsymbol{w} _ 0)= boldsymbol{x}$となる．$ boldsymbol{x}$は正則値なので，$ text{rank}( boldsymbol{ chi}’)=p$である．よって，$( xi _ 0, boldsymbol{w} _ 0)$と$ boldsymbol{x}$の近傍で（局所）微分同相になる．よって，これは$ xi _ 0$が集積点であるので矛盾． . $ boldsymbol{ chi}$は正則値なので，$( lambda _ i, boldsymbol{v} _ i)$の近傍$L _ i$と$ boldsymbol{x}$の近傍$N( boldsymbol{x})$で微分同相になる． . この時，$ tilde{N}( boldsymbol{x}) subset N( boldsymbol{x})$が存在して，$ boldsymbol{ chi} ^ {-1}( tilde{N}) subset bigcup _ {i=1} ^ kL _ i$が成立する． . 仮に上記のような$ tilde{N}$が存在しないと仮定する．この時，$ boldsymbol{x}$に収束する点列${ boldsymbol{x} _ i}$が存在し，$( xi _ i, boldsymbol{w} _ i) notin bigcup _ {i=1} ^ kL _ i$で，$ boldsymbol{ chi}( xi _ i, boldsymbol{w} _ i)= boldsymbol{x} _ i$が成立する．点列${ xi _ i} _ {i in boldsymbol{N}}$は集積点$ xi _ 0 in bigcup _ {i in boldsymbol{N}}L _ i$を持つ．しかし，これは$ boldsymbol{ chi}( xi _ 0, boldsymbol{w} _ 0)= boldsymbol{x}$の連続性及び，$ boldsymbol{ chi} ^ {-1}( boldsymbol{x})$の有限性から矛盾となる． . 以上から，$ boldsymbol{y} in tilde{N}( boldsymbol{x})$に対し，$ boldsymbol{ chi}( lambda _ i( boldsymbol{y}), boldsymbol{v} _ i( boldsymbol{y}))= boldsymbol{y}$を満たす$( lambda _ i( boldsymbol{y}), boldsymbol{v} _ i( boldsymbol{y}))$が各$L _ i$に唯一存在する．ここで，$d _ i( boldsymbol{y})=| boldsymbol{y}- boldsymbol{f}( lambda _ i( boldsymbol{y}))| ^ 2$とする．補題3.1から， . grad(di(y))=grad∑j=1p[yj−fj(λi(y))]2=(∂∂y1∑j=1p[yj−fj(λi(y))]2,…)=∑j=1p(2[yj−fj(λi(y))]∂∂y1[yj−fj(λi(y))],…)=(∑j=1p2[yj−fj(λi(y))]δ1j,…)−(∑j=1p2[yj−fj(λi(y))]fj′(λi(y))∂λi∂y1,…)=(2[y1−f1(λi(y))],…)−2∂λi∂y1([y−f(λi(y))]⋅f′(λi(y)))=2(y−f(λi(y))). begin{aligned} text{grad}(d _ i( boldsymbol{y})) &amp;= text{grad} sum _ {j=1} ^ p[y _ j-f _ j( lambda _ i( boldsymbol{y}))] ^ 2 &amp;= left( frac{ partial}{ partial y _ 1} sum _ {j=1} ^ p[y _ j-f _ j( lambda _ i( boldsymbol{y}))] ^ 2, ldots right) &amp;= sum _ {j=1} ^ p left(2[y _ j-f _ j( lambda _ i( boldsymbol{y}))] frac{ partial}{ partial y _ 1}[y _ j-f _ j( lambda _ i( boldsymbol{y}))], ldots right) &amp;= left( sum _ {j=1} ^ p2[y _ j-f _ j( lambda _ i( boldsymbol{y}))] delta _ {1j}, ldots right) - left( sum _ {j=1} ^ p2[y _ j-f _ j( lambda _ i( boldsymbol{y}))]f ^ prime _ j( lambda _ i( boldsymbol{y})) frac{ partial lambda _ i}{ partial y _ 1}, ldots right) &amp;= left(2[y _ 1-f _ 1( lambda _ i( boldsymbol{y}))], ldots right)-2 frac{ partial lambda _ i}{ partial y _ 1} left([ boldsymbol{y}- boldsymbol{f}( lambda _ i( boldsymbol{y}))] cdot boldsymbol{f}&amp;#x27;( lambda _ i( boldsymbol{y})) right) &amp;= 2( boldsymbol{y}- boldsymbol{f}( lambda _ i( boldsymbol{y}))). end{aligned}grad(di​(y))​=gradj=1∑p​[yj​−fj​(λi​(y))]2=(∂y1​∂​j=1∑p​[yj​−fj​(λi​(y))]2,…)=j=1∑p​(2[yj​−fj​(λi​(y))]∂y1​∂​[yj​−fj​(λi​(y))],…)=(j=1∑p​2[yj​−fj​(λi​(y))]δ1j​,…)−(j=1∑p​2[yj​−fj​(λi​(y))]fj′​(λi​(y))∂y1​∂λi​​,…)=(2[y1​−f1​(λi​(y))],…)−2∂y1​∂λi​​([y−f(λi​(y))]⋅f′(λi​(y)))=2(y−f(λi​(y))).​ . $ boldsymbol{y} in tilde{N}( boldsymbol{x})$がambiguity pointになるのは， . Aij={z∈N~(x)∣di(z)=dj(z),λi(z)≠λj(z)}A _ {ij} = { boldsymbol{z} in tilde{N}( boldsymbol{x}) mid d _ i( boldsymbol{z})=d _ j( boldsymbol{z}), lambda _ i( boldsymbol{z}) neq lambda _ j( boldsymbol{z}) }Aij​={z∈N~(x)∣di​(z)=dj​(z),λi​(z)​=λj​(z)} . として，$ boldsymbol{y} in A _ {ij} ( exists i,j;i neq j)$の時のみである． . $ boldsymbol{f}$は自己交叉しない（$ eqref{no-self _ crossing$}が成立する）ので，$ lambda _ i( boldsymbol{z}) neq lambda _ j( boldsymbol{z})$ とすると，$ eqref{grad _ d}$から， . grad(di(z)−dj(z))=2(f(λj(z))−f(λi(z)))≠0. begin{aligned} text{grad}(d _ i( boldsymbol{z})-d _ j( boldsymbol{z})) &amp;= 2( boldsymbol{f}( lambda _ j( boldsymbol{z}))- boldsymbol{f}( lambda _ i( boldsymbol{z}))) &amp; neq0. end{aligned}grad(di​(z)−dj​(z))​=2(f(λj​(z))−f(λi​(z)))​=0.​ . $A _ {ij}$は$p-1$次元多様体であるので，その$p$次元測度は$0$となる：$ mu(A _ {ij})=0$．よって，$ mu(A cap tilde{N})= mu( bigcup _ {ij}A _ {ij}) leq sum _ {ij} mu(A _ {ij})=0$． . bendingアルゴリズム . Principal curveは次のアルゴリズムによって求めることが出来る： . . initializaiton . $ boldsymbol{a}$を第１主成分として，$ boldsymbol{f} ^ {0}( lambda)= bar{ boldsymbol{x}}+ boldsymbol{a} lambda$とする．$ lambda ^ {(0)}( boldsymbol{x})= lambda _ { boldsymbol{f} ^ {(0)}}( boldsymbol{x})=0$とする． . repeat . $ boldsymbol{f} ^ {(j)}( cdot)= boldsymbol{E}( boldsymbol{X} mid lambda _ { boldsymbol{f} ^ {(j-1)}}= cdot)$とする | $ boldsymbol{f} ^ {(j)}( lambda)$が単位速さとなるように調整する（曲線の形は変わらない）． | $ lambda ^ {(j)}( boldsymbol{x})= lambda _ { boldsymbol{f} ^ {(j)}}( boldsymbol{x}) quad forall boldsymbol{x} in h$とする． | $D ^ 2(h, boldsymbol{f} ^ {(j)})=E _ lambda E(| boldsymbol{X}- boldsymbol{f} ^ {(j)}( lambda ^ {(j)}( boldsymbol{X}))| ^ 2 mid lambda ^ {(j)}( boldsymbol{X})= lambda)$を計算する． | until $D ^ 2(h, boldsymbol{f} ^ {(j)})$がある閾値以下になるまで繰り返す． . . これはつまり，点${ boldsymbol{X}}$と曲線の距離の２乗平均が極小値となる曲線を探している． . 有限データ点のためのbendingアルゴリズム . $p$次元の$n$個のデータ点を考える．点が有限の場合，principal curveは$( lambda _ i, boldsymbol{f} _ i)$の$n$個の集合になる．曲線は単位速さであるとするので，$ lambda _ i$は$ boldsymbol{f} _ 1$から$ boldsymbol{f} _ i$までの折れ線に沿った距離とする．$ lambda _ 1=0$としておく． . まずは，projection index $ lambda _ { boldsymbol{f} ^ {(j)}}( boldsymbol{x} _ i)$を求める．$ boldsymbol{x} _ i$と，$ boldsymbol{f} _ k{} ^ {(j)}$と$ boldsymbol{f} _ {k+1}{} ^ {(j)}$を両端とする線分の最短距離を$d _ {ik}$とする．また，$ boldsymbol{f} _ 1{} ^ {(j)}$から最短距離を与える線分側の点までの折れ線に沿った距離を$ lambda _ {ik}$とする．こうして，$1 leq k leq n-1$に対し$d _ {ik}$と$ lambda _ {ik}$を求める． . . 複数データ点のためのprojection index . $ boldsymbol{x} _ i$のprojection index $ lambda _ {i}$は$d _ {ik}$の最小値を与える$ lambda _ {ik}$とする： . λi=λik∗,k∗=argmin⁡1≤k≤n−1dik. lambda _ {i}= lambda _ {ik*}, quad k*= text{arg} min _ {1 leq k leq n-1}d _ {ik}.λi​=λik∗​,k∗=arg1≤k≤n−1min​dik​. . 次に，projection indexが$ lambda _ i$になるような点${ boldsymbol{x}}$の平均を求めて，$ boldsymbol{f} ^ {(j+1)}( lambda _ i)$（つまり$ boldsymbol{f} _ i{} ^ {(j+1)}$）を構成する．しかし，有限データ点の場合は$ lambda _ i$に投影されるのは$ boldsymbol{x} _ i$唯一という状況がほとんどである．よって，projection index $ lambda _ k$が$ lambda _ i$に近い点${ boldsymbol{x} _ k}$を取ってきて，それらの局所平均を取る(linear weighted running-line smoother)． . $ lambda _ i$に近い$wn (0&lt;w&lt;1)$個の$ lambda _ j$及び点${ boldsymbol{x} _ j}$に対し直線を，重み付け最小２乗法でフィッティングする．この際，各重み付けは$ lambda _ i$で最大値を取って，$ lambda _ i$との差が大きいほど$0$に近いもの，例えば， . wij=[1−∣λj−λiλwn th nearest−λi∣3]3w _ {ij} = left[1- mid frac{ lambda _ j- lambda _ i}{ lambda _ {wn text{ th nearest}}- lambda _ i} mid ^ 3 right] ^ 3wij​=[1−∣λwn th nearest​−λi​λj​−λi​​∣3]3 . などとする．${ boldsymbol{x} _ j}$の平均は，この直線$ boldsymbol{l}( lambda)$の$ lambda _ i$での値とする： . fi(j+1)=l(λi). boldsymbol{f} _ i{} ^ {(j+1)} = boldsymbol{l}( lambda _ i).fi​(j+1)=l(λi​). . これによって$n$個の点${ boldsymbol{f} _ i}$が得られる．曲線の長さに従って${ lambda _ {i}{} ^ {(j+1)}}$を定めておく． . k-segmentsアルゴリズム . 上に述べたbendingアルゴリズムの他にもPrincipal Curveを求めるアルゴリズムはいくつかあるが，データの曲率が大きかったり，自己交叉があると使い物にならなくなる．それに対処出来るのが，k-segmentsアルゴリズムである(Verbeek, Vlassis, Kr&quot;{o}se, 2001)． . 直線$s={ boldsymbol{s}(t)= boldsymbol{c}+ boldsymbol{u}t mid t in boldsymbol{R}}$を考える．点$ boldsymbol{x}$との距離は d(x,s)=inf⁡t∈R∥s(t)−x∥d( boldsymbol{x},s)= inf _ {t in boldsymbol{R}} | boldsymbol{s}(t)- boldsymbol{x} |d(x,s)=inft∈R​∥s(t)−x∥ で定義される． . 定義F . $X _ n$を$ boldsymbol{R} ^ d$から取ってきた$n$個のサンプルの集合とする時，Voronoi領域(VR) $V _ 1, dots,V _ k$を次のように定義する： . Vi={x∈Xn∣i=argmin⁡jd(x,sj)}.V _ i = { boldsymbol{x} in X _ n mid i= text{arg} min _ j d( boldsymbol{x},s _ j) }.Vi​={x∈Xn​∣i=argjmin​d(x,sj​)}. . . つまり，$V _ i$は$i$番目の線$s _ i$が最短となるような$X _ n$の部分集合である．アルゴリズムの目標は全ての点の最短直線との距離の２乗和 . ∑i=1k∑x∈Vid(x,si)2 sum _ {i=1} ^ k sum _ { boldsymbol{x} in V _ i}d( boldsymbol{x},s _ i) ^ 2i=1∑k​x∈Vi​∑​d(x,si​)2 . を最小にするような$k$本の直線$s _ 1, ldots,s _ k$を見つけることである． . $ eqref{kmeans _ square _ sum}$を最小にするような$k$本の直線を見つけるためには，ランダムな向きと位置にある$k$本の直線を用意して，次のステップを繰り返せば良い： . それぞれの直線についてVRを決定する． | 直線をそれぞれのVRの第１主成分のベクトルで置き換える． | このアルゴリズムが収束することは次のことから分かる： . $ eqref{kmeans _ square _ sum}$はVRの定義から，第１ステップで減少する．さらに，主成分の性質から，第２ステップでも減少する． | $X _ n$は有限個なので，VRの構成方法は有限である． | . ここで，無限に長い直線ではアルゴリズムの計算量が増えるので，線分に限定する．すなわち，アルゴリズムを次のように変える： . それぞれの線分についてVRを決定する． | 線分をそれぞれのVRの第１主成分のベクトルで置き換える．そして，VRのデータ点の第１主成分の分散を$ sigma ^ 2$として，VRの重心から$ frac{3}{2} sigma$以内の範囲に存在する部分を新しい線分とする． | $k$の決定 . $k$を決めるには，$k=1$から初めて，ある条件（線分の上限数やパフォーマンスについての条件など）が満たされるまで増やしていけば良い． . 新しい線分の挿入場所を決めるために，各データ点$ boldsymbol{x} _ i$の所に長さ$0$の線分（つまり点$ boldsymbol{c}$）を追加する．この場合，点$ boldsymbol{x}$との最短距離は$d( boldsymbol{x},s)=| boldsymbol{x}- boldsymbol{c}|$で与えられる．よって，この$0$長線分も追加した$k+1$本の成分でVRを構成し，$ eqref{kmeans _ square _ sum}$を計算する． . 新しく追加した$0$長線分の中で，$ eqref{kmeans _ square _ sum}$を最も減らすような線分に関するVRを$V _ {k+1}$とする．次に，$V _ {k+1}$での第１主成分のベクトルから，平均からの距離$ frac{3}{2} sigma$までの線分を作る．これによって$k+1$本の線分が得られた． . 有限個の点の集合$S subset boldsymbol{R} ^ d$について，平均$ boldsymbol{m}$は２乗距離を最小にする： . m=argmin⁡μ∈Rd∑x∈S∥x−μ∥. boldsymbol{m}= text{arg} min _ { mu in boldsymbol{R} ^ d} sum _ { boldsymbol{x} in S} | boldsymbol{x}- mu |.m=argμ∈Rdmin​x∈S∑​∥x−μ∥. . よって，任意の$i$に対し . ∑x∈S∥x−m∥2≤∑x∈S∥x−xi∥2. sum _ { boldsymbol{x} in S} | boldsymbol{x}- boldsymbol{m} | ^ 2 leq sum _ { boldsymbol{x} in S} | boldsymbol{x}- boldsymbol{x} _ i | ^ 2.x∈S∑​∥x−m∥2≤x∈S∑​∥x−xi​∥2. . 新しい線分$s$は$ boldsymbol{m}$を含むので，$S$内の点から線分への２乗距離和は，$S$内の点から$ boldsymbol{m}$への２乗距離和以下である： . ∑x∈Sd(x,s)2≤∑x∈S∥x−m∥2. sum _ { boldsymbol{x} in S}d( boldsymbol{x},s) ^ 2 leq sum _ { boldsymbol{x} in S} | boldsymbol{x}- boldsymbol{m} | ^ 2.x∈S∑​d(x,s)2≤x∈S∑​∥x−m∥2. . よって，線分の挿入による$ eqref{kmeans _ square _ sum}$の減少には下限が存在することが分かる（$V _ {k+1}$を決めた時点で，$ eqref{kmeans _ square _ sum}$の減少は確約されており，以上の式で$S$を$V _ {k+1}$とすれば，そこから更に減少することが言える）． . 新しい線分の場所の効率的な探し方 . あらかじめ$n$個のデータ間の２乗距離を表す$n times n$行列$D$を考える： . Dij=∥xi−xj∥2.D _ {ij}= | boldsymbol{x} _ i- boldsymbol{x} _ j | ^ 2.Dij​=∥xi​−xj​∥2. . 各点$ boldsymbol{x} _ i$に対し，最も近い線分までの２乗距離$d _ i ^ text{VR}$を求めておく．更に . DVR=(d1VRd2VR⋮dnVR),VD=[DVR,…,DVR] begin{aligned} boldsymbol{D} ^ text{VR}= begin{pmatrix} d _ 1 ^ text{VR} d _ 2 ^ text{VR} vdots d _ n ^ text{VR} end{pmatrix}, quad mathit{VD}=[ boldsymbol{D} ^ text{VR}, ldots, boldsymbol{D} ^ text{VR}] end{aligned}DVR=⎝⎜⎜⎜⎜⎛​d1VR​d2VR​⋮dnVR​​⎠⎟⎟⎟⎟⎞​,VD=[DVR,…,DVR]​ . とおく．ここで， . Gij=max⁡(VDij−Dij,0)G _ {ij}= max( mathit{VD} _ {ij}-D _ {ij},0)Gij​=max(VDij​−Dij​,0) . とする．この時$G _ {ij}$は，$0$長線分を$ boldsymbol{x} _ j$に挿入した時の$ boldsymbol{x} _ i$に関する２乗距離の減少に等しい．よって$ eqref{kmeans _ square _ sum}$の減少は$ sum _ iG _ {ij}$に等しいので， . I=(1,…,1) boldsymbol{I}=(1, ldots,1)I=(1,…,1) . を考えれば，$0$長線分を$ boldsymbol{x} _ j$に挿入した時の$ eqref{kmeans _ square _ sum}$の減少は$ boldsymbol{I}G$の第$i$成分に等しい． . 線分の結合 . グラフ$G=(V,E)$を考える．ただし，$V$は$k$本の線分の$2k$個の端点である．更に，$k$本の線分と対応する辺全てを含むような$A subset E$を考える．全ての頂点を１度ずつつ通過する経路をハミルトニアン経路(HP)と呼ぶ．HPは辺の集合$P subset E$と考えることができる．線分を結合して多角形のPrincipal Curveを作るために，コストを最小にするようなHP $A subset P subset E$（線分に対応する辺を全て含み，かつ全頂点を一度だけ通る経路）を求めたい．経路$P$のコストは . l(P)+λa(P)l(P)+ lambda a(P)l(P)+λa(P) . とする．$l(P)$は経路の長さの総和である．辺$e=(v _ i,v _ j)$の長さは . l(e)=∥vi−vj∥l(e)= |v _ i-v _ j |l(e)=∥vi​−vj​∥ . とする．$a(P)$は隣接する角度の和である．$ lambda$を調節することによって，辺の向きが変わることに対するペナルティの重み付けを調節する． . HPを作るためgreedy algorithmで考える．sub-HP $P _ i$と$P _ j$をそれぞれの頂点$v _ i$，$v _ j$で結ぶとする．$e=(v _ i,v _ j)$として，新しくできるsub-HPのコストは元のsub-HPのコストの和と$l(e)+ lambda a(e)$の合計になる．全ての辺$e in(E backslash A)$に対し，コスト$c(e)=l(e)+ lambda a(e)$を計算しておく．アルゴリズムは以下のようになる： . k個のsub-HP $A$から始める． | ２個以上のsub-HPがある限り続ける． | $i neq j$として，２個のsub-HP $P _ i$と$P _ j$を$c(e)$が最小となるような辺$e in(E-A)$で結ぶ． これによって，折れ線が得られる． | 目的関数 . 最適な$k$を求めるため，データの対数尤度を最大とする折れ線を考える．この折れ線の長さを$l$とし，$ boldsymbol{f}:[0,l] to boldsymbol{R} ^ d$で表されるとする．簡単のために，$t in[0,l]$に対し，$p(t)= frac{1}{l}$，$p( boldsymbol{x} mid t)$は正規分布とする．よって，各データ点の対数尤度への寄与は負号をかけて . −log⁡p(x)=−log⁡∫t∈[0,l]p(x∣t)p(t) dt=log⁡l+c1−log⁡∫t∈[0,l]exp⁡(−∥x−f(t)∥22σ2) dt begin{aligned} &amp;- log p( boldsymbol{x}) &amp;= - log int _ {t in[0,l]}p( boldsymbol{x} mid t)p(t) ,dt &amp;= log l+c _ 1- log int _ {t in[0,l]} exp left(- frac{ | boldsymbol{x}- boldsymbol{f}(t) | ^ 2}{2 sigma ^ 2} right) ,dt end{aligned}​−logp(x)=−log∫t∈[0,l]​p(x∣t)p(t)dt=logl+c1​−log∫t∈[0,l]​exp(−2σ2∥x−f(t)∥2​)dt​ . となる．ここで，図のように距離を取ると， . ∥x−f(t)∥2=d⊥(f(t),x)+d∥(f(t),x)=d⊥(s,x)+d∥(f(t),x). begin{aligned} | boldsymbol{x}- boldsymbol{f}(t) | ^ 2 &amp;= d _ perp( boldsymbol{f}(t), boldsymbol{x})+d _ parallel( boldsymbol{f}(t), boldsymbol{x}) &amp;= d _ perp( boldsymbol{s}, boldsymbol{x})+d _ parallel( boldsymbol{f}(t), boldsymbol{x}). end{aligned}∥x−f(t)∥2​=d⊥​(f(t),x)+d∥​(f(t),x)=d⊥​(s,x)+d∥​(f(t),x).​ . . よって，$ eqref{k-segment _ log _ likelihood}$の最後の項は次のように書ける： . d⊥(s,x)22σ2−log⁡∫t∈[0,l]exp⁡(−d∥(f(t),x)22σ2) dt. frac{d _ perp( boldsymbol{s}, boldsymbol{x}) ^ 2}{2 sigma ^ 2}- log int _ {t in[0,l]} exp left(- frac{d _ parallel( boldsymbol{f}(t), boldsymbol{x}) ^ 2}{2 sigma ^ 2} right) ,dt.2σ2d⊥​(s,x)2​−log∫t∈[0,l]​exp(−2σ2d∥​(f(t),x)2​)dt. . $d _ parallel( boldsymbol{s}, boldsymbol{x})= inf _ {t in[0,l]}d _ parallel( boldsymbol{f}(t), boldsymbol{x})$とする．$ eqref{k-segment _ log _ likelihood}$は次のように近似できる： . log⁡l+c1+d⊥(s,x)22σ2−log⁡∫exp⁡(−d∥(f(t),x)22σ2) dt∼log⁡l+c1+d⊥(s,x)22σ2−log⁡∫exp⁡(−d∥(s,x)22σ2) dt=log⁡l+d(s,x)22σ2+c. begin{aligned} &amp; log l+c _ 1+ frac{d _ perp( boldsymbol{s}, boldsymbol{x}) ^ 2}{2 sigma ^ 2}- log int exp left(- frac{d _ parallel( boldsymbol{f}(t), boldsymbol{x}) ^ 2}{2 sigma ^ 2} right) ,dt &amp; sim log l+c _ 1+ frac{d _ perp( boldsymbol{s}, boldsymbol{x}) ^ 2}{2 sigma ^ 2}- log int exp left(- frac{d _ parallel( boldsymbol{s}, boldsymbol{x}) ^ 2}{2 sigma ^ 2} right) ,dt &amp;= log l + frac{d( boldsymbol{s}, boldsymbol{x}) ^ 2}{2 sigma ^ 2} + c. end{aligned}​logl+c1​+2σ2d⊥​(s,x)2​−log∫exp(−2σ2d∥​(f(t),x)2​)dt∼logl+c1​+2σ2d⊥​(s,x)2​−log∫exp(−2σ2d∥​(s,x)2​)dt=logl+2σ2d(s,x)2​+c.​ . よって，全点についての和を取れば，対数尤度に負号をかけたものは . nlog⁡l+∑i=1k∑x∈Vid(si,x)22σ2n log l+ sum _ {i=1} ^ k sum _ { boldsymbol{x} in V _ i} frac{d( boldsymbol{s} _ i, boldsymbol{x}) ^ 2}{2 sigma ^ 2}nlogl+i=1∑k​x∈Vi​∑​2σ2d(si​,x)2​ . となる．$ eqref{k-segment _ log _ likelihood _ sum}$が初めて極小となる$k$が最適な$k$となる． . 参考文献 . Hastie, T. and Stuetzle, W. (1989). Principal Curves. JASA, 84 (406), 502-516. (pdf) | Hastie, T., Tibshirani, R., and Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. (2nd ed.). Springer. (website) | Kegl, B.,Krzyzak, A., Linder, T., and Zeger, K. (2000). Learning and design of principal curves. IEEE. 22 (3), 281-297. (pdf) | Verbeek, J., Vlassis, N., and Krose, B. (2002). A k-segments algorithm for finding principal curves. Pattern Recognition Letters, Elsevier. 23 (8), 1009–1017. (pdf) | .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/statistics/2019/09/29/princurve.html",
            "relUrl": "/statistics/2019/09/29/princurve.html",
            "date": " • Sep 29, 2019"
        }
        
    
  
    
        ,"post16": {
            "title": "Boltzmann Generatorsの解説",
            "content": "Boltzmann Generatorsの論文についての解説，というかざっくりしたメモ．機械学習とかニューラルネットワークとかは完全にエアプなんで間違ってたらゴメンね．RealNVPとかGANの話知ってると楽かも． . Boltzmann分布と正規分布を対応させる写像を，機械学習で作る．ただし，Boltzmann分布での低エネルギー状態が正規分布で原点にあるとする． . 例えばタンパク質がopenとcloseの２形態を取ったとする．Boltzmann分布からサンプリングしようとすれば，普通はある安定な状態（今回はopenとする）から始める．openの状態から，タンパク質の側鎖の角度や長さなどに微妙な摂動を加えていって，エネルギーが低下するようなものをどんどん選んでいく．この時，openとcloseの間に準安定な領域が存在すれば，そこから抜け出せなくなって，サンプリングが詰む． . そこで，Boltzmann分布と結ばれた正規分布を考える．この正規分布では，低エネルギー状態は原点付近に存在する．正規分布の方でopenの点（原点付近）からスタートして，摂動を加えてサンプリング（原点付近）する．close（原点付近）が見つかるまでサンプリングできる． . Boltzmann Generatorとは . ある系のconfigrationが$ boldsymbol{x}$で，その時のエネルギーが$U( boldsymbol{x})$で表されるとすると，系がconfigration $ boldsymbol{x}$を取る確率はBoltzmann分布に従い， . exp⁡(−U(x)kT0)=exp⁡(−u(x))(1) begin{aligned} exp left(- frac{U( boldsymbol{x})}{kT_0} right)= exp(-u( boldsymbol{x})) end{aligned} tag{1}exp(−kT0​U(x)​)=exp(−u(x))​(1) . に比例する．ただ，全ての状態を数えるのは非現実的なので，平衡状態からニューラルネットワークを使って，one-shotでサンプリングすることを目標とする．latent spaceとしては，latent variable$ boldsymbol{z}$の正規分布$p_Z( boldsymbol{z})$を使う．与えられたBoltzmann分布に近いconfigurationの確率分布$p_X( boldsymbol{x})$と，$p_Z( boldsymbol{z})$と$p_X( boldsymbol{x})$の間の全単射$F_{zx}: boldsymbol{z} mapsto boldsymbol{x}$を求めることが目標となる． . 実際にBoltzmann分布でのサンプルを得るには，$p_X( boldsymbol{x})$を重みづけすることが必要である．この場合， . w(x)=e−u(x)pX(x)(2) begin{aligned} w( boldsymbol{x})= frac{e^{-u( boldsymbol{x})}}{p_X( boldsymbol{x})} end{aligned} tag{2}w(x)=pX​(x)e−u(x)​​(2) . で重みづけすればよい． . 写像の構成 . configration variable $ boldsymbol{x}$とlatent variable $z$の間には，次のような関係があるとする： . z=Fxz(x;θ)x=Fzx(z;θ).(3) begin{aligned} boldsymbol{z} &amp;= boldsymbol{F} _ {xz}( boldsymbol{x}; theta) boldsymbol{x} &amp;= boldsymbol{F} _ {zx}( boldsymbol{z}; theta). end{aligned} tag{3}zx​=Fxz​(x;θ)=Fzx​(z;θ).​(3) . もちろん， $ boldsymbol{F} _ {xz}= boldsymbol{F} _ {zx}{}^{-1}$ ．これらの変換のJacobi行列を求めれば， . Jzx(z;θ)=∂x∂z=(∂Fzx∂z1,…,∂Fzx∂zn)(z;θ)Jxz(x;θ)=∂z∂x=(∂Fxz∂x1,…,∂Fxz∂xn)(x;θ)(4) begin{aligned} J_{zx}( boldsymbol{z}; theta) &amp;= frac{ partial boldsymbol{x}}{ partial boldsymbol{z}} = left( frac{ partial boldsymbol{F} _ {zx}}{ partial z_1}, dots, frac{ partial boldsymbol{F} _ {zx}}{ partial z_n} right)( boldsymbol{z}; theta) J_{xz}( boldsymbol{x}; theta) &amp;= frac{ partial boldsymbol{z}}{ partial boldsymbol{x}} = left( frac{ partial boldsymbol{F} _ {xz}}{ partial x_1}, dots, frac{ partial boldsymbol{F} _ {xz}}{ partial x_n} right)( boldsymbol{x}; theta) end{aligned} tag{4}Jzx​(z;θ)Jxz​(x;θ)​=∂z∂x​=(∂z1​∂Fzx​​,…,∂zn​∂Fzx​​)(z;θ)=∂x∂z​=(∂x1​∂Fxz​​,…,∂xn​∂Fxz​​)(x;θ)​(4) . となり，さらにJacobianを . Rxz(x;θ)=∣det⁡Jxz(x;θ)∣Rzx(x;θ)=∣det⁡Jzx(z;θ)∣(5) begin{aligned} R_{xz}( boldsymbol{x}; theta) &amp;= left| det J_{xz}( boldsymbol{x}; theta) right| R_{zx}( boldsymbol{x}; theta) &amp;= left| det J_{zx}( boldsymbol{z}; theta) right| end{aligned} tag{5}Rxz​(x;θ)Rzx​(x;θ)​=∣detJxz​(x;θ)∣=∣detJzx​(z;θ)∣​(5) . とする． . 写像を$ boldsymbol{x}$から$ boldsymbol{z}$への体積非保存の「流れ」として考えれば，$p_X( boldsymbol{x}) ,dx=p_Z( boldsymbol{z}) ,dz$が成立するので， . pX(x;θ)=pZ(z)∣∂z∂x∣=pZ(Fxz(x;θ))Rxz(x;θ)pZ(z;θ)=pX(x)∣∂x∂z∣=pX(Fzx(z;θ))Rzx(z;θ)(6) begin{aligned} p_X( boldsymbol{x}; theta) &amp;= p_Z( boldsymbol{z}) left| frac{ partial boldsymbol{z}}{ partial{x}} right| &amp;= p_Z( boldsymbol{F} _ {xz}( boldsymbol{x}; theta))R_{xz}( boldsymbol{x}; theta) p_Z( boldsymbol{z}; theta) &amp;= p_X( boldsymbol{x}) left| frac{ partial boldsymbol{x}}{ partial{z}} right| &amp;= p_X( boldsymbol{F} _ {zx}( boldsymbol{z}; theta))R_{zx}( boldsymbol{z}; theta) end{aligned} tag{6}pX​(x;θ)pZ​(z;θ)​=pZ​(z)∣∣∣∣∣​∂x∂z​∣∣∣∣∣​=pZ​(Fxz​(x;θ))Rxz​(x;θ)=pX​(x)∣∣∣∣∣​∂z∂x​∣∣∣∣∣​=pX​(Fzx​(z;θ))Rzx​(z;θ)​(6) . となる．右辺の確率分布は$ theta$に依存しない（系がはじめから有している確率分布）が，左辺の計算結果は$ theta$に依存する（Boltzmann Generatorによって得られた確率分布）． . RealNVPによるニューラルネットワーク構成 . $F_{zx}$を直接求めることは困難なので，アフィンカップリングレイヤ（入出力の一部が比較的簡単な関係にある全単射）を考える．具体的には，$ boldsymbol{x}$を$( boldsymbol{x} _ 1, boldsymbol{x} _ 2)$，$ boldsymbol{z}$を$( boldsymbol{z} _ 1, boldsymbol{z} _ 2)$に分ける．これらについて，非線形変換を次のように定義する： . fxz(x1,x2):{z1=x1z2=x2⊙exp⁡(S(x1;θ))+T(x1;θ);log⁡Rxz=∑iSi(x1;θ).(7) begin{aligned} &amp; boldsymbol{f} _ {xz}( boldsymbol{x_1}, boldsymbol{x_2}) quad: begin{cases} boldsymbol{z} _ 1 = boldsymbol{x} _ 1 boldsymbol{z} _ 2 = boldsymbol{x} _ 2 odot exp( boldsymbol{S}( boldsymbol{x} _ 1; theta))+ boldsymbol{T}( boldsymbol{x} _ 1; theta) end{cases}; &amp; log R_{xz} = sum_iS_i( boldsymbol{x} _ 1; theta). end{aligned} tag{7}​fxz​(x1​,x2​):{z1​=x1​z2​=x2​⊙exp(S(x1​;θ))+T(x1​;θ)​;logRxz​=i∑​Si​(x1​;θ).​(7) . さらに，アフィンカップリングレイヤの逆写像は次の様になる： . fzx(z1,z2):{x1=z1x2=(z2−T(x1;θ))⊙exp⁡(−S(z1;θ));log⁡Rzx=−∑iSi(z1;θ).(8) begin{aligned} &amp; boldsymbol{f} _ {zx}( boldsymbol{z_1}, boldsymbol{z_2}) quad: begin{cases} boldsymbol{x} _ 1 = boldsymbol{z} _ 1 boldsymbol{x} _ 2 = ( boldsymbol{z} _ 2- boldsymbol{T}( boldsymbol{x_1}; theta)) odot exp(- boldsymbol{S}( boldsymbol{z} _ 1; theta)) end{cases}; &amp; log R_{zx} = - sum_iS_i( boldsymbol{z} _ 1; theta). end{aligned} tag{8}​fzx​(z1​,z2​):{x1​=z1​x2​=(z2​−T(x1​;θ))⊙exp(−S(z1​;θ))​;logRzx​=−i∑​Si​(z1​;θ).​(8) . このニューラルネットワークによって，合成写像$F_{zx}$が得られる． . 機械学習のtraining . Boltzmann Generatorでは，主に２つの機械学習：training by energyとtraining by exampleを使う． . training by energyの場合 . latent spaceから正規分布$p_Z( boldsymbol{z})$に従って適当に$ boldsymbol{z}$を選ぶ | $ boldsymbol{F} _ {zx}: boldsymbol{z} mapsto boldsymbol{x}$ を使って$p_X(x)$を計算する | 生成された確率分布$p_X(x)$と目標のBoltzmann分布$e^{-u( boldsymbol{x})}$との差を次のロスで評価する． | JKL=Ez[u(Fzx(z))−log⁡Rzx(z)](9) begin{aligned} J_ text{KL}=E_{ boldsymbol{z}}[u( boldsymbol{F} _ {zx}( boldsymbol{z}))- log R_{zx}( boldsymbol{z})] end{aligned} tag{9}JKL​=Ez​[u(Fzx​(z))−logRzx​(z)]​(9) . $J_ text{KL}$を小さくすることが目標となる． . training by exampleの場合 . シミュレーションや観測結果から，実際に得られた$ boldsymbol{x}$を用意する．つまり，$e^{-u( boldsymbol{x})}$が大きい． | $F_{xz}: boldsymbol{x} mapsto boldsymbol{z}$で，これを$ boldsymbol{z}$に変換する．この$ boldsymbol{z}$が正規分布$p_Z( boldsymbol{z})$から選ばれやすければよい．それは，次のロス | JML=Ex[12∥Fxz(x)∥2−log⁡Rxz(x)](10) begin{aligned} J_ text{ML}=E_{ boldsymbol{x}} left[ frac{1}{2} | boldsymbol{F} _ {xz}( boldsymbol{x}) |^2- log R_{xz}( boldsymbol{x}) right] end{aligned} tag{10}JML​=Ex​[21​∥Fxz​(x)∥2−logRxz​(x)]​(10) . を最小化することに等しい．$J_ text{ML}$の第１項は正規分布に対応する調和振動子のエネルギーを表す． . 一般の場合 . 一般の場合にはBoltzmann Generatorでは合計のロス . J=wMLJML+wKLJKL+wMLJML(11) begin{aligned} J=w_ text{ML}J_ text{ML}+w_ text{KL}J_ text{KL}+w_ text{ML}J_ text{ML} end{aligned} tag{11}J=wML​JML​+wKL​JKL​+wML​JML​​(11) . を最小とすることが目標になる． . KLロス関数 . 以下では，真の確率分布と学習によって得られた確率分布を区別する．すなわち，$ boldsymbol{x}$の真の確率分布$ mu_X( boldsymbol{x})$は分配関数を$Z_X$とするBoltzmann分布である： . μX(x)=1ZXe−u(x).(12) begin{aligned} mu_X( boldsymbol{x})= frac{1}{Z_X}e^{-u( boldsymbol{x})}. end{aligned} tag{12}μX​(x)=ZX​1​e−u(x).​(12) . また，$ boldsymbol{z}$の真の確率分布$ mu_Z( boldsymbol{z})$は正規分布である： . μZ(z)=1ZZexp⁡(−z22σ2).(13) begin{aligned} mu_Z( boldsymbol{z})= frac{1}{Z_Z} exp left(- frac{ boldsymbol{z}^2}{2 sigma^2} right). end{aligned} tag{13}μZ​(z)=ZZ​1​exp(−2σ2z2​).​(13) . ただし，１つの温度しか考えない場合は$ sigma=1$とする．また， . uZ(z)=−log⁡μZ(z)=12σ2z2+C.(14) begin{aligned} u_Z( boldsymbol{z})=- log mu_Z( boldsymbol{z})= frac{1}{2 sigma^2} boldsymbol{z}^2+C. end{aligned} tag{14}uZ​(z)=−logμZ​(z)=2σ21​z2+C.​(14) . とする．学習による$ boldsymbol{x}$，$ boldsymbol{z}$の確率分布をそれぞれ$q_X( boldsymbol{x})$，$q_Z( boldsymbol{z})$とする．これらについては，(6)式から， . qZ(z;θ)=μX(Fzx(z;θ))Rzx(z)qX(x;θ)=μZ(Fxz(x;θ))Rxz(x)(15) begin{aligned} q_Z( boldsymbol{z}; theta) &amp;= mu_X( boldsymbol{F} _ {zx}( boldsymbol{z}; theta))R_{zx}( boldsymbol{z}) q_X( boldsymbol{x}; theta) &amp;= mu_Z( boldsymbol{F} _ {xz}( boldsymbol{x}; theta))R_{xz}( boldsymbol{x}) end{aligned} tag{15}qZ​(z;θ)qX​(x;θ)​=μX​(Fzx​(z;θ))Rzx​(z)=μZ​(Fxz​(x;θ))Rxz​(x)​(15) . が成り立つ． . 確率分布$p$，$q$に対して，Kullback-Leibler divergenceは次の式で定義される： . KL(q∥p)=∫q(x)[log⁡q(x)−log⁡p(x)] dx=−Hq(x)−∫q(x)log⁡p(x) dx.(16) begin{aligned} text{KL}(q | p) &amp;= int q( boldsymbol{x})[ log q( boldsymbol{x})- log p( boldsymbol{x})] ,d boldsymbol{x} &amp;= -H_q( boldsymbol{x})- int q( boldsymbol{x}) log p( boldsymbol{x}) ,d boldsymbol{x}. end{aligned} tag{16}KL(q∥p)​=∫q(x)[logq(x)−logp(x)]dx=−Hq​(x)−∫q(x)logp(x)dx.​(16) . よって，$ mu_Z$と$q_Z$のKullback-Leibler divergenceは . KLθ(μZ∥qZ)=−HZ−∫μZ(z)log⁡q(z) dz=−HZ−∫μZ(z)[log⁡μX(Fzx(z;θ)+log⁡Rzx(z;θ))] dz=−HZ−∫μZ(z)[log⁡exp⁡(−u(Fzx(z;θ)))ZX+log⁡Rzx(z;θ))] dz=−HZ+log⁡ZX+Ez∼μZ(Z)[u(Fzx(z;θ))−log⁡Rzx(z;θ)](17) begin{aligned} &amp; text{KL} _ theta( mu_Z | q_Z) = -H_Z- int mu_Z( boldsymbol{z}) log q( boldsymbol{z}) ,d boldsymbol{z} &amp;= -H_Z- int mu_Z( boldsymbol{z})[ log mu_X( boldsymbol{F} _ {zx}( boldsymbol{z}; theta) &amp; qquad qquad+ log R_{zx}( boldsymbol{z}; theta))] ,d boldsymbol{z} &amp;= -H_Z- int mu_Z( boldsymbol{z}) biggl[ log frac{ exp(-u( boldsymbol{F} _ {zx}( boldsymbol{z}; theta)))}{Z_X} &amp; qquad qquad+ log R_{zx}( boldsymbol{z}; theta)) biggr] ,d boldsymbol{z} &amp;= -H_Z+ log Z_X &amp; qquad+E_{ boldsymbol{z} sim mu_Z( boldsymbol{Z})}[u(F_{zx}( boldsymbol{z}; theta))- log R_{zx}( boldsymbol{z}; theta)] end{aligned} tag{17}​KLθ​(μZ​∥qZ​)=−HZ​−∫μZ​(z)logq(z)dz=−HZ​−∫μZ​(z)[logμX​(Fzx​(z;θ)+logRzx​(z;θ))]dz=−HZ​−∫μZ​(z)[logZX​exp(−u(Fzx​(z;θ)))​+logRzx​(z;θ))]dz=−HZ​+logZX​+Ez∼μZ​(Z)​[u(Fzx​(z;θ))−logRzx​(z;θ)]​(17) . この第３項が，(9)式の$J_ text{KL}$である．また， . HX=−∫qX(x;θ)log⁡qX(x;θ) dx=−∫μZ(Fxz(x;θ))Rxz(x)×log⁡(μZ(Fxz(x;θ))Rxz(x)) dx=−∫μZ(Fxz(x;θ))∣∂z∂x∣×log⁡(μZ(Fxz(x;θ))Rxz(x)) dx=−∫μZ(z)log⁡(μZ(z)Rzx−1(z)) dz=−∫μZ(z)log⁡μZ(z) dz+Ez∼μZ(z)(18) begin{aligned} H_X &amp;= - int q_X( boldsymbol{x}; theta) log q_X( boldsymbol{x}; theta) ,d boldsymbol{x} &amp;= - int mu_Z(F_{xz}( boldsymbol{x}; theta))R_{xz}( boldsymbol{x}) &amp; qquad qquad times log( mu_Z(F_{xz}( boldsymbol{x}; theta))R_{xz}( boldsymbol{x})) ,d boldsymbol{x} &amp;= - int mu_Z(F_{xz}( boldsymbol{x}; theta)) left| frac{ partial boldsymbol{z}}{ partial boldsymbol{x}} right| &amp; qquad qquad times log( mu_Z(F_{xz}( boldsymbol{x}; theta))R_{xz}( boldsymbol{x})) ,d boldsymbol{x} &amp;= - int mu_Z( boldsymbol{z}) log( mu_Z( boldsymbol{z})R_{zx}{}^{-1}( boldsymbol{z})) ,d boldsymbol{z} &amp;= - int mu_Z( boldsymbol{z}) log mu_Z( boldsymbol{z}) ,d boldsymbol{z}+E_{ boldsymbol{z} sim mu_Z( boldsymbol{z})} end{aligned} tag{18}HX​​=−∫qX​(x;θ)logqX​(x;θ)dx=−∫μZ​(Fxz​(x;θ))Rxz​(x)×log(μZ​(Fxz​(x;θ))Rxz​(x))dx=−∫μZ​(Fxz​(x;θ))∣∣∣∣∣​∂x∂z​∣∣∣∣∣​×log(μZ​(Fxz​(x;θ))Rxz​(x))dx=−∫μZ​(z)log(μZ​(z)Rzx​−1(z))dz=−∫μZ​(z)logμZ​(z)dz+Ez∼μZ​(z)​​(18) . なので， . KLθ(μZ∥qZ)=−HZ+log⁡ZX+Ez∼μZ(Z)[u(Fzx(z;θ))−log⁡Rzx(z;θ)]=−HX+log⁡ZX+Ez∼μZ(z)[u(Fzx(z;θ))]=−HX+log⁡ZX+Ex∼μX(x)[u(x)].(19) begin{aligned} &amp; text{KL} _ theta( mu_Z | q_Z) &amp;= -H_Z+ log Z_X &amp; qquad+E_{ boldsymbol{z} sim mu_Z( boldsymbol{Z})}[u(F_{zx}( boldsymbol{z}; theta))- log R_{zx}( boldsymbol{z}; theta)] &amp;= -H_X+ log Z_X+E_{ boldsymbol{z} sim mu_Z( boldsymbol{z})}[u( boldsymbol{F} _ {zx}( boldsymbol{z}; theta))] &amp;= -H_X+ log Z_X+E_{ boldsymbol{x} sim mu_X( boldsymbol{x})}[u( boldsymbol{x})]. end{aligned} tag{19}​KLθ​(μZ​∥qZ​)=−HZ​+logZX​+Ez∼μZ​(Z)​[u(Fzx​(z;θ))−logRzx​(z;θ)]=−HX​+logZX​+Ez∼μZ​(z)​[u(Fzx​(z;θ))]=−HX​+logZX​+Ex∼μX​(x)​[u(x)].​(19) . 同様に， . KLθ(qX∥μX)=−HX−∫qX(x;θ)log⁡μX(x) dx=−HX−∫μZ(Fxz(x;θ))Rxz(x)log⁡μX(x) dx=−HX+log⁡ZX+Ez∼μZ(z)[u(Fzx(z;θ))]=−HX+log⁡ZX+Ex∼μX(x)[u(x)].(20) begin{aligned} &amp; text{KL} _ theta(q_X | mu_X) &amp;= -H_X- int q_X( boldsymbol{x}; theta) log mu_X( boldsymbol{x}) ,d boldsymbol{x} &amp;= -H_X- int mu_Z( boldsymbol{F} _ {xz}( boldsymbol{x}; theta))R_{xz}( boldsymbol{x}) log mu_X( boldsymbol{x}) ,d boldsymbol{x} &amp;= -H_X+ log Z_X+E_{ boldsymbol{z} sim mu_Z( boldsymbol{z})}[u( boldsymbol{F} _ {zx}( boldsymbol{z}; theta))] &amp;= -H_X+ log Z_X+E_{ boldsymbol{x} sim mu_X( boldsymbol{x})}[u( boldsymbol{x})]. end{aligned} tag{20}​KLθ​(qX​∥μX​)=−HX​−∫qX​(x;θ)logμX​(x)dx=−HX​−∫μZ​(Fxz​(x;θ))Rxz​(x)logμX​(x)dx=−HX​+logZX​+Ez∼μZ​(z)​[u(Fzx​(z;θ))]=−HX​+logZX​+Ex∼μX​(x)​[u(x)].​(20) . 以上から， . KLθ(μZ∥qZ)=KLθ(qX∥μX).(21) begin{aligned} text{KL} _ theta( mu_Z | q_Z)= text{KL} _ theta(q_X | mu_X). end{aligned} tag{21}KLθ​(μZ​∥qZ​)=KLθ​(qX​∥μX​).​(21) . さらに，$U=E_{ boldsymbol{x} sim mu_X( boldsymbol{x})}[u( boldsymbol{x})]$として， . JKL=U−HX+HZ(22) begin{aligned} J_ text{KL}=U-H_X+H_Z end{aligned} tag{22}JKL​=U−HX​+HZ​​(22) . が得られる．この式を参考にすれば，(9)式で定義した$J_ text{KL}$の表式において，第１項は系の内部エネルギーを，第２項は自由エネルギーに対するエントロピーの寄与を表すことが分かる． . (2)式でも述べたように，重み付けは， . w(x)=μX(x)qX(x)=qZ(z)μZ(z)=ZXexp⁡(−u(x))μZ(z)Rxz(x)∝exp⁡(−u(Fzx(x;θ))+uZ(z)+log⁡Rzx(z))(23) begin{aligned} w( boldsymbol{x}) &amp;= frac{ mu_X( boldsymbol{x})}{q_X( boldsymbol{x})}= frac{q_Z( boldsymbol{z})}{ mu_Z( boldsymbol{z})} &amp;= frac{Z_X exp(-u( boldsymbol{x}))}{ mu_Z( boldsymbol{z})R_{xz}( boldsymbol{x})} &amp; propto exp(-u( boldsymbol{F} _ {zx}( boldsymbol{x}; theta))+u_Z( boldsymbol{z})+ log R_{zx}( boldsymbol{z})) end{aligned} tag{23}w(x)​=qX​(x)μX​(x)​=μZ​(z)qZ​(z)​=μZ​(z)Rxz​(x)ZX​exp(−u(x))​∝exp(−u(Fzx​(x;θ))+uZ​(z)+logRzx​(z))​(23) . となる．これによって，平衡状態でのある量$A( boldsymbol{x})$の期待値は . E(A)≈∑i=1Nw(xA(x))∑i=1Nw(w)(24) begin{aligned} E(A) approx frac{ sum_{i=1}^Nw( boldsymbol{x}A( boldsymbol{x}))}{ sum_{i=1}^Nw( boldsymbol{w})} end{aligned} tag{24}E(A)≈∑i=1N​w(w)∑i=1N​w(xA(x))​​(24) . で与えられる．さらに， . min⁡KLθ(μZ∥qZ)=min⁡Ez∼μZ(Z)[u(Fzx(z;θ))−log⁡Rzx(z;θ)]=min⁡Ez∼μZ(Z)[u(Fzx(x;θ))−uZ(z)−log⁡Rzx(z)]=min⁡Ez∼μZ(Z)[log⁡μZ(z)−log⁡qZ(z;θ)]=max⁡Ez∼μZ(Z)[log⁡w(x)].(25) begin{aligned} &amp; min text{KL} _ theta( mu_Z | q_Z) &amp;= min E_{ boldsymbol{z} sim mu_Z( boldsymbol{Z})}[u( boldsymbol{F} _ {zx}( boldsymbol{z}; theta))- log R_{zx}( boldsymbol{z}; theta)] &amp;= min E_{ boldsymbol{z} sim mu_Z( boldsymbol{Z})}[u( boldsymbol{F} _ {zx}( boldsymbol{x}; theta))-u_Z( boldsymbol{z})- log R_{zx}( boldsymbol{z})] &amp;= min E_{ boldsymbol{z} sim mu_Z( boldsymbol{Z})}[ log mu_Z( boldsymbol{z})- log q_Z( boldsymbol{z}; theta)] &amp;= max E_{ boldsymbol{z} sim mu_Z( boldsymbol{Z})}[ log w( boldsymbol{x})]. end{aligned} tag{25}​minKLθ​(μZ​∥qZ​)=minEz∼μZ​(Z)​[u(Fzx​(z;θ))−logRzx​(z;θ)]=minEz∼μZ​(Z)​[u(Fzx​(x;θ))−uZ​(z)−logRzx​(z)]=minEz∼μZ​(Z)​[logμZ​(z)−logqZ​(z;θ)]=maxEz∼μZ​(Z)​[logw(x)].​(25) . MLロス関数 . $ text{KL} _ theta( mu_Z | q_Z)= text{KL} _ theta(q_X| mu_X)$であったが，次のKLロスを考える： . KLθ(μX∥qX)=−HX−∫μX(x)log⁡qX(x;θ) dx=−HX−∫μX(x)[log⁡μZ(Fxz(x;θ))+log⁡Rxz(x)] dx=−HX+log⁡ZZ+Ex∼μX(x)[12σ2∥Fxz(x;θ)∥2−log⁡Rxz(x)].(26) begin{aligned} &amp; text{KL} _ theta( mu_X | q_X) = -H_X- int mu_X( boldsymbol{x}) log q_X( boldsymbol{x}; theta) ,d boldsymbol{x} &amp;= -H_X- int mu_X( boldsymbol{x})[ log mu_Z( boldsymbol{F} _ {xz}( boldsymbol{x}; theta))+ log R_{xz}( boldsymbol{x})] ,d boldsymbol{x} &amp;= -H_X+ log Z_Z &amp; qquad+E_{ boldsymbol{x} sim mu_X( boldsymbol{x})} left[ frac{1}{2 sigma^2} | boldsymbol{F} _ {xz}( boldsymbol{x}; theta) |^2- log R_{xz}( boldsymbol{x}) right]. end{aligned} tag{26}​KLθ​(μX​∥qX​)=−HX​−∫μX​(x)logqX​(x;θ)dx=−HX​−∫μX​(x)[logμZ​(Fxz​(x;θ))+logRxz​(x)]dx=−HX​+logZZ​+Ex∼μX​(x)​[2σ21​∥Fxz​(x;θ)∥2−logRxz​(x)].​(26) . training by exampleを実行する場合，そのexampleが$ mu_X( boldsymbol{x})$に従ったものかは分からないので，このロスを評価することは困難である． . 代わりに，サンプルの分布$ rho( boldsymbol{x})$を使って， . JML=−Ex∼ρ[log⁡qX(x;θ)]=Ex∼ρ(x)[12σ2∥Fxz(x;θ)∥2−log⁡Rxz(x)](27) begin{aligned} J_ text{ML} &amp;= -E_{ boldsymbol{x} sim rho}[ log q_X( boldsymbol{x}; theta)] &amp;= E_{ boldsymbol{x} sim rho( boldsymbol{x})} left[ frac{1}{2 sigma^2} | boldsymbol{F} _ {xz}( boldsymbol{x}; theta) |^2- log R_{xz}( boldsymbol{x}) right] end{aligned} tag{27}JML​​=−Ex∼ρ​[logqX​(x;θ)]=Ex∼ρ(x)​[2σ21​∥Fxz​(x;θ)∥2−logRxz​(x)]​(27) . を考える．これを最小にすることは，サンプル$ rho( boldsymbol{x})$が正規分布で選ばれる確率が最大になることに対応する．通常は$ sigma=1$なので，(10)式​が得られる． . RCロス関数 . configration spaceで定義されたreaction cordinate $r( boldsymbol{x})$を使う場合は，次のRCロス関数を考える： . JRC=∫p(r(x))log⁡p(r(x)) dr(x)=Ex∼qX(x)log⁡p(r(x)).(28) begin{aligned} J_ text{RC} &amp;= int p(r( boldsymbol{x})) log p(r( boldsymbol{x})) ,dr( boldsymbol{x}) &amp;= E_{ boldsymbol{x} sim q_X( boldsymbol{x})} log p(r( boldsymbol{x})). end{aligned} tag{28}JRC​​=∫p(r(x))logp(r(x))dr(x)=Ex∼qX​(x)​logp(r(x)).​(28) . $p(r( boldsymbol{x}))$は，上限と下限でのカーネル密度推定として計算される． . 複数の温度を扱う場合 . 基準の温度の$ tau_k$倍の温度について計算する場合，$u( boldsymbol{x})$は$1/ tau_k$倍，$ sigma^2$は$ tau_k$倍とする． .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/machine-learning/2019/09/25/boltzmanngenerators.html",
            "relUrl": "/machine-learning/2019/09/25/boltzmanngenerators.html",
            "date": " • Sep 25, 2019"
        }
        
    
  
    
        ,"post17": {
            "title": "Palantirの解説",
            "content": "Palantirの論文の解説．元論文はほとんど式の解説とかなくて，ほぼsupplementaryに数式による詳細が載っている．内容としては，コンピュータにヤバヤバな計算をさせる．サンプルの細胞の経路を複数調べたりできるけど，細胞の数を減らしたりする操作（scEpathならメタ細胞を作ったりした）が無いから，計算量はお察し． . データ幾何学（多様体） . データが２次元曲面に分布しているとする．これは３次元空間$ boldsymbol{R}^3$（多様体）に埋め込まれた部分多様体$L^2$（もう少し詳しく言えば超曲面）と考えられる．データを解析する場合は，３次元空間でやるより，これを２次元に直した方が圧倒的に効率が良い．よって，次元を落とすわけだが，この際にデータ間の距離を不変にしたい．この場合，距離は部分多様体での測地線距離とする． . diffusion mapは，データを低次元のユークリッド空間に埋め込む方法．Palantirではdiffusion mapを使う． . Nearest Neighbor Graph . nearest neighbor graphは似た細胞同士を結んだグラフ．当然のことながら，この長い経路は細胞の成長におけるトラジェクトリーに一致する．以下では，$k$-nearest neighbor graph，つまり$k$個の近い細胞を結んだグラフ$G_X in boldsymbol{R}^{N times N}$を考える． . $N$個の細胞と$M$個の遺伝子のデータとして$X in boldsymbol{R}^{N times M}$が与えられたとする．$ boldsymbol{R}^M$での距離をaffinityに変換するため，細胞$i$と細胞$j$について，次のadaptive Gaussian kernelを定義する： . K(xi,xj)=12π(σi+σj)exp⁡(−12(xi−xj)t(xi+xj)σi−σj).(1)K( boldsymbol{x} _ i, boldsymbol{x} _ j) = frac{1}{ sqrt{2 pi( sigma_i+ sigma_j)}} exp left(- frac{1}{2} frac{( boldsymbol{x} _ i- boldsymbol{x} _ j)^t( boldsymbol{x} _ i+ boldsymbol{x} _ j)}{ sigma_i- sigma_j} right). tag{1}K(xi​,xj​)=2π(σi​+σj​) . ​1​exp(−21​σi​−σj​(xi​−xj​)t(xi​+xj​)​).(1) . ただし，$ sigma_i$は，細胞$i$について，$l$番目に近い細胞までの距離である($l&lt;k$)． . $ boldsymbol{x} _ i$ は細胞$i$での遺伝子発現を表す（行）ベクトルである．これによって，affinity行列 $K in boldsymbol{R}^{N times N}$ が得られる．さらに，$K$のラプラシアン行列$T in boldsymbol{R}^{N times N}$を計算する（$X$上での$k(x,y)$の$y$についての和で$k(x,y)$を割る）．$T_{ij}$は規格化されており，細胞$i$から細胞$j$へ１回で遷移できる確率を表わしている（Markov過程での遷移確率行列）． . $T$の固有ベクトルをdiffusion componentとする．さらに，対応する固有値を$ lambda_1, lambda_2, ldots$とする．diffusion componentの順序は固有値が降順になるようにする． . 元々のグラフ$G_X$から，ノイズを減らしたグラフ$G_E in boldsymbol{R}^{N times N}$を考える．pseudotimeは，このグラフ$G_E$での最短経路の長さによって計算される．また，pseudotimeを決める際，waypointを定めておく．pseudotimeはwaypointによる重み付けスコアに基づいて順番が決まる．すなわち，細胞に最も近いwaypointが，細胞の位置を決める際に最も高い重み付けを与える．多様体全体にwaypointが存在するのが望ましい（詳しくは後述）． . 多様体$E in R^{N times L}$を考える($L&lt;M$)．つまり，diffusion componentを$L$番目まで取ってきて，$N$個の細胞について$L$種類のdiffusion componentに沿った遺伝子発現をデータにしている．細胞$i$と細胞$j$のdiffusion distanceを次のように定義する： . DDt(ei,ej)2=∑l=1Lλl2t(ei(l)−ej(l))2. begin{aligned} text{DD} _ t(e_i,e_j)^2= sum_{l=1}^L lambda_l{}^{2t}(e_i{}^{(l)}-e_j{}^{(l)})^2. end{aligned}DDt​(ei​,ej​)2=l=1∑L​λl​2t(ei​(l)−ej​(l))2.​ . $t$はグラフに沿った（ランダムウォーキングの）ステップの数，$e_i{}^{(l)}$はdiffusion component$l$に沿った細胞$i$の埋め込みである（$l$番目の固有ベクトルを基底とした場合の成分，つまり遺伝子発現）．さらに，multi-scale distanceを次のように定義する： . MS(ei,ej)2=∑t=1∞∑l=1Lλl2t(ei(l)−ej(l))2. begin{aligned} text{MS}(e_i,e_j)^2= sum_{t=1}^ infty sum_{l=1}^L lambda_l{}^{2t}(e_i{}^{(l)}-e_j{}^{(l)})^2. end{aligned}MS(ei​,ej​)2=t=1∑∞​l=1∑L​λl​2t(ei​(l)−ej​(l))2.​ . 定義から，$1&gt; lambda_1&gt; lambda_2&gt; cdots&gt; lambda_L&gt;0$なので，multi-scale distanceは次のように書くことができる： . MS(ei,ej)2=∑l=1L(λl1−λl)2(ei(l)−ej(l))2. begin{aligned} text{MS}(e_i,e_j)^2= sum_{l=1}^L left( frac{ lambda_l}{1- lambda_l} right)^2(e_i{}^{(l)}-e_j{}^{(l)})^2. end{aligned}MS(ei​,ej​)2=l=1∑L​(1−λl​λl​​)2(ei​(l)−ej​(l))2.​ . waypointの決定 . Max-min samplingは既に存在するwaypointとの最短距離の最大値を新しいwaypointとする．これによって，多様体全体にwaypointを置くことができる． . $ boldsymbol{E}^{(l)}$を$l$番目のdiffusion componentとする．まず，$ text{WS}^{(l)}$を$N$個の細胞から適当に１つ選んだ細胞とする（初期値）．$j in text{WS}^{(l)}$に対し，diffusion componentに沿った，waypointまでの距離は次のように計算される： . wdij=(ei(l)−ej(l))2. begin{aligned} text{wd} _ {ij}=(e_i{}^{(l)}-e_j{}^{(l)})^2. end{aligned}wdij​=(ei​(l)−ej​(l))2.​ . 細胞$i$に対し，waypoint distanceの最小値は次のように計算される： . mdi=min⁡(wdij). begin{aligned} text{md} _ i= min( text{wd} _ {ij}). end{aligned}mdi​=min(wdij​).​ 先述の通り，既存のwaypointからの最短距離が最大になる点がwaypointの集合に加えられる： . WS(l)=⋃(WS(l),argmax⁡({mdi})). begin{aligned} text{WS}^{(l)}= bigcup( text{WS}^{(l)}, text{arg} max( { text{md} _ i })). end{aligned}WS(l)=⋃(WS(l),argmax({mdi​})).​ . 満足な数のwaypointが得られるまでこれを繰り返し，これを全成分に関して行えば良い．得られた$L$個の集合の和集合を考えれば，最終的に$nW$個のwaypointの集合$ text{WS}$が得られる． . pseudotimeの計算 . Palantirでは，スタートの細胞は多様体の境界にある，つまりあるdiffusion componentに関する端点であるとする．ここで，境界にある細胞を . C=⋃l=1L(argmin⁡E(l),argmax⁡E(l)) begin{aligned} C= bigcup_{l=1}^L( text{arg} min boldsymbol{E}^{(l)}, text{arg} max boldsymbol{E}^{(l)}) end{aligned}C=l=1⋃L​(argminE(l),argmaxE(l))​ . と表す．ユーザーが設定したスタートの細胞$s$に最も近いような$C$の要素をpseudotimeの開始点$s’$とする： . s′=argmin⁡i∈CMS(es,ei). begin{aligned} s&amp;#x27;= text{arg} min_{i in C} text{MS}(e_s,e_i). end{aligned}s′=argi∈Cmin​MS(es​,ei​).​ . pseudotimeの初期値$ tau_i{}^{(0)}$は細胞$s’$から細胞$i$への最短経路の距離とする．これを全細胞に対して考える：${ tau_i{}^{(0)}}$． . waypoint $w$を通って，より離れた（pseudotimeが大きい）細胞$i$に行く場合，その経路の長さは，$w$までのpseudotimeと$w,i$間の距離の和になる．もし，$i$より離れたwaypointを通って$i$に行く経路という場合は，$V_{wi}$は$ tau_w$から$D_{wi}$を引けばスタートから$i$までの適切な距離が求まる．以上のことを踏まえると，$D_{wi}$を細胞$i$からのwaypoint $w$への最短経路の距離として，細胞$i$のwaypoint $w$に対するperspective（上で言った「長さ」）は次のように計算される： . Vwi={τw(0)+Dwi (τi(0)&gt;τw(0))τw(0)−Dwi (otherwise). begin{aligned} V_{wi}= begin{cases} tau_w{}^{(0)}+D_{wi} &amp;( tau_i{}^{(0)}&gt; tau_w{}^{(0)}) tau_w{}^{(0)}-D_{wi} &amp;( text{otherwise}) end{cases}. end{aligned}Vwi​={τw​(0)+Dwi​ τw​(0)−Dwi​ ​(τi​(0)&gt;τw​(0))(otherwise)​.​ . あるwaypointを経由して細胞$i$に到達する経路のperspective（長さ）を計算した．よって，細胞$i$に至る経路の長さを求めるには全てのwaypointについて平均しなければならない．$i$に近いようなwaypointを通る経路は尤もらしいので，平均を取る時に重みを大きくする．逆に，$i$から離れたwaypointを通るような経路は，平均を取る時に重みを小さくする．具体的には，waypointのperspectiveの重み付け平均は，waypointと細胞の距離に反比例するような，指数関数による重み付けを使う．そのために，重み付け行列$W in R^{nW times N}$を次のように定める： . Wwi=exp⁡(−Dwi2/σ)∑k=1Nexp⁡(−Dwk2/σ). begin{aligned} W_{wi}= frac{ exp(-{D_{wi}{}^2}/ sigma)}{ sum_{k=1}^{N} exp(-{D_{wk}{}^2}/ sigma)}. end{aligned}Wwi​=∑k=1N​exp(−Dwk​2/σ)exp(−Dwi​2/σ)​.​ . $ sigma$は距離行列$D$の標準偏差である．重み付け平均は . τi(1)=∑w∈WSVwiWwi begin{aligned} tau_i{}^{(1)}= sum_{w in text{WS}}V_{wi}W_{wi} end{aligned}τi​(1)=w∈WS∑​Vwi​Wwi​​ . と計算される．これによって，順次$ boldsymbol{ tau^{(0)}}, boldsymbol{ tau^{(1)}}, ldots$が得られ，最終的にpseudotime $ boldsymbol{ tau}$に収束する． . 最終状態 . waypointを結んだようなグラフ$G’_E in G_E$ を考える．$G’_E$ は無向グラフであるが，pseudotime $ boldsymbol{ tau}$を使って有向グラフ$G_D in boldsymbol{R}^{N times N}$を定義する： . GDij={GE′ijτi&lt;τj;τi&gt;τj, τi−τj&lt;σi0τi&gt;τj, τi−τj&gt;σi. begin{aligned} G_D{} _ {ij}= begin{cases} G&amp;#x27;_E{} _ {ij} quad &amp; tau_i&lt; tau_j; &amp; tau_i&gt; tau_j, , tau_i- tau_j&lt; sigma_i 0 quad &amp; tau_i&gt; tau_j, , tau_i- tau_j&gt; sigma_i end{cases}. end{aligned}GD​ij​=⎩⎪⎪⎨⎪⎪⎧​GE′​ij​0​τi​&lt;τj​;τi​&gt;τj​,τi​−τj​&lt;σi​τi​&gt;τj​,τi​−τj​&gt;σi​​.​ . 次に，$(1)$で使ったkernelを使って，affinity行列$Z in boldsymbol{R}^{nW times nW}$を作る．さらに，これを . Pij=Zij∑kZik begin{aligned} P_{ij}= frac{Z_{ij}}{ sum_kZ_{ik}} end{aligned}Pij​=∑k​Zik​Zij​​​ . によってMarkov過程の遷移確率行列$P$にする．$P_{ij}$は細胞が１回の遷移で状態$i$から状態$j$に到達する確率である． . 最終状態は，それ以上分化しない状態なので，最終状態の集合$ text{TS}$が与えられれば，Markov過程$P$から， . Aij=0 (i∈TS,j=1,…,nW) begin{aligned} A_{ij}=0 (i in text{TS},j=1, ldots,nW) end{aligned}Aij​=0 (i∈TS,j=1,…,nW)​ . によって吸収的Markov過程$A$を定義できる． . Markov過程の定常分布$ boldsymbol{ pi}$があるとする： $ boldsymbol{ pi}=P* boldsymbol{ pi}$．さらに，平均絶対偏差は次のように計算される： . sc=Median(∣πi−Median(π)∣). begin{aligned} text{sc}= text{Median}( mid pi_i- text{Median}( boldsymbol{ pi}) mid). end{aligned}sc=Median(∣πi​−Median(π)∣).​ . これを使って，この分布における外れ値を次のように計算する： . TScands={i∣πi&gt;Gppf(0.9999,Median(π),sc)}. begin{aligned} text{TS}^ text{cands}= {i mid pi_i&gt; text{Gppf}(0.9999, text{Median}( boldsymbol{ pi}), text{sc}) }. end{aligned}TScands={i∣πi​&gt;Gppf(0.9999,Median(π),sc)}.​ . ただし，$ text{Gppf}(p, mu, sigma^2)$は平均$ mu$，分散$ sigma^2$の正規分布関数での$p$パーセント点である．この$ text{TS}^ text{cands}$のうち，diffusion componentの端点であるものを最終状態とする： . TS=⋂(TScands,C). begin{aligned} text{TS}= bigcap( text{TS}^ text{cands},C). end{aligned}TS=⋂(TScands,C).​ . 分化ポテンシャル . Markov過程でのランダムウォークにより，細胞が辿る経路が分かる．それぞれの細胞について，吸収的最終状態$b$に到達する確率を表す分岐確率ベクトル$ boldsymbol{B} _ i$を求める．吸収的Markov過程Aは次のように表すことができる： . A=(QR0E) begin{aligned} A= begin{pmatrix} Q &amp; R 0 &amp; E end{pmatrix} end{aligned}A=(Q0​RE​)​ . $Q$は中間状態での遷移確率を表す$(nW-b) times(nW-b)$行列，$R$は中間状態から最終状態への遷移確率を表す$(nW-b) times b$行列，$E$は単位行列である．fundamental行列$F$を次のように定義する： . F∗(E−Q)=E,F=(E−Q)−1. begin{aligned} F*(E-Q)=E, quad F=(E-Q)^{-1}. end{aligned}F∗(E−Q)=E,F=(E−Q)−1.​ . $Q_{ij}$は中間状態$i$から中間状態$j$に遷移する確率を表すので，$F_{ij}$は中間状態$j$から（いつか）中間状態$i$に到達する確率を表す．次に，分化確率を . B=F∗R begin{aligned} B=F*R end{aligned}B=F∗R​ . によって計算することができる．$R _ {ij} $ は中間状態$i$から最終状態$j$（$nW-b+j$番目の状態）を表すので，$B_{ij}= sum_kF_{ki}R_{kj}$は中間状態$i$から（いつか）最終状態$j$に到達する確率を表わしている． . $ boldsymbol{B} _ {i}$ は $ sum _ j B _ {ij}=1$ となる多項分布である．また，最終状態から最終状態への分岐確率は，明らかに . Bij={1i=j0i≠j begin{aligned} B_{ij}= begin{cases} 1 quad&amp; i=j 0 quad&amp; i neq j end{cases} end{aligned}Bij​={10​i=ji​=j​​ . となる．これはwaypointの分岐確率なので，$W$がwaypointによる全細胞への投影の重み付けであったことを思い出せば，全細胞に関する遷移確率 . Bij=∑w∈WSBwjWwi begin{aligned} B_{ij}= sum_{w in text{WS}}B_{wj}W_{wi} end{aligned}Bij​=w∈WS∑​Bwj​Wwi​​ . が得られる． . 分岐確率ベクトル$ boldsymbol{B} _ i$のエントロピー . Si=−∑jBijlog⁡Bij begin{aligned} S_i=- sum_jB_{ij} log B_{ij} end{aligned}Si​=−j∑​Bij​logBij​​ . を各状態での分化ポテンシャルとする． .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/single-cell-analysis/2019/09/19/palantir.html",
            "relUrl": "/single-cell-analysis/2019/09/19/palantir.html",
            "date": " • Sep 19, 2019"
        }
        
    
  
    
        ,"post18": {
            "title": "scEpathの解説",
            "content": "scEpathの論文のメソッドのざっくりした解説，というかメモ．エネルギー最高のクラスタから単純にエネルギーが下がっていくって方針を取っているので，複数の経路を再現するとかはできない．悲しみ． . scEnergyの導入 . 遺伝子遺伝子間相互作用gene-gene interactionのネットワークを作る．つまり，遺伝子が$n$あるとして，そのグラフを考える．$i$番目の遺伝子と$k$番目の遺伝子が繋がっていれば$a_{ik}=1$，そうでなければ$a_{ik}=0$とする（隣接行列adjacency matrix）．詳しくは，$m$個の細胞での$i$番目の遺伝子と$k$番目の遺伝子の発現をそれぞれ，$x_i=(x_{i1}, ldots,x_{im}),x_k=(x_{k1}, ldots,x_{km})$とする．この時， . aik={1(∣cor(xi,xk)∣&gt;τ)0(∣cor(xi,xk)∣≤τ).(1) begin{aligned} a_{ik}= begin{cases} 1 &amp; ( mid text{cor}(x_i,x_k) mid&gt; tau) 0 &amp; ( mid text{cor}(x_i,x_k) mid leq tau) end{cases}. end{aligned} tag{1}aik​={10​(∣cor(xi​,xk​)∣&gt;τ)(∣cor(xi​,xk​)∣≤τ)​.​(1) . カノニカル分布（正準分布）にいついて．系がエネルギー$E_i (i=0,1, ldots)$を取るとき，系のエネルギーが$E_j$である確率$p_j$は . pj=exp⁡(−βEj)∑jexp⁡(−βEj) begin{aligned} p_j= frac{ exp(- beta E_j)}{ sum_{j} exp(- beta E_j)} end{aligned}pj​=∑j​exp(−βEj​)exp(−βEj​)​​ . で与えられる．特に，分母の$ sum_j exp(- beta E_j)$を分配関数$Z$と呼ぶ． . scEpathでは$ beta=1$としている 1．つまり，遺伝子発現が$y$である細胞が状態$j$にある確率は，細胞が取るステージの総数，もっと言えば，分化していくときに異種とみなす細胞の数を$m$とすれば， . pj(y)=exp⁡(Ej(y))∑j=1mexp⁡(−Ej(y))(2) begin{aligned} p_j(y)= frac{ exp(E_j(y))}{ sum_{j=1}^{m} exp(-E_j(y))} end{aligned} tag{2}pj​(y)=∑j=1m​exp(−Ej​(y))exp(Ej​(y))​​(2) . となる．ステージは適当なunsupervised clusteringでのclusterなどを使う．この場合，$m$はクラスターの数． . $E_j(y)$（遺伝子発現が$y$である$j$番目の細胞のエネルギー）については，状態$j$の細胞での$i$番目の遺伝子の発現レベルを$x_{ij}$として，標準化発現レベル$y_{ij}$を . yij=xij−xjminxjmax−xjmin begin{aligned} y_{ij}= frac{x_{ij}-x_j^ text{min}}{x_j^ text{max}-x_j^ text{min}} end{aligned}yij​=xjmax​−xjmin​xij​−xjmin​​​ . で定義する．ただし，$x_j^ text{max},x_j^ text{min}$は$j$番目の細胞の中にある全遺伝子の発現レベルのうちの最大値，最小値である． . $E_j(y)$（遺伝子発現が$y$である$j$番目の細胞のエネルギー）については，$j$番目の細胞での$i$番目の遺伝子の標準化発現レベルを$y_{ij} (0 leq y_{ij} leq1)$を使って， . Ej(y)=∑inEij(y)=−∑inyijln⁡yij∑k∈N(i)ykj. begin{aligned} E_j(y)= sum_i^nE_{ij}(y)=- sum_i^ny_{ij} ln frac{y_{ij}}{ sum_{k in N(i)}y_{kj}}. end{aligned}Ej​(y)=i∑n​Eij​(y)=−i∑n​yij​ln∑k∈N(i)​ykj​yij​​.​ . $N(i)$は，$(1)$で考えたネットワークで，$i$番目の遺伝子の隣のある遺伝子の集合．さらに，これを標準化した . Ej^(y)=(Ej(y)/Eˉ(y))21+(Ej(y)/Eˉ(y))2 begin{aligned} hat{E_j}(y)= frac{(E_j(y)/ bar{E}(y))^2}{1+(E_j(y)/ bar{E}(y))^2} end{aligned}Ej​^​(y)=1+(Ej​(y)/Eˉ(y))2(Ej​(y)/Eˉ(y))2​​ . を使う．ただし，$ bar{E}(y)$は，全細胞（仮に$m’$個とする）についての$E_j(y)$の平均である： . Eˉ(y)=1m′∑j=1m′Ej(y). begin{aligned} bar{E}(y)= frac{1}{m&amp;#x27;} sum_{j=1}^{m&amp;#x27;}E_j(y). end{aligned}Eˉ(y)=m′1​j=1∑m′​Ej​(y).​ . メタ細胞 . 各クラスタ（主成分分析を行い，２成分からなっている）2でクラスタのエネルギーの$ theta_1=80$%を占める細胞たちをメタ細胞metacellとして定義する．クラスタの数を$N$個とすれば，それぞれのメタ細胞のエネルギー$E_k^ text{M}$は，トリム平均trimeanを取る（外れ値を弾くことができる）．すなわち，$k$番目のメタ細胞のエネルギーの四部位数を下から$Q_1,Q_2,Q_3$として， . EkM=12(Q2+Q1+Q32). begin{aligned} E_k^ text{M}= frac{1}{2} left(Q_2+ frac{Q_1+Q_3}{2} right). end{aligned}EkM​=21​(Q2​+2Q1​+Q3​​).​ . $(2)$と同様に3すれば，ある細胞が$k$番目のメタ細胞にある確率 $p_k^ text{M}$は， . pkM=exp⁡(−EkM)∑j=1Nexp⁡(−EjM) begin{aligned} p_k^ text{M}= frac{ exp(-E_k^ text{M})}{ sum_{j=1}^N exp(-E_j^ text{M})} end{aligned}pkM​=∑j=1N​exp(−EjM​)exp(−EkM​)​​ . で与えられる．ここで，$k$番目のメタ細胞に属する細胞のエネルギーは大体，$E_k^ text{M}$だと仮定している4．もちろん，メタ細胞，つまり，ある細胞が取りうる状態は$N$個あるので分配関数は$N$個の和になっている．また，ある細胞が$k$番目のメタ細胞に残る確率は$p_k^ text{M}$，それ以外のメタ細胞へ遷移する確率は$1-p_k^ text{M}$である． . Markov過程 . 時間的に定常なMarkov過程において，時刻が$1$経過して，$i$から$j$になる確率は， . pij=Pr⁡(Xn=j∣Xn−1=i) begin{aligned} p_{ij}= Pr(X_n=j mid X_{n-1}=i) end{aligned}pij​=Pr(Xn​=j∣Xn−1​=i)​ . で与えられる．ここで，あるベクトル$ boldsymbol{ pi}$を考える．$ boldsymbol{ pi}$が . πj=∑iπipij,∑jπj=1 begin{aligned} pi_j= sum_i pi_ip_{ij}, quad sum_j pi_j=1 end{aligned}πj​=i∑​πi​pij​,j∑​πj​=1​ . を満たす時，$ boldsymbol{ pi}$を定常分布stationary distributionと呼ぶ． . 主成分分析で得られた２次元空間で，$k$番目のメタ細胞から$l$番目のメタ細胞に遷移する確率は，これらの距離に反比例すると仮定する．メタ細胞間の隣接行列を，距離によって次のように定義する： . Gkl=exp⁡(−∥zk−zl∥24ε2). begin{aligned} G_{kl}= exp left(- frac{ |z_k-z_l |^2}{4 varepsilon^2} right). end{aligned}Gkl​=exp(−4ε2∥zk​−zl​∥2​).​ . ただし，$z_k$は２次元空間におけるメタ細胞の中心，$ varepsilon$はメタ細胞間の距離の最大値である．$G_{kl}$を規格化すれば遷移行列が得られる： . G~klasym=Gkl∑j=1NGkj. begin{aligned} tilde{G}_{kl}^ text{asym}= frac{G_{kl}}{ sum_{j=1}^NG_{kj}}. end{aligned}G~klasym​=∑j=1N​Gkj​Gkl​​.​ . 遷移行列$ tilde{G}_{kl}^ text{asym}$は非対称であるが，次に示す定常状態$ boldsymbol{ pi}_k (1 leq k leq N)$を持つ： . πk=∑i=1NGkj∑k=1N∑j=1NGkj. begin{aligned} boldsymbol{ pi}_k= frac{ sum_{i=1}^NG_{kj}}{ sum_{k=1}^N sum_{j=1}^NG_{kj}}. end{aligned}πk​=∑k=1N​∑j=1N​Gkj​∑i=1N​Gkj​​.​ . また，$G_{kl}$が対称であることに注意すれば，容易に分かるように， . πkG~klasym=πlG~lkasym begin{aligned} pi_k tilde{G}_{kl}^ text{asym}= pi_l tilde{G}_{lk}^ text{asym} end{aligned}πk​G~klasym​=πl​G~lkasym​​ . が成立する．よって， . G~klsym=πkπlG~klasym begin{aligned} tilde{G}_{kl}^ text{sym}= sqrt{ frac{ pi_k}{ pi_l}} tilde{G}_{kl}^ text{asym} end{aligned}G~klsym​=πl​πk​​ . ​G~klasym​​ . を定義すれば，これは対称な遷移行列となる． . 遷移確率 . メタ細胞の項の最後にも述べたことに注意すると，$k$番目のメタ細胞から$l$番目のメタ細胞に遷移する確率行列$T$は， . Tkl={(1−pkM)G~klsymk≠lpkMk=l (no transition) begin{aligned} T_{kl}= begin{cases} (1-p_k^ text{M}) tilde{G}_{kl}^ text{sym} &amp; k neq l p_k^ text{M} &amp; k=l ( text{no transition}) end{cases} end{aligned}Tkl​={(1−pkM​)G~klsym​pkM​​k​=lk=l (no transition)​​ . となる． . 遷移確率行列$T$は２つのメタ細胞間がお互いに遷移する確率($T_{kl},T_{lk}$)を含む．つまり，この状態ではメタ細胞の集合は双方向性のあるグラフになる．しかし，実際に細胞が時間発展していく場合はエネルギーが減少する経路のみを取るので，グラフは一方向性なはずである．よって，Wilcoxonの順位和検定によって，２つのメタ細胞を比較する．この場合の帰無仮説$ text{H}_0$は２つのメタ細胞間のエネルギー（のトリム平均）に差がないことである．scEpathでは，p値が$ alpha=0.01$以下の場合は帰無仮説が棄却される．これらのことを考慮すれば，向き付けを考慮したメタ細胞のグラフの遷移確率行列$W$は . Wkl={(1−pkM)G~klsymk≠l,p&lt;α,EkM&gt;ElM;k≠l,p≥αpkMk=l (no transition) begin{aligned} W_{kl}= begin{cases} (1-p_k^ text{M}) tilde{G}_{kl}^ text{sym} &amp; k neq l, p&lt; alpha,E_k^ text{M}&gt;E_l^ text{M}; &amp; k neq l,p geq alpha p_k^ text{M} &amp; k=l ( text{no transition}) end{cases} end{aligned}Wkl​=⎩⎪⎪⎨⎪⎪⎧​(1−pkM​)G~klsym​pkM​​k​=l,p&lt;α,EkM​&gt;ElM​;k​=l,p≥αk=l (no transition)​​ . となる．$p$は$k$番目と$l$番目のメタ細胞を検定した時の$p$値．帰無仮説が採用された場合は，$k$番目と$l$番目のメタ細胞のエッジは双方向性となる．これによって，メタ細胞をノードとするグラフはエネルギーが増加しない遷移のみを許す有向グラフとなる．その遷移確率行列は$W$である． . scEpathを求める . 細胞の実際の経路を求める場合は，最もエネルギーが高いメタ細胞から，遷移確率が最も高いような経路を辿ることになる．これは最もエネルギーが高いメタ細胞を根とし，各エッジの重みを$1-W$として構成した最小全域木minimum directed spanning tree (MDST)を見つけるのに等価である． . メタ細胞の中心をあらかじめ求めておき，その中心から$ theta_2=0.75$分位数より内側にある細胞をコア細胞とする．そして，先程求めた経路をコア細胞に限定してprincipal curveでフィッティングし，それを$[0,1]$にスケール変換する．そして，psudotime reconstruction score (PRS)を，他のデータを使って次のように定める： . PRS=c−c′c+c′. begin{aligned} text{PRS}= frac{c-c&amp;#x27;}{c+c&amp;#x27;}. end{aligned}PRS=c+c′c−c′​.​ . ただし，$c$は一致した細胞の数，$c^ prime$は一致しなかった細胞の数である． . pseudotime依存的なTF . pseudotime依存的な遺伝子を見つけるため，pseudotimeを10個に分割する．さらに，各部分での遺伝子発現のトリム平均を取る．各遺伝子の発現の平均を３次スプライン曲線で滑らかにする．細胞の順番をランダムに並び替えたものと比べて，標準偏差が$0.5$以上，Bonferroni調節p値が$0.01$以下のものをpseudotime依存的な遺伝子とする． . 既存の転写因子(TF)の中から，pseudotime依存的な遺伝子を探し，細胞系列で発言が有意に変化している（系列に含まれるクラスター間で比べた際，Bonferroni調節p値が$0.01$以下，少なくとも閾値倍以上の変化がある）ものを，pseudotime依存的なTFとする． . さらに，TFのネットワークを構成し，Spearmanの順位相関係数を使ってp値が$10^{-5}$以下のものは相関がないとし，そうでなければ，相関があるとして結ぶ．この操作を$90$%の細胞で1000回繰り返し，どの操作でも結ばれたTFは，真に関係があると結論づける． . . 逆温度$ beta$を必ずしも$1$にする必要はない？ &#8617; . | これはunsupervised clusteringとかで得られた結果から行う？ &#8617; . | 正準分布の考え方．つまり，ある細胞が$k$番目のメタ細胞に属する，と言う状態になる確率を議論している． &#8617; . | この妥当性を検証するのは面白いかも &#8617; . |",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/single-cell-analysis/2019/09/14/scepath.html",
            "relUrl": "/single-cell-analysis/2019/09/14/scepath.html",
            "date": " • Sep 14, 2019"
        }
        
    
  
    
        ,"post19": {
            "title": "macOS Xが起動しない（FFFFFFFF-FFFF-FFFF-FFFF-FFFFFFFFFFFF）",
            "content": "linux入れてたパーティション消したら，macのパーティションが壊れた . 表題の通りなのですが，優曇華院はmacにmac OS Xとlinuxをデュアルブートしていました．SSD (APPLE SSD SM0128G Media)の中にMacintosh HD（mac OSのための領域，約100GB）とLinux HD（Linuxのための領域，約20GB）のパーティションを作成していました．ある日，「もうLinuxいらねえな」となって，ディスクユーティリティからLinux HDを削除したら，なんか知りませんがエラーが出ました（MS-DOS (FAT)にしてたせい？）．しかも，Macintosh HDは100GBのままです． . パーティションでミスったらヤバい，みたいな話を聞いたことがあるので，嫌な予感はしました．再起動をかけたら案の定macがお陀仏になりました． . 状況 . 起動すると，GNU GRUBが起動する | Macのstartup manager（起動時にoption）を使っても，EFI bootしか表示されない（今まではMacintosh HDが表示されていた） | リカバリーモード（起動時にcmd+R）は，変な地球儀が出た後に，食べかけのリンゴが出て，起動する | セーフモード（起動時にshift）は使えない．GNU GRUBが起動する． | リカバリーモードでディスクユーティリティ使うと，「内蔵」の項目には変なのが表示される（普通ならMacintosh HD），特に何も操作できない． | . 解決策 . リカバリーモードのまま進めます．まずは，ターミナルを開いて， . gpt -r show disk0 . でdisk0の中身を表示します．結果はこんな感じでした： . start size index contents （略） 409640 197530544 2 GPT part - FFFFFFFF-FFFF-FFFF-FFFF-FFFFFFFFFFFF . （もしdisk0がないとか言われたら，こんなのが出るまでdisk1, disk2,…って頑張ってください．）上で表示されている，FFFFFFFF-FFFF-FFFF-FFFF-FFFFFFFFFFFFのstart, size, indexの値をメモってください．優曇華院の場合はそれぞれ，409640, 197530544, 2です． . 次に，一旦ディスクのマウントを解除します： . diskutil umountDisk disk0 . 次に，FFF…を一旦消します： . gpt remove -i [index] /dev/disk0 . 同じパーティションを正しい名前で追加します： . gpt add -i [index] -b [start] -s [size] -t [GUID] /dev/disk0 . GUIDはAPFSコンテナの7C3457EF-0000-11AA-AA11-00306543ECACを使います． . これで終わり．あとはリブートしてstartup managerからMacintosh HDを選べばok普通に起動できます． . まとめ . Time machineとかでバックアップ取っていれば，FFF…を消去してから，もっかいMacintosh HDを切って，OSをインストールすればいい | OSの再インストールをしようとしたが，そもそもディスクを認識していないのでできるはずもなかった | なんで名前変わったかは分からない | 消滅したLinuxパーティションの在処は現在探しています | Linuxが消えたのかは不明 | デュアルブートしたら戻すのが大変だぞ！仮想マシンにしとけ！ | . 追記 . Linux HDは消えて，物理ボリューム（SSD）の中身はMacintosh HDと「空き領域」になっていました | 空き領域を削除して，Macintosh HDを128GBに戻せました | Linuxは消滅しました | .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/macos/2019/09/02/mac_boot_error.html",
            "relUrl": "/macos/2019/09/02/mac_boot_error.html",
            "date": " • Sep 2, 2019"
        }
        
    
  
    
        ,"post20": {
            "title": "如何にして物理学を学ぶか",
            "content": "５セメスターのテストもいよいよ明日で終わりです．最近は惰性でテスト対策しているだけの優曇華院です．教科書を読んでいた頃の自分は何処かに行ってしまったようです． . タイトルに示した話をする前に，少しばかり身の上話をします．優曇華院は医学部に通っていますが，医者になるつもりは今の所ありません．「なぜそんな奴が医学部に入ったんだ」とよく訊かれます．少なくとも実際に医学部で医学教育が始まるまでは医師として他人を救う行為は尊いもので（今でもこの考えは持っていますが），当然ながら自分もその行為をするものだと思っていました．ですが，現在では「医学は事実の詰め込みだけで応用性が無いし，それを教える医学部とその構造はもっとクソ」などと不適切発言ばかりしています．いつからこう感じるようになってしまったのか，自分でも分かりません．医学部をやめて，他の学部に入った方が楽だろうと感じたこともありましたが，結局転学部することはなくここまで来ました．惰性といえば惰性なのかもしれませんが，もしかすると心の何処かでやはり医者という尊い職に何か惹かれる物があるのかもしれません． . 医学部はおそらく，専門科目以外のことに割ける時間はかなり少ない方だと思います．その中で何をするかは人それぞれでしょう．例えば，研究をする人，音楽をする人，ともすると，過去にやった医学の復習をする人もいるかもしれません．優曇華院は，そのような時間で物理学をすることを選びました．高校生の頃から，物理は得意科目だったし，理論物理学にも興味がありました． . 物理学の学び方 . 教科書の読み方 . 物理学は科学の中の一つです．科学は万人が共通の理解を得られるよう作られています．では，理解するためにはどうすればいいでしょうか．まずは当然，何かしらの書籍を読むことになります．物理の教科書は，数式が至る所にあり，非常に計算が煩雑だったりすることがあります．この時，よく言われるのが，「計算に夢中になるのではなく，しっかりと式の意味を吟味しろ．」仰る通りです．物理学における数学は，自然現象を記述するための言語であり，全ての数式は物理の文脈で意味を持っています．数式の意味を理解しようとせずに本を読めば，なんとなく計算が出来た，と言うだけで終わってしまうでしょう．では，ここで別の疑問が生じます：「式の意味を理解するのが重要なのであれば，途中の式変形は自分の手を動かさずにやってもいいのか？」これに関しては，YesでもありNoでもあると優曇華院は考えます．もし，純粋に物理学を趣味としてやるだけなら，計算過程をトレースする必要は無いでしょう．書かれた計算結果を見てその意味を考えることに時間を費やした方が有益です．しかしながら，もし，将来的に物理学に本格的に向かい合うのであれば，計算のトレースは必須と言えるでしょう．この操作は，本の理解には必須で無いかもしれませんが，自分で何か新しいことに応用したりするためには絶対に必要なスキルです．本を読むときに手を動かして計算しろ，と言うのは将来のための基礎訓練とも言えます．もちろん，自分で計算をすることによって理解が深まる，と言う側面もありますが． . 数学との兼ね合い . さて，物理学は数学を言語とする訳ですから，数学の理解は必須です．では，物理学徒は一体，どこまで数学を理解すればいいのでしょうか？「物理学徒は数学者では無いので，数学に対しストイックになる必要は無いし，数学に割ける時間は限られているので，出来るだけ効率よく，最小限の努力で身に付けたい」と言った意見も多いでしょうし，優曇華院も昔はこう思っていました．しかしながら，やはり数学書を使って勉強することを推奨します．数学書は定義，定理，補題，証明，例のオンパレードで，抽象的なものも数多く存在し，読みこなすのはかなり骨が折れます．数学書を使って勉強した人と，何となくで数学を体感した人とでは，その理解に雲泥の差があるのは明白で，物理学の高度な分野をこなすには，曖昧な理解では全くもって太刀打ち出来ません．例えば，相対性理論の初歩はテンソルが何奴か分からなくても，添字の計算をしていれば理解は出来ますが，時空多様体の話などになってくると，数学に精通していなければ厳しいでしょう．また，数学の理解が甘ければ，物理の勉強の最中に，数学的な事項で足止めをくらってしまい，全体像を見失ってしまうことすらあるかもしれません．結局のところ，数学書を読んで「ドーピング」しておけば物理の理解に集中できると言う感じです． . 精神論のようなもの . 物理を勉強するにはもちろん．教科書を読み，記述言語である数学を理解することは必須です．では，それ以外に何がいるでしょうか？有名な話で，ヴェイユとセール（20世紀を代表する数学者）が日本にやって来た時，中禅寺湖で突然寒中水泳を始め，最後に「数学は体力だよ」と言ったそうです．研究者レベルになろうと思うと，数学ほど大変な科目は無いでしょう．勉強すべき事項が他の学問の比になりません．そのような物に耐え，貪欲に学習する姿勢こそが「体力」と言えるでしょう（精神的，肉体的と言う点で論理の飛躍がありますが）． . では物理学はどうでしょうか．物理学も，ある程度の数学的バックグラウンドを身に付けてからがスタートラインと言う主義なら，同様に「体力」が必要でしょう：「物理は体力だよ．」もちろん心身の健康の維持のために，運動は大事です．もしかすると，身体を動かした方が柔軟な思考が出来るのかもしれません． . 最後に . 時間があれば物理（もしくはバックグラウンドの数学，以下同様）を勉強しなければならない．物理の勉強が終わったから寝る，朝起きてから物理をやろう，と言う発想ではまだ甘い．物理のことを考えていたらいつの間にか寝ていて，物理のことを考えていたらいつの間にか目が覚める，というレベルでなければいけない． . （自戒も込め）この標語をもって，この駄文を結びたいと思います． . 余談 . 数学やるのは「ドーピング」，つまり物理をやる上で楽になるから，的なモチベーションでしたが，最近は純粋に数学をやるのが楽しいです． .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/essay/2019/07/31/how_to_learn_physics.html",
            "relUrl": "/essay/2019/07/31/how_to_learn_physics.html",
            "date": " • Jul 31, 2019"
        }
        
    
  
    
        ,"post21": {
            "title": "Sorted plotの注意点",
            "content": "話題元はこのツイート . PUBLIC SERVICE ANNOUNCEMENT: You must cross-validate sorted plots!! E.g. peak-latency sorted colormaps of neural activity. Otherwise you WILL find &quot;sequences&quot; out of random noise. Don&#39;t believe me? A proof in four lines. Anyone feel like you have seen the last panel before? pic.twitter.com/Fbb4jLKORE . &mdash; Nick Steinmetz (@SteinmetzNeuro) July 16, 2019 ニューロンの活動を取ってきて並べて表示する場合、ピーク潜時(peak latency)で昇順に並び替え(sort)する場合がある。もちろん主張とは関係のない単なるplotの場合もあるが、その並びに意味があるということを示すためにはちゃんと交差検証して有意かを調べなくてはならない。 . というのもsorted plotをすると見る人に与える印象はかなり変わってしまうためである。元ではMATLABだが、全く同じ簡単な実験をPythonで書き直してみる。 . プロットの形式による印象の変化 . 実験としては . bin内のスパイク数を表す(という設定の)行列をランダム(～正規分布)に作成。 | ニューロンごとに活動をガウシアンフィルタで平滑化 | ピーク潜時に応じて列を並び替え | plotのカラーマップをjetに変更 | 用いる手順としては他にピーク発火率で正規化(normalization)するというのも入れる場合もあるが、今回は省略。 . 結果は下図。左から1, 2, 3, 4となっている。 . . 左端と右端が同じ神経活動を示しているとは思えないほどの変わり方である。不正とは言えないが、見る人に与える印象はとても変わってしまうということに注意しなければならない。 . import numpy as np from scipy import signal import matplotlib.pyplot as plt def GaussianKernel(size, alpha=2.5): x = np.arange(0, size, 1, float) sigma = (size - 1)/(2*alpha) mu = size // 2 return np.exp(-((x-mu)**2 / (2*sigma**2))) np.random.seed(seed=1) N = 100 dt = 1e-2 # sec T = 0.5 # sec nt = round(T/dt) # random (number of spikes in dt) z = np.random.randn(N, nt) # smoothing kernel = np.expand_dims(GaussianKernel(13), 0) zs = signal.convolve2d(z, kernel, &quot;same&quot;) # sorted plot max_idx = np.argmax(zs, axis=1) zs_sorted = zs[np.argsort(max_idx)] plt.figure(figsize=(10, 4)) plt.subplot(1,4,1) plt.imshow(z) plt.title(&#39;Random numbers&#39;) plt.xlabel(&#39;Time (s)&#39;) plt.ylabel(&#39;Neuron #&#39;) plt.subplot(1,4,2) plt.imshow(zs) plt.title(&#39;Smoothed with gaussian&#39;) plt.xlabel(&#39;Time (s)&#39;) plt.subplot(1,4,3) plt.imshow(zs_sorted) plt.title(&#39;Sorted by peak time&#39;) plt.xlabel(&#39;Time (s)&#39;) plt.subplot(1,4,4) plt.imshow(zs_sorted, cmap=&quot;jet&quot;) plt.title(&#39;Sorted by peak time (jet)&#39;) plt.xlabel(&#39;Time (s)&#39;) plt.tight_layout() #plt.savefig(&quot;sorted_plot.png&quot;) plt.show() . smoothingをする場合はkernelを(1, ksize)の行列としてscipyのconvolve2dを用いると速い。 . まとめ . このsorted plotの問題はtime cellsの解析において大事なことである。発火潜時で並び替えるとランダムな活動であっても何か意味があるような（＝連鎖的発火が起こっているような）気がしてしまう。そのため適切に統計処理を行わなくてはならない。 . 関連する話題 . correlation - What happens if the explanatory and response variables are sorted independently before regression? - Cross Validated | .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/neuroscience/2019/07/16/sorted_plot.html",
            "relUrl": "/neuroscience/2019/07/16/sorted_plot.html",
            "date": " • Jul 16, 2019"
        }
        
    
  
    
        ,"post22": {
            "title": "鬱な気分の時にはラヴェルを聴きましょう",
            "content": "お久しぶりです。とっても久しぶりの投稿ですがそんなの気にしません、みこです。さて、伝えたいことは表題の通りで特にこれ以上述べる必要はないのですが、こんなナゾ理系サイトに行き着いた人の中で、もしも音楽バカの奇天烈をご覧になりたい方がいらっしゃいましたら、少しばかしお付き合いください。 . 音楽で感情をコントロールする . みなさん誰しも辛いこと悲しいことを抱えながらも、死なない程度にはそれなりに楽しく世の中をお過ごしになっていることと存じます。自分も大学生になってからというものの、人生的になかなか面白い経験をし、この世の終わりみたいな感情になりました。時間が経った今では落ち着いてきましたけども、ずっと引きずったままでいます。そんなとき、友人たちの話を聞いていて気づいたことというのは、誰だって同じようにこの世の終わりみたいなのを経験している、ということです。愕然としながらも、おかげさまで人生に希望を持つことができました。あなたの友達も、電車で乗り合わせた人たちも、普通にしている仮面の奥では死にたい死にたい考えてるなんて面白くないですか？ . まあ、みんながそうだとしても今のあなたの苦しみが軽くなるわけではありません。どうやって乗り越えるか、乗り越えたかは人それぞれあるでしょうが、ここで話題にしたいのは当然音楽です。さて、あなたはどんな音楽を聴いて乗り越えますか？ . 音楽を聴いて感情をコントロールすることは可能です。落ち込んでいる時に、アップテンポのBGMを聴く人は多いかと思います。８ビートは心臓の鼓動と一致するから興奮すると言われたりしますね。勉強に集中したい時に、静かなピアノ音楽を聴く人もいるかと思います。仕事場のBGMを会社が設定して、効率の向上を図る会社もあるらしいですね。ある雰囲気を作りたい時に流す音楽のジャンルはおおよそ決まっていますから、そのセットに慣れた人たちはその逆も起こり得ます。 . では鬱を乗り越えるには気分を上げるためにロックでも聴けばいいのでしょうか？ここからは根拠のない個人の考えなのでご注意ください。答えはノーです。程度にもよるのでしょうが、人生で最悪の気分の時に激しい音楽を聴くだけで気分が回復するはずがありません。雰囲気に流されるだけでなんとかなるなら困ることはないでしょう。どん底に沈んだ状態では、そもそもそういった激しく溌剌とした明るい空気を一切受け付けないはずです。共感していただける疑問ですが、感情を持っている自分を理解するのが不可能とも思える感覚というのを経験したことはありませんか？死にたくなるのを引き止めているのはただ理性的な判断のみという不安定さを経験したことはありませんか？こうなってしまうと、もう気分は変えられません。できることは忘れることだけです。 . なぜラヴェルを聴くべきか . ラヴェルの音楽は人工的な美の極致と称されることが多いです。実際、僕自身もそう思います。聴いて盛り上がるとか、悲しい気分になるとかとは正反対にある音楽だと感じます。後に残すのはただただ純粋な感動と心の平静です。つまり、ラヴェルを聴くのに感情は不要です。僕がどんなときにラヴェルを聴くかというと、だいたいいつも聴いているのですが、通学中や病気のときはいいタイミングです。ひとりでいて、何かできるわけでもなく考えることもない状況です。 . 繰り返しますが、鬱な気分を変えることはできません。変えたいなら時間が経つのを待つか向精神薬でも使ってください。その感情を直視しても、ただ悪循環に陥るだけです。それを回避するためのラヴェルです。感情の面から攻めても無意味なのです。 . ラヴェルを聴く時に鬱だったら、まるでその感情がラヴェルの音楽と一体であるかのような感覚を覚えます。こればっかりは実際に聴いてくれとしか言えないのですが、そんな気分になるんです。感情という感情が消え去って、真にラヴェルを理解する資格を得たのです。まさに、音楽を聴く姿勢と音楽が合わさって一つの芸術作品となっているのです。このとき、心の中の、鬱になって死にたいとかいう部分は音楽に吸収されてしまって、頭の中で巡っている思考は、ただ音楽を鑑賞する審美眼に支配されているのです。精神が美で満たされたとき幸福を感じているはずです。 . さあ、これで忘れることができました。見返してみても意味不明ですが、精神科医になるつもりもないし、どうだっていいでしょう。おすすめの曲は『序奏とアレグロ』、『ダフニスとクロエ』（第２組曲が有名ですがここでは第１組曲を薦めます）です。 .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/essay/2019/06/04/ravel.html",
            "relUrl": "/essay/2019/06/04/ravel.html",
            "date": " • Jun 4, 2019"
        }
        
    
  
    
        ,"post23": {
            "title": "ラチェット婦長",
            "content": "人に指示されることがあまり好きではない。自分がやられて嫌なことは人にもしてはいけないと思っているので、当然、人に指示することに対してもあまり気が進まない。ただ、医師という職業を選んだ以上、他人に何かを強いる、ということは避けて通れない。生涯気持ちよく職務を遂行するためには、指示や強制に対する自分の（そしておそらく相手の）嫌悪感の正体を突き止める必要がある。人に何かを指示しているとき、あるいは指示されているとき、果たして人は何を考えているのだろうか？ . 「カッコーの巣の上で」における看護婦ラチェット . 「カッコーの巣の上で」（1975年、ミロス・フォアマン監督）という映画を見たことがある。精神病棟を舞台としたこの作品には、極めて優秀な看護婦ラチェットが登場する。与えられた責務を確実にこなし、患者に対してはてきぱきと、的を射た指導を行う。医療関係者に就こうとしている学生から見ると、憧れのような存在だ。にもかかわらず、AFI（American Film Institute）が選ぶ「映画史に残る悪役」ランキングで、ルイーズ・フレッチャー演じるこの看護婦長は、白雪姫の邪悪な継母や、「ジョーズ」の人食いザメをはるかに上回る第５位にランクインしている (https://www.afi.com/100years/handv.aspx）。 . 皮肉なのは、白雪姫の継母とは違い、看護婦長には邪悪なマインドが全くないことである。それどころか、患者を自分の手で更正させるという使命感や、病院の運営に対する責任感に溢れている。例えば、恋人との関係が思わしくない患者には、グループセラピーとしてみんなの前で自分の経験を話させる。別の患者が、なぜ嫌がっているのに話させるのか、と問うと静かにこう答える―「話すことはストレスを軽減するための第一の方法です」。あるいは、音楽の音がもう少し静かだったら話がしやすいのに、と言った患者に対しては静かに諭す。「ここには耳の遠いお年寄りもおられます。音楽は治療の一環なので、音量を下げることは不公平に当たります」。徹頭徹尾、正論で固められた彼女の凛とした演技は、「お前を殺してやる」と牙をむく悪魔よりよほど恐ろしい。 . 「人を救う」ということ . 本当に人を救うとはどういうことなのだろうか。病院や学校といった人を教え導く場所では、患者や学生のためを思ってやっているはずのことが、必ずしも彼らに喜ばれるとは限らない。未熟な子供を扱う学校ならともかく、一個の大人を扱う病院では、指示や強制は人間の尊厳を傷つけてしまいかねない。悲しいことに、いくらその指示が医学的根拠に基づいた適切なものであっても、である。病院で行われる指導は、その人の長い人生に刻まれた生活習慣を根本から変えるものも多いので、お前に何がわかる、と言われたら答えようがない。 . 高校時代、国語の先生がこんな話をしてくれた。「私の祖父が75で大病を患ったとき、地元のある先生に見てもうたんや。祖父は酒が大好きで、糖尿病も高血圧もあったから、何を言われるかと思って行ったんやな。そしたら、その先生も酒が大好きで、診察中に大酒を煽りながらがははっと笑って言うわけや。『〇〇はん、あんたももう75や。今まで色んなこと我慢してきてしんどかったやろう。これからは酒も好きなだけ飲んで、煙草も好きなだけ吸うて、好きなことだけして死んでいったらええ』ってな。そんで、叔父は96まで生きたから、まあ名医っちゃ名医やわな」。この話を聞いたときは受験の真っ只中だったので、ふざけるな、学問をなめているのかとしか思えなかった。今、同じことを言われてそう思える自信はない。 . 医者にとって最も大事なことは、患者さんが何を求めているのかを正しく把握することなのだと思う。カルテ上の医学的な情報を追うのと同時に、この人はどんな生涯を送ってきてどんな思いで病院に来ているのだろうかと想像を巡らせることによって初めて、ニーズを的確に読みとることが可能になるのだろう。そのためには、陳腐な言い方だが、普段から人の心や人生を理解するよう努力することが重要なのだと思う。読書や映画、もしくは自分自身の社会経験が助けになるかもしれない。その上で、各治療法の負担や予後を学問的に勉強し、それを患者さんにしっかり伝えて検討を重ねていく、というのが医師としての理想的な態度なのだろう。 . 先に挙げた映画で、患者が結託して病院を抜け出し、魚釣りに行く場面がある。大自然の中に身を置いた彼らは、病院にいる時には全く見せない目をしている。本当にこれが精神病患者なのかと驚くほど希望に満ち、生気に溢れている。あの場面を見ると、病院があるから「患者」という概念が生まれるのではないか、患者を過度に不幸だと思わせているのは病院なのではないか、という思いが拭い去れない。人間ならば一度はしたことのあるはずのあの目をもう一度取り戻すことが、本来の病院の使命であり、医師の力量なのではないか。そう考えると、一度は優秀だと感じたラチェット婦長も、やはり力不足に思えてくるのである。 .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/essay/2019/06/04/nurse_ratched.html",
            "relUrl": "/essay/2019/06/04/nurse_ratched.html",
            "date": " • Jun 4, 2019"
        }
        
    
  
    
        ,"post24": {
            "title": "ネコLGN細胞のスパイクトリガー平均による受容野解析",
            "content": "ネコLGNのX細胞のスパイクトリガー平均(Spike triggered average; STA)による受容野解析をしてみます。データと設定はTheoretical Neuroscienceの演習問題(Chapter 2-3)より (神経活動の元データはP. Kara, P.Reinagel, &amp; R.C. Reid. (2000)より)。 . データとコードは https://github.com/takyamamoto/STA-for-catLGNcell で公開しています。動かしたい場合はリポジトリをCloneした後に sta_catLGN.pyを実行してください。 . &#12473;&#12497;&#12452;&#12463;&#12488;&#12522;&#12460;&#12540;&#24179;&#22343;&#12398;&#35336;&#31639; . 今回のスパイクトリガー平均はざっくり次のように求めます。 . ネコにホワイトノイズ(2D: 16 x 16)を見せ、その時のLGNのX細胞の神経活動を記録。 | X細胞のスパイクが生じるまでの200ms程度前までの視覚刺激を、時間ごとに加算。 | 今回のデータではビンが15.6 msで12ステップ前(= 187.2 ms)までの刺激に対して計算をします。ただし、スパイクのデータ(count)の1つのビンには1つ以上のスパイクが記録されており、スパイクの数に応じて刺激を重みづけ加算します。 . &#12521;&#12452;&#12502;&#12521;&#12522;&#12398;import . import numpy as np import matplotlib.pyplot as plt import scipy.io import scipy.ndimage from matplotlib import gridspec from tqdm import tqdm . Data&#12398;load&#12392;STA&#12398;&#35336;&#31639; . 以下はスパイクトリガー平均の計算部分のコードです。staが解析結果（12 x 16 x 16の配列）。 . # Load data data = scipy.io.loadmat(&#39;./data/lgn_sta_data/c2p3.mat&#39;) # stim : 16 x 16 images that were presented at the corresponding times. stim = data[&#39;stim&#39;] # (16, 16, 32767) # counts : vector containing the number of spikes in each 15.6 ms bin. counts = data[&#39;counts&#39;] counts = np.reshape(counts, (-1)) # (32767) &quot;&quot;&quot; Calculate the spike-triggered average images for each of the 12 time steps(= 187.2 ms) before each spike &quot;&quot;&quot; spike_timing = np.where(counts &gt; 0)[0] num_spikes = np.sum(counts &gt; 0) # number of spikes num_timesteps = 12 H, W = 16, 16 # height and width of stimulus image sta = np.zeros((H, W, num_timesteps)) #spike-triggered average for t in spike_timing: if t &gt; num_timesteps: sta += stim[:, :, t-num_timesteps:t]*counts[t] sta /= num_spikes . &#21463;&#23481;&#37326;&#12398;&#21487;&#35222;&#21270; . &#31354;&#38291;&#30340;&#21463;&#23481;&#37326; . 計算したSTAを用いて受容野の可視化を行います。全て表示すると長くなるので初めと最初だけ表示します。 . #collapse-hide for T in tqdm(range(num_timesteps)): if T in [0, num_timesteps-2, num_timesteps-1]: fig = plt.figure(figsize=(10,3)) fig.suptitle(&quot;The receptive field of a cat LGN X cell. (timesteps : &quot;+str(T)+&quot;)&quot;, fontsize=14) gs = gridspec.GridSpec(1, 3, width_ratios=[1, 1, 2], height_ratios=[1]) x = np.arange(-W//2, W//2+2, 2) y = np.arange(-H//2, H//2+2, 2) ax1 = plt.subplot(gs[0,0]) plt.imshow(sta[:,:,T]) plt.gca().set_aspect(&#39;equal&#39;) plt.gca().invert_yaxis() plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) ax1.title.set_text(&#39;heatmap&#39;) plt.xticks(list(np.arange(0, W+1, 2)-0.5), list(x)) plt.yticks(list(np.arange(0, H+1, 2)-0.5), list(y)) zoom = 3 smoothed_sta = scipy.ndimage.zoom(sta[:,:,T], zoom=zoom) # smoothed x = np.arange(-W//2*zoom, W//2*zoom) y = np.arange(-H//2*zoom, H//2*zoom) X, Y = np.meshgrid(x, y) ax2 = plt.subplot(gs[0,1]) plt.contour(X, Y, smoothed_sta, 25) plt.xlabel(&#39;x&#39;) ax2.title.set_text(&#39;contour&#39;) plt.gca().set_aspect(&#39;equal&#39;) ax3 = plt.subplot(gs[0,2], projection=&#39;3d&#39;) ax3.plot_surface(X, Y, smoothed_sta, shade=True, color=&#39;grey&#39;) ax3.set_xlabel(&#39;x&#39;) ax3.set_ylabel(&#39;y&#39;) plt.show() plt.close() . . 0%| | 0/12 [00:00&lt;?, ?it/s] . 8%|██████▉ | 1/12 [00:01&lt;00:13, 1.20s/it] . 92%|███████████████████████████████████████████████████████████████████████████▏ | 11/12 [00:01&lt;00:00, 1.16it/s] . 100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:02&lt;00:00, 4.34it/s] . いずれもcenter-surroundingな受容野を持っているのですが、最後は符号が反転しています。 . 問題文に . In the averaged images, you should see a central receptive field that reverses sign over time. . とあるので解析を間違えたわけではなさそうですが、ちゃんと理解できていません。 . &#26178;&#31354;&#38291;&#30340;&#21463;&#23481;&#37326; . 計算されたSTAについてy方向の加算を行い、x-tの軸の受容野を可視化します。 . #collapse-hide # Calculate spatio-temporal receptive field sum_sta = np.sum(sta, axis=1).T fig = plt.figure(figsize=(12, 3)) fig.suptitle(&quot;The spatio-temporal receptive field of a cat LGN X cell.&quot;, fontsize=14) gs = gridspec.GridSpec(1, 3, width_ratios=[1, 1, 1.5], height_ratios=[1]) x = np.arange(-H//2, H//2+2, 2) ax1 = plt.subplot(gs[0,0]) plt.imshow(sum_sta) plt.gca().set_aspect(&#39;equal&#39;) plt.gca().invert_yaxis() plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;time steps&#39;) ax1.title.set_text(&#39;heatmap&#39;) plt.xticks(list(np.arange(0, W+1, 2)-0.5), list(x)) zoom = 3 smoothed_sum_sta = scipy.ndimage.zoom(sum_sta, zoom=zoom) # smoothed x = np.arange(-H//2*zoom, H//2*zoom) t = np.arange(12*zoom) X, T = np.meshgrid(x, t) ax2 = plt.subplot(gs[0,1]) plt.contour(X, T, smoothed_sum_sta, 25) plt.xlabel(&#39;x&#39;) ax2.title.set_text(&#39;contour&#39;) plt.gca().set_aspect(&#39;equal&#39;) ax3 = plt.subplot(gs[0,2], projection=&#39;3d&#39;) ax3.plot_surface(X, T, smoothed_sum_sta, shade=True, color=&#39;grey&#39;) ax3.set_xlabel(&#39;x&#39;) ax3.set_ylabel(&#39;time steps&#39;) plt.show() plt.close() . . Theoretical Neuroscienceの図2.25Cのような図が再現できました。 .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/neuroscience/2019/05/31/lgn_sta.html",
            "relUrl": "/neuroscience/2019/05/31/lgn_sta.html",
            "date": " • May 31, 2019"
        }
        
    
  
    
        ,"post25": {
            "title": "ニューラルネットワークにおける意味発達の数学理論",
            "content": "A. M. Saxe, J. L. McClelland, S. Ganguli. “A mathematical theory of semantic development in deep neural networks”. PNAS. (2019). (arXiv). (PNAS) . を読んだまとめ (+ 実装) です。全部ではなく、初めの部分だけについて解説します。 . 概要 . Abstractからではなく、著者の一人であるS. Ganguli氏のtwitterを日本語訳しました。とりあえず大まかな内容を掴むためのメモのようなものです。 . 人間の意味的認知における現象が深層ニューラルネットワークでいくつ発生するか、そしてこれらの現象が単純な深層線形ネットワークで解析的に理解されることができるかという研究。 . 以下は、人間の意味的認知における現象の例。 . 幼児の意味発達に対する概念の階層的な区別。幼児は細かいカテゴリーの区別を取得する前に広いカテゴリーの区別を取得するということである。 | 幼児にはしばらくの間何も学習せず、突然新しい概念を習得するというような発達的学習における段階的な急速な遷移(Rapid stage)が存在する。 | 幼児は発達過程において誤った事実（例えば『芋虫には骨がある』）を思い込んでしまうことがある。そのような事実を裏付けるデータは外部環境には存在しないので、古典的な関連モデルでは説明できない。 | あるカテゴリーの典型的な要素と非典型的な要素の概念（例えば、カナリアは典型的な鳥だがダチョウはそうではない）が存在し、そしてなぜニューラルネットワークが典型的な要素をより迅速に意味的に処理できるのか。 | いくつかのカテゴリはより「統一がある(coherent)」（例えば『犬』の集合）ものであり、他のカテゴリはそうではない（例えば『すべてのものが青』の集合）。カテゴリーの統一度(category coherence)の定量的概念を定義し、より統一のあるカテゴリーがより早く学習されることを数学的に証明した。 . 基本レベルの効果：ニューラルネットワークが上位レベル（例えば『動物』）または下位レベル（例えば『ロビン(コマドリなど)』）よりも、基本的な階層レベル（例えば『鳥』）の構造を優先的に学習し、名前付けすることができる理由の説明。 | 発達過程における帰納的一般化のパターンの変化：なぜ幼児やニューラルネットワークは学習の早い段階で過剰に一般化され、その後徐々に一般化のパターンを制限するのかということの説明。 | 表現類似度解析 (Representational Similarity Analysis; RSA)：異なる初期重みから同じデータを学習した2つの深層線形ネットワークは、学習が最適（つまり最小ノルム重み）である場合に限り、同じ類似度構造を持つ内部表現を学習する。 | 知識の獲得とそのダイナミクス . タスク . まずタスクについて。入力 $ boldsymbol{x}$ は「もの」の項目(例えばカナリア、犬、サーモン、樫など)、出力 $ boldsymbol{y}$はそれぞれの項目の性質・特性となっています。例えばカナリア(Canary)は成長し(Grow)、動き(Move)、空を飛べる(Fly)ので、Canaryという入力に対し、ネットワークが出力するのはGrow, Move, Flyとなります。 . . (Saxe et al. 2019, Fig. 1B) 上図はマンボウ(sunfish)の場合です。マンボウはBarkするのでしょうか…。 . モデル . モデルは3層の全結合線形ネットワークです。 y^=W2W1x hat{ boldsymbol{y}} = boldsymbol{ W} ^ {2} boldsymbol{ W} ^ {1} boldsymbol{x}y^​=W2W1x ただし非線形な活性化関数が無いので、普段使うニューラルネットワークではありません。これはニューラルネットワークを学ぶ人なら誰でも知っていると思いますが、 $ boldsymbol{ W} ^ {s}={ bf W} ^ {2} boldsymbol{ W} ^ {1}$として、 上のネットワークは . y^=Wsx hat{ boldsymbol{y}} = boldsymbol{ W} ^ {s} boldsymbol{x}y^​=Wsx . とまとめることができます。なので線形な活性化関数で深いニューラルネットワークを構築しても何の意味もなく、それゆえ非線形な活性化関数が必要となるということです。しかし、この論文では深い(3層の)ネットワークである場合のみ、幼児の発達における非線形な現象が説明でき、浅い(2層の)ネットワークでは当てはまらないと主張しています。3層で「深い」というのはどうかと思いますが、2層を「浅い」とした場合、比較して「深い」ということです。 . 学習 . ネットワークの学習(重みの更新)は誤差逆伝搬から導かれる次の2式により行います。 . τddtW1=(W2)T(Σyx−W2W1Σx)τddtW2=(Σyx−W2W1Σx)(W1)T begin{aligned} tau frac{d}{dt} boldsymbol{ W} ^ {1}&amp;=( boldsymbol{W} ^ 2) ^ T ( boldsymbol{ Sigma} ^ {yx} - boldsymbol{W} ^ 2 boldsymbol{W} ^ 1 boldsymbol{ Sigma} ^ {x}) tau frac{d}{dt} boldsymbol{W} ^ {2}&amp;=( boldsymbol{ Sigma} ^ {yx} - boldsymbol{W} ^ {2} boldsymbol{W} ^ 1 boldsymbol{ Sigma} ^ {x}) ( boldsymbol{W} ^ 1)^T end{aligned}τdtd​W1τdtd​W2​=(W2)T(Σyx−W2W1Σx)=(Σyx−W2W1Σx)(W1)T​ . ただし、$ boldsymbol{ Sigma} ^ {x}$は入力間の関係を表す行列、$ boldsymbol{ Sigma} ^ {yx}$は入出力の関係を表す行列です。他にも定義していない変数がありますが、これは後の実装を見ると分かりやすいと思います。 . 特異値分解(SVD)による学習ダイナミクスの解析 . 学習ダイナミクスは$ boldsymbol{ Sigma} ^ {yx}$に対する特異値分解(SVD)を用いて説明できます。 . Σyx=USVT boldsymbol{ Sigma} ^ {yx}= boldsymbol{USV}^TΣyx=USVT . 行列$ boldsymbol{ S}$の対角成分の非ゼロ要素が特異値です。 . . (Saxe et al. 2019, Fig. 3A) 次の図は学習途中における$ hat{ boldsymbol{ Sigma}} ^ {yx}(t)= boldsymbol{W} ^ 2 (t) boldsymbol{W} ^ 1(t) boldsymbol{ Sigma} ^ {x}$のSVDの結果です。$ boldsymbol{ A}(t)$の要素$a_{ alpha}(t)$が特異値です。 . . (Saxe et al. 2019, Fig. 3B) この $a _ { alpha}(t)$ですが、3層のネットワークの場合、大きな特異値から先に学習されます(C)。2層の浅いネットワークの場合、全ての特異値が同時に学習されます(D)。 . . (Saxe et al. 2019, Fig. 3C,D) このダイナミクスですが、低ランク近似 (low-rank approximation)が生じていて、特異値の大きな要素から学習されていると捉えることができます。学習が進むとランクが大きくなっていく感じです。 . 低ランク近似の例として、SVDによる画像の圧縮と復元を見てみましょう。 . . 元画像はカメラマンの写真です。この画像に対し、低ランク近似を行い、ランクを上げていきます。するとランクが上がるにつれて、画像が鮮明になります。 . . from skimage import data import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation from tqdm import tqdm # Low-rank approximation with SVD def LowRankApprox(u, s, v, rank): ur = u[:, :rank] sr = np.diag(s[:rank]) vr = v[:rank, :] return ur @ sr @ vr img = data.camera() # 512 x 512 #plt.imshow(img) #plt.show() #plt.tight_layout() #plt.savefig(&quot;camera.png&quot;) # Animation fig = plt.figure() ax = fig.add_subplot(1,1,1) ims=[] max_rank = 50 u, s, v = np.linalg.svd(img) for r in tqdm(range(1, max_rank+1)): title = fig.text(.3, .97, &quot;rank = &quot;+str(r), transform = ax.transAxes, verticalalignment = &quot;center&quot;, fontsize=&quot;large&quot;) img_approx = LowRankApprox(u, s, v, rank=r) im, = [ax.imshow(img_approx)] ims.append([im, title]) #run animation ani = animation.ArtistAnimation(fig,ims, interval=500) plt.tight_layout() #plt.show() ani.save(&quot;results_video.mp4&quot;) . これと同じことが知識の獲得において生じていると見なすことができます。 . 実装 . 再現自体は簡単だったので、実装してみました。ニューラルネットワークといえど、numpyで十分なレベルです。一部のハイパーパラメータは適当に設定しています。 . 3層線形ネットワーク (deep) . Fig. 3Cに対応。 . . import numpy as np import matplotlib.pyplot as plt # Set initial values N_1 = 4 N_2 = 16 N_3 = 7 s_yx = np.array([[1,1,1,1], [1,1,0,0], [0,0,1,1], [1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]]) s_x = np.eye(N_1) u, s, v = np.linalg.svd(s_yx) eps = 1e-2 w_1 = eps*np.random.rand(N_2,N_1) w_2 = eps*np.random.rand(N_3,N_2) # Simulation &amp; training dt = 0.005 N_t = 1500 A = np.zeros((N_t, N_1)) for t in range(N_t): # Update D = s_yx - w_2 @ w_1 @ s_x dw_1 = (w_2.T @ D) * dt dw_2 = (D @ w_1.T) * dt w_1 += dw_1 w_2 += dw_2 # SVD &amp; save results s_yx_hat = w_2 @ w_1 @ s_x u, a, v = np.linalg.svd(s_yx_hat) A[t] += a # Plot results T = np.linspace(0.0, 1.0, N_t) fig = plt.figure(figsize=(6,3)) plt.plot(T, A[:,0], label=&quot;a1&quot;) plt.plot(T, A[:,1], label=&quot;a2&quot;) plt.plot(T, A[:,2], label=&quot;a3&quot;) plt.plot(T, A[:,3], label=&quot;a4&quot;) plt.xlabel(&quot;t&quot;) plt.ylabel(&quot;A(t)&quot;) plt.legend() plt.tight_layout() #plt.show() plt.savefig(&quot;result_deep.png&quot;) . 大きな特異値から学習が始まっているのが分かります。また、それぞれの特異値の学習においてはシグモイド関数様の急速な学習段階が見られます。 . 2層線形ネットワーク (shallow) . Fig. 3Dに対応。 . . import numpy as np import matplotlib.pyplot as plt # Set initial values N_1 = 4 #N_2 = 16 N_3 = 7 s_yx = np.array([[1,1,1,1], [1,1,0,0], [0,0,1,1], [1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]]) s_x = np.eye(N_1) u, s, v = np.linalg.svd(s_yx) eps = 1e-2 w_s = eps*np.random.rand(N_3,N_1) # Simulation &amp; training dt = 0.005 N_t = 1000 A = np.zeros((N_t, N_1)) for t in range(N_t): # Update dw_s = (s_yx - w_s @ s_x) * dt w_s += dw_s # SVD &amp; save results s_yx_hat = w_s @ s_x u, a, v = np.linalg.svd(s_yx_hat) A[t] += a # Plot results T = np.linspace(0.0, 1.0, N_t) fig = plt.figure(figsize=(6,3)) plt.plot(T, A[:,0], label=&quot;a1&quot;) plt.plot(T, A[:,1], label=&quot;a2&quot;) plt.plot(T, A[:,2], label=&quot;a3&quot;) plt.plot(T, A[:,3], label=&quot;a4&quot;) plt.xlabel(&quot;t&quot;) plt.ylabel(&quot;A(t)&quot;) plt.legend() plt.tight_layout() #plt.show() plt.savefig(&quot;result_shallow.png&quot;) . 全ての特異値の学習が初めから起こっています。ただ、収束はこちらの方が速かったです（パラメータが少ないので）。 . 考察 . 学習ダイナミクスにおいて特異値分解が関わっているというのは面白く思いました。ただ、特異値分解自体を学習しているわけではないと思います。 . もう1つ面白いと思ったのが知識の混同（例えば『芋虫には骨がある』）の仕組みです。発達において、大きい特異値から先に学習されるため、「動く」、「成長する」などの動物の要素が先に獲得されます。身の回りの動物のほとんどが「骨を持つ」ので、低ランク近似により、『芋虫にも骨がある』と錯覚してしまうのではないか、という話です。 . データ数もモデルも小さいので、大きなデータセットに対し、非線形な深層ニューラルネットワークでもこの理論が成り立つのかというのが今後の課題となりそうですね。 .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/neuroscience/2019/01/15/semantic_development.html",
            "relUrl": "/neuroscience/2019/01/15/semantic_development.html",
            "date": " • Jan 15, 2019"
        }
        
    
  
    
        ,"post26": {
            "title": "グリッド細胞の発火パターンをPythonで可視化する",
            "content": "&#27010;&#35201; . Edvard Moser博士の研究室が公開している、グリッド細胞の活動をPythonで可視化してみました。データはhttps://www.ntnu.edu/kavli/research/grid-cell-dataからダウンロードできます。 . コードを書く上でhttp://felix11h.github.io/blog/grid-cell-rate-mapsを参考にしました。一部の関数はこのブログから引用しています。今回は上記のサイトで実装されていない、Gaussian kernelを用いたSmoothed rate mapとAutocorrelation mapの実装をしてみます。 . Important: 著者はGrid cellsの研究をしていません。実際の研究で用いられるコードと異なる可能性があります。 . &#12464;&#12522;&#12483;&#12489;&#32048;&#32990;(Grid Cells)&#12395;&#12388;&#12356;&#12390; . 実装とは関係ないですが、グリッド細胞についてまとめておきます。 . &#31354;&#38291;&#22522;&#24213;&#12392;&#12375;&#12390;&#12398;&#12464;&#12522;&#12483;&#12489;&#32048;&#32990; . 詳しくは場所細胞 - 脳科学辞典や2014年のノーベル生理学・医学賞の解説（神経科学学会）、Grid cells (Scholarpedia)などをお読みいただければと思います。簡単にまとめると、海馬には場所特異的に発火する場所細胞(place cell)があり、これはO&#39;keefe博士によって発見されました。次にMay-Britt Moser博士とEdvard Moser博士は六角形格子状の場所受容野を持つグリッド細胞(格子細胞, grid cell)を内側嗅内皮質(medial entorhinal cortex; MEC)で発見しました。この3人は2014年のノーベル生理学・医学賞を受賞しています。 . . http://www.scholarpedia.org/article/Grid_cellsより。左図の黒線はラットの経路、赤は発火が生じた位置。右図は発火率マップ(rate map)。 . 最近、外側膝状体背側核(dorsal lateral geniculate nucleus)で場所細胞が見つかったそうです（V Hok, et al., 2018, bioRxiv）。 . &#12487;&#12540;&#12479;&#12395;&#12388;&#12356;&#12390; . 公開されているデータはMatLabのmatファイル形式です。しかし、scipy.io.loadmatを用いることでpythonでデータの中身を取得することができます。 . 使用するデータは以下の通りです。 . 10704-07070407_POS.mat | 10704-07070407_T2C3.mat | . これらのファイルはhttps://archive.norstore.no/pages/public/datasetDetail.jsf?id=8F6BE356-3277-475C-87B1-C7A977632DA7からダウンロードできるファイルの一部です。ただし全体で2.23GBあるので、簡単に試したい場合は上記のリンクからダウンロードしてください。以下では./data/grid_cells_data/ディレクトリの下にファイルを置いています。 . データの末尾の&quot;POS&quot;と&quot;T2C3&quot;の意味について説明しておきます。まず、&quot;POS&quot;はpost, posx, posyを含む構造体でそれぞれ試行の経過時間、x座標, y座標です。座標は-50~50で記録されています。恐らく1m四方の正方形の部屋で、原点を部屋の中心としているのだと思います。&quot;T2C3&quot;はtがtetrode（テトロード電極）でcがcell（細胞）を意味します。後ろの数字は番号付けたものと思われます。 . Smoothed Rate Map&#12395;&#12388;&#12356;&#12390; . 発火率$ lambda( boldsymbol{x})$は、場所$ boldsymbol{x}=(x,y)$で記録されたスパイクの回数を、場所$ boldsymbol{x}$における滞在時間(s)で割ることで得られます。 $$ lambda( boldsymbol{x})= frac{ displaystyle sum_{i=1}^n g left( frac{ boldsymbol{s}_i- boldsymbol{x}}{h} right)}{ displaystyle int_0^T g left( frac{ boldsymbol{y}(t)- boldsymbol{x}}{h} right)dt} $$ ただし、$n$はスパイクの回数、$T$は計測時間、$g( cdot)$はGaussain Kernel（中身の分子が平均、分母が標準偏差）、$ boldsymbol{s}_i$は$i$番目のスパイクの発生した位置、$ boldsymbol{y}(t)$は時刻$t$でのラットの位置です。分母は積分になっていますが、実際には離散的に記録をするので、累積和に変更し、$dt$を時間のステップ幅(今回は0.02s)とします。 . Gaussian Kernelを用いて平滑化することで「10cm四方での発火を同じ位置での発火とする」などとした場合よりも、得られるマップは滑らかになります。 . &#23455;&#35013; . まず、ライブラリをインポートしてデータを読み込みます。 . import numpy as np import matplotlib.pyplot as plt from scipy import io as io from tqdm import tqdm # from http://www.ntnu.edu/kavli/research/grid-cell-data pos = io.loadmat(&#39;./data/grid_cells_data/10704-07070407_POS.mat&#39;) spk = io.loadmat(&#39;./data/grid_cells_data/10704-07070407_T2C3.mat&#39;) . posファイル内の構造は次のようになっています。 . pos[&quot;post&quot;]: times at which positions were recorded | pos[&quot;posx&quot;]: x positions | pos[&quot;posy&quot;]: y positions | spk[&quot;cellTS&quot;]: spike times | . 次に種々の関数を実装します。 . def nearest_pos(array, value): k = (np.abs(array - value)).argmin() return k . def GaussianKernel(sizex, sizey, sigma=0.5, center=None): &quot;&quot;&quot; sizex : kernel width sizey : kernel height sigma : gaussian Sd center : gaussian mean return gaussian kernel &quot;&quot;&quot; x = np.arange(0, sizex, 1, float) y = np.arange(0, sizey, 1, float) x, y = np.meshgrid(x,y) if center is None: x0 = sizex // 2 y0 = sizey // 2 else: if np.isnan(center[0])==False and np.isnan(center[1])==False: x0 = center[0] y0 = center[1] else: return np.zeros((sizey,sizex)) return np.exp(-((x-x0)**2 + (y-y0)**2) / 2*sigma**2) . def smoothed_rate_map(pos, spk, kernel_sigma=0.1, W=100, H=100): # load datas posx = pos[&quot;posx&quot;].flatten() posy = pos[&quot;posy&quot;].flatten() spkt = spk[&quot;cellTS&quot;].flatten() #change positions range: -50 ~ 50 -&gt; 0 ~ H or W posx = (posx + 50) / 100 * W posy = (posy + 50) / 100 * H # find nearest positions when spikes occur indx = [nearest_pos(pos[&quot;post&quot;],t) for t in spkt] indy = [nearest_pos(pos[&quot;post&quot;],t) for t in spkt] # occup position while trajectory occup_m_list = [] for i in tqdm(range(len(posx))): occup_m_list.append(GaussianKernel(W, H, kernel_sigma, (posx[i], posy[i]))) occup_m = sum(occup_m_list) occup_m *= 0.02 # one time step is 0.02s occup_m[occup_m==0] = 1 # avoid devide by zero # activation activ_m_list = [] for i in tqdm(range(len(spkt))): activ_m_list.append(GaussianKernel(W, H, kernel_sigma, (posx[indx][i] ,posy[indy][i]))) activ_m = sum(activ_m_list) rate_map = activ_m / occup_m return rate_map . 最後に実行します。 . rm = smoothed_rate_map(pos, spk, 0.2, 100, 100) plt.figure(figsize=(6,4)) plt.imshow(rm, cmap=&quot;jet&quot;) plt.colorbar(label=&quot;Hz&quot;) plt.gca().invert_yaxis() plt.tight_layout() # plt.savefig(&quot;smoothed_rate_map.png&quot;) plt.show() . 100%|██████████████████████████████████████████████████████████████████████████| 30000/30000 [00:09&lt;00:00, 3306.34it/s] 100%|█████████████████████████████████████████████████████████████████████████████| 2326/2326 [00:02&lt;00:00, 959.91it/s] . Autocorrelation Map&#12395;&#12388;&#12356;&#12390; . https://core.ac.uk/download/pdf/30859910.pdfのSupporting Online Materialに書いてある式通りに実装してみましたが、遅い＆論文と見た目が全く異なるので、scipy.signal.correlate2dを使いました。 . from scipy.signal import correlate2d rm = smoothed_rate_map(pos, spk, 0.5, 100, 100) a_corr = correlate2d(rm, rm, fillvalue=5) plt.figure(figsize=(6,4)) plt.imshow(a_corr, cmap=&quot;jet&quot;) plt.colorbar(label=&quot;Autocorrelation&quot;) plt.tight_layout() # plt.savefig(&quot;autocorr.png&quot;) plt.show() . 100%|██████████████████████████████████████████████████████████████████████████| 30000/30000 [00:16&lt;00:00, 1795.03it/s] 100%|█████████████████████████████████████████████████████████████████████████████| 2326/2326 [00:02&lt;00:00, 929.87it/s] . 若干論文と図が異なる上、cross-correlationが-1~1の範囲でないのはおかしい気がするのですが、六角形格子が見えているので良しとします。 . &#21442;&#32771;&#12395;&#12375;&#12383;&#25991;&#29486;&#12539;&#12469;&#12452;&#12488; . https://github.com/Felix11H/grid_cell_rate_map | https://www.ntnu.edu/kavli/research/grid-cell-data | https://core.ac.uk/download/pdf/30859910.pdfのSupporting Online Material | https://github.com/MattNolanLab/gridcells | https://arxiv.org/pdf/1810.07429.pdf | .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/neuroscience/2018/11/23/grid_cells.html",
            "relUrl": "/neuroscience/2018/11/23/grid_cells.html",
            "date": " • Nov 23, 2018"
        }
        
    
  
    
        ,"post27": {
            "title": "格子ボルツマン法による流体シミュレーション (Python)",
            "content": "1年次の時のレポートを記事化したものです。元はProcessingによるシミュレーションでした。 . &#26684;&#23376;&#12508;&#12523;&#12484;&#12510;&#12531;&#27861; . &#35336;&#31639;&#26041;&#27861;&#12398;&#27010;&#35201; . 流体解析（流体シミュレーション, computational fluid dynamics : CFD）を行う上では、連続体である流体の運動を離散化して計算する必要があります。このとき計算する方程式は次式で表される、流体の運動量保存則です. . $$ begin{aligned} rho frac{D boldsymbol{v}}{Dt}=- nabla P+ mu nabla^2 boldsymbol{v}+ boldsymbol{f} end{aligned} $$この式はNavier-Stokes方程式と呼ばれます。ここで、$ rho$は密度、$P$は圧力、$ mu$は粘性抵抗、$ boldsymbol{f}$は外力であり、$ dfrac{D boldsymbol{v}}{Dt}$は流れの速度場$ boldsymbol{v}$のLagrange微分です。よって . $$ begin{aligned} frac{D boldsymbol{v}}{Dt}= frac{ partial boldsymbol{v}}{ partial t}+( boldsymbol{v} cdot nabla) boldsymbol{v} end{aligned} $$が成り立ちます。また、質量の保存則として連続の式 . $$ begin{aligned} nabla cdot boldsymbol{v}=0 end{aligned} $$があります。このような偏微分方程式で表される質量・運動量・エネルギーの保存則を離散化して数値的に解く方法としては有限差分法、有限要素法、有限体積法などがあります。また、流体を粒子の集合に分割し、流体の方程式に従う粒子の動きを計算します方法を粒子法と呼びます。今回用いる格子ボルツマン法（Lattice Boltzmann Method;LBM）とは、これらに属さない方法であり、流体を有限個の種類の速度を持つ仮想的な粒子の集合とみなし、それらの粒子の衝突と並進を計算します。流体を連続体ではなく粒子として扱う点は粒子法に似ていますが、LBMではあくまでも仮想的な粒子の分布を扱います。質量・運動量・エネルギーの保存則を巨視的（マクロ）な方程式であるとすれば、LBMが解く方程式は統計熱力学に基づく微視的 (ミクロ) な方程式と言えます。 . LBM&#12398;&#26684;&#23376; . LBMの格子は2次元では正方形、3次元では立方体を用いることが多いです。今回は、2次元平面における流れを考えるため、2D9Qモデルを用います。&#39;&#39;2D&#39;&#39;というのは2次元である事を意味し、&#39;&#39;9Q&#39;&#39;は粒子の取りうる速度$ boldsymbol{c}_i$の方向が9つあることを示しています (下図：LBMの格子(2D9Q)と9方向の速度 ($c=1$のとき))。 . . 9つの速度は $$ begin{aligned} boldsymbol{c}_0&amp;=(0,0) (i=0) boldsymbol{c}_i&amp;=c left[ cos left( frac{i-1}{2} pi right), sin left( frac{i-1}{2} pi right) right] (i=1,2,3,4) boldsymbol{c}_i&amp;= sqrt{2}c left[ cos left( frac{i-1}{2} pi+ frac{ pi}{4} right), sin left( frac{i-1}{2} pi+ frac{ pi}{4} right) right] (i=5,6,7,8) end{aligned} $$ . で表されます。ここで$c$は定数ですが、実装上は簡単のため$c=1$とします。なお、この2D9Qモデルですが、数式で表記する場合には数字で方向をラベリングするほうがよいです。逆に実装する上では方角による方向のラベリングを行う方がミスがなくなりやすいでしょう (下図：2D9Q のラベリング)。 . . &#26178;&#38291;&#30330;&#23637;&#12398;&#24335; . 座標$ boldsymbol{x}=(x,y)$、時刻$t$において速度$ boldsymbol{c}_i$を持つ粒子についての速度分布関数(確率密度関数)を$f_i( boldsymbol{x},t)$とします。この速度分布関数$f_i( boldsymbol{x},t)$は以下の格子ボルツマン方程式 (lattice Boltzmann equation) に従います. . $$ begin{aligned} underbrace{f_i( boldsymbol{x}+ boldsymbol{c}_i Delta t,t+ Delta t)-f_i( boldsymbol{x},t)}_{並進 textrm{(streaming)}}= underbrace{ Omega_i[f_i( boldsymbol{x},t)]}_{衝突 textrm{(collision)}} end{aligned} tag{1} $$ただし、$ Delta t$は時間刻みです。速度$ boldsymbol{c}_i$の粒子は$ Delta t$で空間刻み$ Delta boldsymbol{x}= boldsymbol{c}_i Delta t$だけ離れた格子に移動します。(1)式は初期条件さえ与えれば速度分布関数$f_i( boldsymbol{x},t)$を陽的に計算できます。また上式の左辺は粒子の並進(streaming)を表し、右辺は粒子同士の衝突(collision)を表します。並進は下図のように計算します。 . . 実装上では$ Omega_i$の取り扱いが重要とりますが、BGK (Bhatnagar-Gross-Krook)近似に基づく衝突則を用いられることが多いです。BGK近似モデルを用いたLBMの基礎方程式は次式で表されます。 . $$ begin{aligned} f_i( boldsymbol{x}+ boldsymbol{c}_i Delta t,t+ Delta t)-f_i( boldsymbol{x},t)=- frac{1}{ tau}[f_i( boldsymbol{x},t)-f_i^{ textrm{eq}}( boldsymbol{x},t)] end{aligned} tag{2} $$ここで、$ tau$は単一緩和時間です。 実装上は$ omega:= dfrac{1}{ tau}$とします。なお、粘度（動粘性抵抗）$ nu$と単一緩和時間$ tau$および$ omega$との間には . $$ tau= frac{1}{ omega} approx 3 nu+0.5 Delta t $$が成り立ちます。 . &#24179;&#34913;&#20998;&#24067;&#38306;&#25968;&#12398;&#35336;&#31639; . (2)式中の$f_i^{ textrm{eq}}( boldsymbol{x},t)$を平衡分布関数と呼びます。ここでMaxwell-Boltzmann平衡分布は次式で表されます。 . $$ g^{ textrm{eq}}( boldsymbol{ xi})= rho{ left( frac{m}{2 pi k_BT} right)}^{ frac{D}{2}} exp left(- frac{m( boldsymbol{c}_i- boldsymbol{v})^2}{2k_BT} right) $$ただし、$ boldsymbol{ xi}:= boldsymbol{c}_i- boldsymbol{v}$とし、$D$ は次元数としました。上式を$ boldsymbol{v}$についてTayler展開することで平衡分布関数は求められ、次式で表されます。 . $$ begin{aligned} f_i^{ textrm{eq}}( boldsymbol{x},t)=w_i rho left[1+ frac{3( boldsymbol{c}_i cdot boldsymbol{v})}{c^2}+ frac{9}{2c^4}( boldsymbol{c}_i cdot boldsymbol{v})^2- frac{3}{2c^2} boldsymbol{v}^2 right] end{aligned} tag{3} $$ただし、 . $$ w_i = begin{cases} frac{4}{9} &amp; (i=0) frac{1}{9} &amp; (i=1, 2, 3, 4) frac{1}{36} &amp; (i=5, 6, 7, 8) end{cases} $$です。この$f_i^{ textrm{eq}}( boldsymbol{x},t)$の計算のために密度$ rho( boldsymbol{x},t)$と流速$ boldsymbol{v}( boldsymbol{x},t)=(v_x( boldsymbol{x},t), v_y( boldsymbol{x},t))$を計算する必要がありますが、これらは次のように$f_i( boldsymbol{x},t)$から求めることができます。 . $$ begin{align} rho( boldsymbol{x},t)&amp;= sum_{i=0}^{8}f_i( boldsymbol{x},t) tag{4} v_x( boldsymbol{x},t)&amp;= frac{1}{ rho} left[(f_1+f_5+f_8)-(f_3+f_6+f_7) right] tag{5} v_y( boldsymbol{x},t)&amp;= frac{1}{ rho} left[(f_2+f_5+f_6)-(f_4+f_7+f_8) right] tag{6} end{align} $$これより、仮想粒子の速度$ boldsymbol{c}_i$と流速$ boldsymbol{v}( boldsymbol{x}、t)$の内積$( boldsymbol{c}_i cdot boldsymbol{v})$の計算ができます。また$c=1$とするので、最終的な実装上の式は次のようになります. . $$ begin{array}[H]{c||c|c} i&amp;( boldsymbol{c}_i cdot boldsymbol{v})&amp;f_i^{ textrm{eq}}( boldsymbol{x},t) hline 0&amp;0&amp; frac{4}{9} rho left[1- frac{3}{2} boldsymbol{v}^2 right] 1(E)&amp;v_x&amp; frac{1}{9} rho left[1+3v_x+ frac{9}{2}v_x^2- frac{3}{2} boldsymbol{v}^2 right] 2(N)&amp;v_y&amp; frac{1}{9} rho left[1+3v_y+ frac{9}{2}v_y^2- frac{3}{2} boldsymbol{v}^2 right] 3(W)&amp;-v_x&amp; frac{1}{9} rho left[1-3v_x+ frac{9}{2}v_x^2- frac{3}{2} boldsymbol{v}^2 right] 4(S)&amp;-v_y&amp; frac{1}{9} rho left[1-3v_y+ frac{9}{2}v_y^2- frac{3}{2} boldsymbol{v}^2 right] 5(NE)&amp;v_x+v_y&amp; frac{1}{36} rho left[1+3(v_x+v_y)+ frac{9}{2}(v_x+v_y)^2- frac{3}{2} boldsymbol{v}^2 right] 6(NW)&amp;-v_x+v_y&amp; frac{1}{36} rho left[1+3(-v_x+v_y)+ frac{9}{2}(-v_x+v_y)^2- frac{3}{2} boldsymbol{v}^2 right] 7(SW)&amp;-v_x-v_y&amp; frac{1}{36} rho left[1-3(v_x+v_y)+ frac{9}{2}(v_x+v_y)^2- frac{3}{2} boldsymbol{v}^2 right] 8(SE)&amp;v_x-v_y&amp; frac{1}{36} rho left[1+3(v_x-v_y)+ frac{9}{2}(v_x-v_y)^2- frac{3}{2} boldsymbol{v}^2 right] hline end{array} $$ &#22659;&#30028;&#26465;&#20214; . 粒子と障壁(barrier)との衝突を考えます。滑りなし壁の条件($ boldsymbol{v}=0$)を簡易に実装するために用いられているのが、bounce-back条件 です。障害物に指定された格子点に仮想粒子が並進運動で侵入した場合、bounce-back条件では、仮想粒子をその侵入方向と180°反対の方向に跳ね返します (次図)。なお、今回は障害物の境界は格子点の中央に存在するとします。 . . &#35336;&#31639;&#12398;&#12450;&#12523;&#12468;&#12522;&#12474;&#12512; . アルゴリズムは以下のようになります. . 密度$ rho$, 流速$ boldsymbol{v}$, 速度分布関数$f_i$, 平衡分布関数$f$を初期化します。今回の場合は、左から右に流れが生じている場合を考えるので$ boldsymbol{v}$は$x$方向のみ大きさを持ちます。また、計算開始前、分布関数は平衡に達していたとします。よって次式のようになります。 . $$ begin{align} rho( boldsymbol{x},t)&amp;:=1 tag{7} boldsymbol{v}( boldsymbol{x},t)=(v_x, v_y)&amp;:=(u,0) (u:定数) tag{8} f_i( boldsymbol{x},t=0)&amp;:=f_i^{ textrm{eq}}( boldsymbol{x},t=0) tag{9} end{align} $$ . | 密度$ rho$, 流速$ boldsymbol{v}$を$f_i( boldsymbol{x},t)$から(4), (5), (6)式によって求めます。 . | 平衡分布関数$f_i^{ textrm{eq}}( boldsymbol{x},t)$を(3)式より計算します。 . | (Streaming Step) (2)式の左辺により、並進後の分布関数を次のように計算します。 . $$ begin{aligned} f_i( boldsymbol{x}+ boldsymbol{c}_i Delta t,t+ Delta t)=f_i^*( boldsymbol{x},t) end{aligned} tag{10} $$ . | (Bounce Step) 5において並進した格子に障害物（境界）が存在した場合、bounce-back条件により、修正します。 . | (Collision Step) 仮想粒子の衝突を(2)式の右辺により次のように計算します。ただし、$f_i^*( boldsymbol{x},t)$は仮想粒子が衝突し、並進する前の分布関数です。 . $$ begin{aligned} f_i^*( boldsymbol{x},t)=f_i( boldsymbol{x},t)- frac{1}{ tau}[f_i( boldsymbol{x},t)-f_i^{ textrm{eq}}( boldsymbol{x},t)] end{aligned} tag{11} $$ . | 2～6を繰り返します。 . | Python&#12391;&#12398;&#23455;&#35013; . ここからはPythonでの実装について解説していきます。なお、コードはFluid Dynamics Simulationを参考にしています。Processing版はOpenProcessingで公開しています (https://www.openprocessing.org/sketch/497312/)。 . ライブラリをimportします。このとき、%matplotlib nbaggを付ければipynb上でアニメーションが実行できます。 . %matplotlib nbagg import numpy as np import matplotlib import matplotlib.pyplot as plt import matplotlib.animation as animation . 各種定数を定義していきます。 . # Define constants: height = 80 # lattice dimensions width = 200 viscosity = 0.02 # fluid viscosity omega = 1 / (3*viscosity + 0.5) # &quot;relaxation&quot; parameter u0 = 0.1 # initial and in-flow speed four9ths = 4.0/9.0 # abbreviations for lattice-Boltzmann weight factors one9th = 1.0/9.0 one36th = 1.0/36.0 . 格子の初期化を(3)式に基づいて行います。 . # Initialize all the arrays to steady rightward flow: n0 = four9ths * (np.ones((height,width)) - 1.5*u0**2) # particle densities along 9 directions nN = one9th * (np.ones((height,width)) - 1.5*u0**2) nS = one9th * (np.ones((height,width)) - 1.5*u0**2) nE = one9th * (np.ones((height,width)) + 3*u0 + 4.5*u0**2 - 1.5*u0**2) nW = one9th * (np.ones((height,width)) - 3*u0 + 4.5*u0**2 - 1.5*u0**2) nNE = one36th * (np.ones((height,width)) + 3*u0 + 4.5*u0**2 - 1.5*u0**2) nSE = one36th * (np.ones((height,width)) + 3*u0 + 4.5*u0**2 - 1.5*u0**2) nNW = one36th * (np.ones((height,width)) - 3*u0 + 4.5*u0**2 - 1.5*u0**2) nSW = one36th * (np.ones((height,width)) - 3*u0 + 4.5*u0**2 - 1.5*u0**2) rho = n0 + nN + nS + nE + nW + nNE + nSE + nNW + nSW # macroscopic density ux = (nE + nNE + nSE - nW - nNW - nSW) / rho # macroscopic x velocity uy = (nN + nNE + nNW - nS - nSE - nSW) / rho # macroscopic y velocity . 障害物の位置を初期化します。このとき、障害物内の粒子の数は0とします。 . # Initialize barriers: barrier = np.zeros((height,width), bool) # True wherever there&#39;s a barrier barrier[int((height/2)-8):int((height/2)+8), int(height/2)] = True # simple linear barrier barrierN = np.roll(barrier, 1, axis=0) # sites just north of barriers barrierS = np.roll(barrier, -1, axis=0) # sites just south of barriers barrierE = np.roll(barrier, 1, axis=1) # etc. barrierW = np.roll(barrier, -1, axis=1) barrierNE = np.roll(barrierN, 1, axis=1) barrierNW = np.roll(barrierN, -1, axis=1) barrierSE = np.roll(barrierS, 1, axis=1) barrierSW = np.roll(barrierS, -1, axis=1) . Streaming StepとBounce-back stepです。np.rollにより仮想粒子を移動させます。このとき, 粒子の進行方向と逆向きの順で格子を選び, 分布関数を更新します. . # Move all particles by one step along their directions of motion (pbc): def stream(): global nN, nS, nE, nW, nNE, nNW, nSE, nSW nN = np.roll(nN, 1, axis=0) # axis 0 is north-south; + direction is north nNE = np.roll(nNE, 1, axis=0) nNW = np.roll(nNW, 1, axis=0) nS = np.roll(nS, -1, axis=0) nSE = np.roll(nSE, -1, axis=0) nSW = np.roll(nSW, -1, axis=0) nE = np.roll(nE, 1, axis=1) # axis 1 is east-west; + direction is east nNE = np.roll(nNE, 1, axis=1) nSE = np.roll(nSE, 1, axis=1) nW = np.roll(nW, -1, axis=1) nNW = np.roll(nNW, -1, axis=1) nSW = np.roll(nSW, -1, axis=1) # Use tricky boolean arrays to handle barrier collisions (bounce-back): nN[barrierN] = nS[barrier] nS[barrierS] = nN[barrier] nE[barrierE] = nW[barrier] nW[barrierW] = nE[barrier] nNE[barrierNE] = nSW[barrier] nNW[barrierNW] = nSE[barrier] nSE[barrierSE] = nNW[barrier] nSW[barrierSW] = nNE[barrier] . Collide stepの関数を定義します。 . # Collide particles within each cell to redistribute velocities (could be optimized a little more): def collide(): global rho, ux, uy, n0, nN, nS, nE, nW, nNE, nNW, nSE, nSW rho = n0 + nN + nS + nE + nW + nNE + nSE + nNW + nSW ux = (nE + nNE + nSE - nW - nNW - nSW) / rho uy = (nN + nNE + nNW - nS - nSE - nSW) / rho ux2 = ux * ux # pre-compute terms used repeatedly... uy2 = uy * uy u2 = ux2 + uy2 omu215 = 1 - 1.5*u2 # &quot;one minus u2 times 1.5&quot; uxuy = ux * uy n0 = (1-omega)*n0 + omega * four9ths * rho * omu215 nN = (1-omega)*nN + omega * one9th * rho * (omu215 + 3*uy + 4.5*uy2) nS = (1-omega)*nS + omega * one9th * rho * (omu215 - 3*uy + 4.5*uy2) nE = (1-omega)*nE + omega * one9th * rho * (omu215 + 3*ux + 4.5*ux2) nW = (1-omega)*nW + omega * one9th * rho * (omu215 - 3*ux + 4.5*ux2) nNE = (1-omega)*nNE + omega * one36th * rho * (omu215 + 3*(ux+uy) + 4.5*(u2+2*uxuy)) nNW = (1-omega)*nNW + omega * one36th * rho * (omu215 + 3*(-ux+uy) + 4.5*(u2-2*uxuy)) nSE = (1-omega)*nSE + omega * one36th * rho * (omu215 + 3*(ux-uy) + 4.5*(u2-2*uxuy)) nSW = (1-omega)*nSW + omega * one36th * rho * (omu215 + 3*(-ux-uy) + 4.5*(u2+2*uxuy)) # Force steady rightward flow at ends (no need to set 0, N, and S components): nE[:,0] = one9th * (1 + 3*u0 + 4.5*u0**2 - 1.5*u0**2) nW[:,0] = one9th * (1 - 3*u0 + 4.5*u0**2 - 1.5*u0**2) nNE[:,0] = one36th * (1 + 3*u0 + 4.5*u0**2 - 1.5*u0**2) nSE[:,0] = one36th * (1 + 3*u0 + 4.5*u0**2 - 1.5*u0**2) nNW[:,0] = one36th * (1 - 3*u0 + 4.5*u0**2 - 1.5*u0**2) nSW[:,0] = one36th * (1 - 3*u0 + 4.5*u0**2 - 1.5*u0**2) . 次に流速$ boldsymbol{v}$の回転$ textrm{rot} boldsymbol{v}$の$z$成分を計算する関数を定義します。$ textrm{rot} boldsymbol{v}$の$z$成分は . $$ ( textrm{rot} boldsymbol{v})_z= frac{ partial v_y}{ partial x}- frac{ partial v_x}{ partial y} $$で表されます。偏微分項の中心差分を取ると, . $$ ( textrm{rot} boldsymbol{v})_z approx frac{v_y(x+ epsilon,y)-v_y(x- epsilon, y)}{2 epsilon}- frac{v_x(x, y+ epsilon)-v_x(x, y- epsilon)}{2 epsilon} $$となります。流れの速度場が格子状に離散化されていることから, $ epsilon=1$とすべきであるので次式のように計算されます。 . $$ ( textrm{rot} boldsymbol{v})_z approx frac{v_y(x+1,y)-v_y(x-1, y)}{2}- frac{v_x(x, y+1)-v_x(x, y-1)}{2} $$なお、実装上は2で割ることを省略しています。 . # Compute rot of the macroscopic velocity field: def rot(ux, uy): return np.roll(uy,-1,axis=1) - np.roll(uy,1,axis=1) - np.roll(ux,-1,axis=0) + np.roll(ux,1,axis=0) . シミュレーションのメインの関数です。nextFrame関数を実行することでシミュレーションが進行します。 . # Here comes the graphics and animation theFig = plt.figure(figsize=(8,3)) fluidImage = plt.imshow(rot(ux, uy), origin=&#39;lower&#39;, norm=plt.Normalize(-.1,.1), cmap=plt.get_cmap(&#39;jet&#39;), interpolation=&#39;none&#39;) bImageArray = np.zeros((height, width, 4), np.uint8) bImageArray[barrier,3] = 255 barrierImage = plt.imshow(bImageArray, origin=&#39;lower&#39;, interpolation=&#39;none&#39;) def nextFrame(arg): for step in range(20): stream() collide() fluidImage.set_array(rot(ux, uy)) return (fluidImage, barrierImage) # return the figure elements to redraw animate = animation.FuncAnimation(theFig, nextFrame, interval=1, blit=True) plt.show() . &#21442;&#32771;&#25991;&#29486; . Schroeder, D., Fluid Dynamics Simulation, (2018年1月30日閲覧) | Guo, Z. and Shu, C., 2013, Lattice Boltzmann method and its applications in engineering, World Scientific (Singapore) | 蔦原 道久, 他2名, 1999, 『格子気体法・格子ボルツマン法―新しい数値流体力学の手法』, コロナ社 | 太田 光浩, 他4名, 2015, 『混相流の数値シミュレーション 』, 丸善出版 | Bao, Y. and Meskas, J., 2011。Lattice Boltzmann Method for Fluid Simulations, (2018年1月30日閲覧) | 立石 絢也, 樫山 和男, 2006, 「非圧縮性粘性流体解析のためのCIVA-格子ボルツマン法の精度と安定性」, 『日本計算工学会論文集』, 2006年度号, No.20060008 | .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/physics/2018/01/30/lattice_boltzmann.html",
            "relUrl": "/physics/2018/01/30/lattice_boltzmann.html",
            "date": " • Jan 30, 2018"
        }
        
    
  
    
        ,"post28": {
            "title": "拡散律速凝集",
            "content": "概要 . 拡散律速凝集(Diffusion-limited aggregation;DLA)は、ブラウン運動する粒子が、核となる粒子に結合することで粒子の集合体（クラスタ）が成長する過程のことです。雪の結晶や雷の成長などに見られます。以下のプログラムではブラウン運動する粒子（ここでは水分子）を白色、核となる粒子を色付きとしました。 . プログラムを見ればわかると思いますが、白い粒子が集合体にくっ付くと、自分もその集合体に取り込まれています（取り込まれるようにしたというのが正しいですが）。なお、取り込まれた時間が分かりやすいように、何番目に取り込まれたかで色相を変化させました。 . 粒子の個数を5000個に増やしてみたのが以下の図です。 . 実装 . int edge=5; int NUM=2000; int speed=2; float[] x=new float[NUM]; float[] y=new float[NUM]; float[] r=new float[NUM]; float[] theta=new float[NUM]; float[] fixed_x=new float[NUM]; float[] fixed_y=new float[NUM]; float[] fixed_hue=new float[NUM]; int nan=1000; float hue=0; void setup(){ size(400,400); colorMode(HSB); for(int i=0;i&lt;NUM;i++){ r[i]=random(50,200); theta[i]=random(0,2*PI); x[i]=int(width/2+r[i]*cos(theta[i])); y[i]=int(width/2+r[i]*sin(theta[i])); } fixed_hue[0]=hue; fixed_x[0]=width/2; fixed_y[0]=height/2; for(int i=1;i&lt;NUM;i++){ fixed_x[i]=nan; fixed_y[i]=nan; } noStroke(); } void draw(){ background(0); Disp_Move_Molecular(); Disp_Fixed_Molecular(); CheckHit(); } void Disp_Move_Molecular(){ fill(255); for(int i=0;i&lt;NUM;i++){ if(x[i]!=nan){ theta[i]=random(0,2*PI); x[i]+= speed*cos(theta[i]); y[i]+= speed*sin(theta[i]); ellipse(x[i],y[i],edge,edge); } } } void Disp_Fixed_Molecular(){ for(int i=0;i&lt;NUM;i++){ if(fixed_x[i]!=nan){ fill(fixed_hue[i],255,255); ellipse(fixed_x[i],fixed_y[i],edge,edge); } } } void CheckHit(){ for(int i=0;i&lt;NUM;i++){ if(x[i]!=nan){ for(int j=0;j&lt;NUM;j++){ if(fixed_x[j]!=nan){ if(distance(i,j)&lt;=edge){ fixed_x[i]=x[i]; fixed_y[i]=y[i]; x[i]=nan; y[i]=nan; hue+=0.1; if(hue&gt;255){ hue=0; } fixed_hue[i]=hue; } } } } } } float distance(int i,int j){ float d; d=sq(x[i]-fixed_x[j])+sq(y[i]-fixed_y[j]); return sqrt(d); } .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/processing/2017/11/22/dla.html",
            "relUrl": "/processing/2017/11/22/dla.html",
            "date": " • Nov 22, 2017"
        }
        
    
  
    
        ,"post29": {
            "title": "反応拡散方程式",
            "content": "概要 . 反応拡散方程式は動物の縞模様など発達生物学において非常に重要である。生物の縞模様がシミュレーションと一致することを示したのが、阪大の近藤先生であったりする。 . こんどうしげるの「生命科学の明日はどっちだ？」 | . 今回は、そんな反応拡散方程式をProcessingで実装してみた。左クリック、左ドラッグで描き、右ドラッグで長方形の範囲を消去できる。 . 実装 . //Reaction Diffusion Simulation float[][][] field; float[][][] next_field; float Da=1.0; float Db=0.5; float f=0.0545; float k=0.062; float dt=1; int i,j; int cx,cy,x,y,X,Y; void setup(){ size(400,400); colorMode(HSB); field=new float[width][height][2]; next_field=new float[width][height][2]; for(i=0;i&lt;width;i++){ for(j=0;j&lt;height;j++){ field[i][j][0]=1; field[i][j][1]=0; next_field[i][j][0]=1; next_field[i][j][1]=0; } } for(i=floor(width/2)-5;i&lt;floor(height/2)+5;i++){ for(j=floor(width/2)-5;j&lt;floor(height/2)+5;j++){ field[i][j][1]=1; } } } void draw(){ calculate(); swap(); loadPixels(); for(i=1;i&lt;width-1;i++){ for(j=1;j&lt;height-1;j++){ float col=((next_field[i][j][0]-next_field[i][j][1])) * 255; int pos = i + j * width; //pixels[pos] = color(col); pixels[pos] = color(col,255,255); } } updatePixels(); } void calculate(){ for(i=1;i&lt;width-1;i++){ for(j=1;j&lt;height-1;j++){ float A=field[i][j][0]; float B=field[i][j][1]; next_field[i][j][0]=A+(Da * Laplacian_A(i,j)-A * B * B+f * (1-A)) * dt; next_field[i][j][1]=B+(Db * Laplacian_B(i,j)+A * B * B-(k+f) * B) * dt; next_field[i][j][0]=constrain(next_field[i][j][0],0,1); //値を0～1に制限 next_field[i][j][1]=constrain(next_field[i][j][1],0,1); } } } void swap(){ float[][][] temp = field; field= next_field; next_field= temp; } float Laplacian_A(int i,int j){ float sumA=0; sumA+=field[i][j][0] * (-1); sumA+=field[i-1][j][0] * 0.2; sumA+=field[i+1][j][0] * 0.2; sumA+=field[i][j-1][0] * 0.2; sumA+=field[i][j+1][0] * 0.2; sumA+=field[i-1][j-1][0] * 0.05; sumA+=field[i-1][j+1][0] * 0.05; sumA+=field[i+1][j-1][0] * 0.05; sumA+=field[i+1][j+1][0] * 0.05; return sumA; } float Laplacian_B(int i,int j){ float sumB=0; sumB+=field[i][j][1] * (-1); sumB+=field[i-1][j][1] * 0.2; sumB+=field[i+1][j][1] * 0.2; sumB+=field[i][j-1][1] * 0.2; sumB+=field[i][j+1][1] * 0.2; sumB+=field[i-1][j-1][1] * 0.05; sumB+=field[i-1][j+1][1] * 0.05; sumB+=field[i+1][j-1][1] * 0.05; sumB+=field[i+1][j+1][1] * 0.05; return sumB; } void mouseClicked(){ x=int(mouseX); y=int(mouseY); for(i=x-5;i&lt;x+5;i++){ for(j=y-5;j&lt;y+5;j++){ field[i][j][1]=1; next_field[i][j][1]=1; } } } void mousePressed(){ cx=mouseX; cy=mouseY; loadPixels(); } void mouseDragged(){ if (mouseButton == RIGHT){ updatePixels(); X=mouseX; Y=mouseY; for(i=cx;i&lt;X;i++){ for(j=cy;j&lt;Y;j++){ field[i][j][1]=0; next_field[i][j][1]=0; } } }else if(mouseButton==LEFT){ updatePixels(); X=mouseX; Y=mouseY; for(i=cx;i&lt;X;i++){ for(j=cy;j&lt;Y;j++){ field[i][j][1]=1; next_field[i][j][1]=1; } } } } .",
            "url": "https://salad-bowl-of-knowledge.github.io/hp/processing/2017/11/12/reaction-diffusion.html",
            "relUrl": "/processing/2017/11/12/reaction-diffusion.html",
            "date": " • Nov 12, 2017"
        }
        
    
  

  
  
      ,"page0": {
          "title": "About Us",
          "content": "山拓：神経科学の研究をしています。 Github : https://github.com/takyamamoto | 凪：理学部（）と工学部（）がいるので人間科学部（）になってみる今日このごろ | ざっきぃ：ライフワークはクラシック音楽鑑賞。しばらく統計から離れて医学に集中します。 | 優曇華院：数理物理学者． | みこ：音楽バカ。医者になるのはフルートを吹くため | .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "神経科学",
          "content": "神経表現の可視化 . グリッド細胞の発火パターンをPythonで可視化する | スパイクトリガー平均・共分散の計算法 (Python) | ネコLGN細胞のスパイクトリガー平均による受容野解析 | Sorted plotの注意点 | . 解剖・形態学 . NibabelとMayaviによるMR画像の可視化 | 脳が対側支配をする進化的な利点は何か | . 色覚 . RGBからXYZ, LMS色空間への変換 | CIELUV色相環をPythonで描画する | . 強化学習 . Distributional Reinforcement Learningの仕組み | . 計算神経科学 . 古典的モデル . FitzHugh-Nagumoモデルをアニメーションで見る | Hodgkin-Huxleyモデルをアニメーションで見る | 心臓刺激伝導系の数理モデルのPythonでのシミュレーション | . ニューラルネットワークと神経科学 . 予測符号化 (predictive coding) とは何か | 掛け算のニューラルネットワークとベイズ推定 | 『人工神経回路で脳を理解する』論文まとめ | ニューラルネットワークにおける意味発達の数学理論 | Spiking Neural UnitをChainerで実装してみた | 物体認識のためのRecurrent CNNまとめ | FORCE法によるRecurrent Spiking Neural Networksの教師あり学習 | 人工神経回路による脳の理解はどこまで進んだか | . Computational neuroscience (Coursera) . Computational neuroscience : Week0 | Computational neuroscience : Week1 | Computational neuroscience : Week2 | Computational neuroscience : Week3 | .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/neuroscience/",
          "relUrl": "/neuroscience/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "機械学習",
          "content": "最適化 . [線形計画法（シンプレックス法）入門　PDFファイル] | Hamiltonian Descent Methodsの実装についての解説 | Boltzmann Generatorsの解説 | . Deep Learning . Chainer . Chainerで学習率のスケジューリングをする方法 | ChainerでPredNetの実装をしてみた | . TensorFlow . tensorflow-gpuとCUDAのバージョン | . Keras . Kerasにおける中間層の出力の可視化 | KerasによるGraph Convolutional Networks | Kerasにおいてforループでmodelを定義する | Keras examples directoryで実装例を見る | KerasのconvLSTM2Dの使用例を見る | . 強化学習 . OpenAI Gymでオリジナルの環境を作る | Kerasを用いたDQNでFlappyBirdを学習させる | .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/machine-learning/",
          "relUrl": "/machine-learning/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "統計学",
          "content": "Pythonで統計処理 . 分散共分散行列を相関行列/偏相関係数に変換 | Graph Lassoによる変数間の関係のグラフ化 | Pythonで線形回帰モデルの計算（１） | Pythonで線形回帰モデルの計算（２） | Pythonで正規分布の区間推定 | PythonでGaussian Fitting | scipyによる1次元混合ガウス回帰 | matplotlibのみで線形回帰の信頼区間を描画する | matplotlibで棒グラフ間の有意差の描画をする | Pythonによる分位点回帰 (Quantile regression) | . 確率論 . 確率モデル . 確率過程とランダムウォーク | マルコフ連鎖 (Markov chain) | . 確率分布 . q分位数と中央値 | 確率変数の変換 | デルタ法 (Delta method) | . 統計的推定・検定 . 比率の区間推定 | ノンパラメトリック検定 | ケンドールの一致係数(Kendall’s coefficient of concordance) | . 統計解析 . 回帰分析 . 重回帰分析① | 重回帰分析② | 正規方程式 | ロジスティック回帰分析 | 対数オッズ比の分散 | . データ分析 . Principle Curve 入門 | PythonによるPrincipal Curveの実装 (bendingアルゴリズム) | . 稲垣宣生 「数理統計学」演習問題の解答 . 書いたものをあげます。 . 演習問題1 | 演習問題2 | 演習問題3 | 演習問題4 | 演習問題5 | 演習問題6 | 演習問題7 | 演習問題8 | 演習問題9 | 演習問題10 | 演習問題11 | 演習問題12 | 演習問題13 | 演習問題14 | . 統計検定 . 統計検定2級のフィードバック（前編） | 統計検定2級のフィードバック（後編） | .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/statistics/",
          "relUrl": "/statistics/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "医学",
          "content": "生理学 . 心臓震盪とリミットサイクル | . 薬理学 . コンパートメントモデル(薬物動態)のシミュレーション | . 環境医学・公衆衛生学 . 疫学基礎 | . 医用工学 . PET検査の仕組み（第1回）「PETの基礎理論」 | PET検査の仕組み（第2回）「画像再構成①：Radon変換」 | PET検査の仕組み（第3回）「画像再構成②：逆Radon変換」 | PET検査の仕組み（第4回）「画像再構成③：FBP法」 | PET検査の仕組み（第5回）「誤差要因の補正(1)：偶発同時計数と散乱同時計数」 | PET検査の仕組み（第6回）「誤差要因の補正(2)：吸収と感度の補正」 | PET検査の仕組み（第7回）「PETシミュレーション」 | 脳波と脳磁図 (PDFファイル) | . シングルセル解析 . scEpathの解説 | Palantirの解説 | scGenの解説 | . 医学知識の整理 . 大学の試験対策用に作成したデータを、学問別に公開しようと思う。初学者が、各学問の全貌を大雑把に把握するのに適していると思う。 . メモ帳というアプリで毎回作っているため、基本的には図がない。ビジュアル面は、自身の参考書などで補ってほしい。 . 組織学 . 骨 | 神経 | 血液 | 心血管系 | 呼吸器系 | 食道・胃 | 肝・胆・膵 | 小腸・大腸 | 口腔（歯・舌・唾液腺） | 内分泌系１ | 内分泌系２ | 泌尿器系 | 眼・耳 | . 生理学 . 細胞の一般生理 | 膜電位と興奮 | シナプス | 筋生理 | 血液生理 | 呼吸 | 循環１ | 循環２ | 循環３ | 末梢神経・自律神経 | . 生物科学（Essential細胞生物学11～16章） . 遺伝学（Essential細胞生物学5～10章） . 解剖学 . 神経解剖学 .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/medicine/",
          "relUrl": "/medicine/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "化学",
          "content": "化学 .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/chemistry/",
          "relUrl": "/chemistry/",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "物理学",
          "content": "物理学 .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/physics/",
          "relUrl": "/physics/",
          "date": ""
      }
      
  

  
      ,"page7": {
          "title": "数学",
          "content": "数学 .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/mathematics/",
          "relUrl": "/mathematics/",
          "date": ""
      }
      
  

  
      ,"page8": {
          "title": "Programming",
          "content": "Programming .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/programming/",
          "relUrl": "/programming/",
          "date": ""
      }
      
  

  
      ,"page9": {
          "title": "Computer",
          "content": "Computer .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/computer/",
          "relUrl": "/computer/",
          "date": ""
      }
      
  

  
      ,"page10": {
          "title": "読書",
          "content": "読書 .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/reading/",
          "relUrl": "/reading/",
          "date": ""
      }
      
  

  
      ,"page11": {
          "title": "エッセイ",
          "content": "エッセイ .",
          "url": "https://salad-bowl-of-knowledge.github.io/hp/essay/",
          "relUrl": "/essay/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  
  

  
  

  

}