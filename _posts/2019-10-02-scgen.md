---
toc: true
layout: post
description:
author: 優曇華院
categories: [single-cell-analysis]
title: scGenの解説
---

## 概要
[scGenの論文](https://www.nature.com/articles/s41592-019-0494-8)の解説．実際はほぼVAEの話になった．実験の詳細とかは割愛．LPSが異なる生物に与える影響とかを予測していてかなりアツい．

細胞が外界から刺激を受けたときにどんな反応をするかを予測する．潜在空間で摂動$\delta$が加えられたときに，遺伝子発現空間での変化をニューラルネットワークを使って予測する．

## VAE    　
variational autoencoder(VAE)では確率分布$P(\boldsymbol{x} _ i;\boldsymbol{\theta})$に従って新しいデータ点が生成される．ただし，確率分布は$P(\boldsymbol{x} _ i;\boldsymbol{\theta})$の対数尤度（を各$\boldsymbol{x} _ i$について足したもの）が最大となるように取る．潜在変数を$\boldsymbol{z}$とすれば，この確率は次のようになる：

$$
\begin{aligned}
  P(\boldsymbol{x} _ i;\boldsymbol{\theta})=\int P(\boldsymbol{x} _ i\mid \boldsymbol{z} _ i;\boldsymbol{\theta})P(\boldsymbol{z} _ i;\boldsymbol{\theta})\,d\boldsymbol{z} _ i.
\end{aligned}
$$

$\boldsymbol{x} _ i$を生成しそうな$\boldsymbol{z} _ i$が潜在空間から正規分布$P(\boldsymbol{z} _ i;\boldsymbol{\theta})$に従ってサンプリングされるような確率分布を求めることが目標になる．

ここで，$P(\boldsymbol{x} _ i\mid \boldsymbol{z} _ i;\boldsymbol{\theta})$に近い確率分布$Q(\boldsymbol{z} _ i\mid \boldsymbol{x} _ i;\boldsymbol{\theta})$をニューラルネットワークで作る．２つの確率分布の近さの評価としてKullback Leibler divergenceを使う：

$$
\begin{aligned}
    &\text{KL}(Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})\|P(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\theta}))\\
    &= E_{Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})}\left[\log Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})-\log P(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\theta})\right]\\
    &= E_{Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})}\Bigl[\log Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})-\log\frac{P(\boldsymbol{z} _ i;\boldsymbol{\theta})}{P(\boldsymbol{x} _ i;\boldsymbol{\theta})}P(\boldsymbol{x} _ i\mid\boldsymbol{z} _ i;\boldsymbol{\theta})\Bigr]\\
    &= E_{Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})}\bigl[\log Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})-\log P(\boldsymbol{z} _ i;\boldsymbol{\theta})-\log P(\boldsymbol{x} _ i\mid\boldsymbol{z} _ i;\boldsymbol{\theta})+\log P(\boldsymbol{x} _ i;\boldsymbol{\theta})\bigr]\\
    &= -E_{Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})}[\log P(\boldsymbol{x} _ i\mid\boldsymbol{z} _ i;\boldsymbol{\theta})]+\log P(\boldsymbol{x} _ i;\boldsymbol{\theta})+\text{KL}(Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})\| P(\boldsymbol{z} _ i;\boldsymbol{\theta})).
\end{aligned}
$$

式変形の途中でBayesの定理を用いた．$Q(\boldsymbol{z} _ i\mid \boldsymbol{x} _ i;\boldsymbol{\theta})$は$P(\boldsymbol{x} _ i\mid \boldsymbol{z} _ i; \boldsymbol{\theta})$の近似であるので，これらの量のKullback Leibler divergenceはほぼ$0$である．よって，

$$
\log P(\boldsymbol{x} _ i;\boldsymbol{\theta})\simeq E_{Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})}[\log P(\boldsymbol{x} _ i\mid\boldsymbol{z} _ i;\boldsymbol{\theta})]-\text{KL}(Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})\| P(\boldsymbol{z} _ i;\boldsymbol{\theta})).
$$

$\eqref{ELBO}$第１項は解析的に解くことは困難なので，Monte Carlo法によるサンプリングによって決める．ただし，$Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})$に従って$\boldsymbol{z} _ i$を選び出す：

$$
\begin{aligned}
  \boldsymbol{z} _ i\sim Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})
\end{aligned}
$$

のは不便であるので，再パラメータ化を考える．すなわち，

$$
\begin{aligned}
  \boldsymbol{z} _ i = g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}, \boldsymbol{x} _ i),\quad \boldsymbol{\epsilon}\sim p(\boldsymbol{\epsilon})
\end{aligned}
$$

となる関数$g_{\boldsymbol{\phi}}$とノイズ$\boldsymbol{\epsilon}$を適当に選んでやる．こうすれば，


$$
\begin{aligned}
E_{Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})}[f(\boldsymbol{z} _ i)] &= \int Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi}) f(\boldsymbol{z} _ i)d\boldsymbol{z} _ i\\
    &= \int p(\boldsymbol{\epsilon}) f(\boldsymbol{z} _ i)d\boldsymbol{\epsilon}\\
    &= E_{p(\boldsymbol{\epsilon})}[f(\boldsymbol{z} _ i)],\quad \boldsymbol{z} _ i = g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}, \boldsymbol{x} _ i)
\end{aligned}
$$

となる．よって，再パラメータ化を施せば$\eqref{ELBO}$第１項は

$$
\begin{aligned}
    E_{Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})}[\log P(\boldsymbol{x} _ i\mid\boldsymbol{z} _ i;\boldsymbol{\theta})] &= \frac{1}{L}\sum_{l=1}^L\log P(\boldsymbol{x} _ i\mid\boldsymbol{z} _ i{}^{(l)};\boldsymbol{\theta})
\end{aligned}
$$

となる．ただし，

$$
\begin{aligned}
  \boldsymbol{z} _ i{}^{(l)}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(l)}, \boldsymbol{x} _ i),\quad \boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})
\end{aligned}
$$

である．よって，$\eqref{ELBO}$は次のようになる：

$$
\begin{aligned}
    &\log P(\boldsymbol{x} _ i;\boldsymbol{\theta})\\
    &=\frac{1}{L}\sum_{l=1}^L\log P(\boldsymbol{x} _ i\mid\boldsymbol{z} _ i{}^{(l)};\boldsymbol{\theta})-\text{KL}(Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})\| P(\boldsymbol{z} _ i;\boldsymbol{\theta})).
\end{aligned}
$$

ここで，$Q(\boldsymbol{z}\mid\boldsymbol{x};\boldsymbol{\theta})$が多変量正規分布であると仮定する：

$$
\begin{aligned}
    &Q(\boldsymbol{z}\mid\boldsymbol{x};\boldsymbol{\theta})\\
    &=\frac{1}{\sqrt{(2\pi)^n\mid\Sigma\mid}}\exp\left[-\frac{1}{2}{}^t(\boldsymbol{z}-\boldsymbol\mu)\Sigma{}^{-1}(\boldsymbol{z}-\boldsymbol\mu)\right].
\end{aligned}
$$

ただし，$\Sigma,\boldsymbol\mu$は$\boldsymbol{x}$によって決まり，特に$\Sigma$は対角行列であるとする：

$$
\begin{aligned}
  \Sigma =
  \begin{pmatrix}
    \sigma_1{}^2 &&&O\\
    & \sigma_2{}^2 &&\\
    &&\ddots&\\
    O &&& \sigma_n{}^2
  \end{pmatrix}.
\end{aligned}
$$

さらに，$P(\boldsymbol{z};\boldsymbol{\theta})$も正規分布であるとする：

$$
\begin{aligned}
  P(\boldsymbol{z};\boldsymbol{\theta}) = \frac{1}{\sqrt{(2\pi)^n}}\exp\left(-\frac{\mid\boldsymbol{z}\mid^2}{2}\right).
\end{aligned}
$$

まず，

$$
\begin{aligned}
    &\int Q(\boldsymbol{z}\mid\boldsymbol{x};\boldsymbol{\theta})\log P(\boldsymbol{z};\boldsymbol{\theta})\,d\boldsymbol{z} \\
    &= \frac{1}{\sqrt{(2\pi)^n\mid\Sigma\mid}}\int\exp\left[-\frac{1}{2}{}^t(\boldsymbol{z}-\boldsymbol\mu)\Sigma{}^{-1}(\boldsymbol{z}-\boldsymbol\mu)\right]\times\left[-\frac{n}{2}\log(2\pi)-\frac{\mid\boldsymbol{z}\mid^2}{2}\right]\,d\boldsymbol{z}\\
    &= -\frac{n}{2}\log(2\pi)-\frac{1}{2\sqrt{(2\pi)^n\mid\Sigma\mid}}\times\int\mid\boldsymbol{z}\mid^2\exp\left[-\frac{1}{2}{}^t(\boldsymbol{z}-\boldsymbol\mu)\Sigma{}^{-1}(\boldsymbol{z}-\boldsymbol\mu)\right]\,d\boldsymbol{z}
\end{aligned}
$$

第２項の積分は

$$
\begin{aligned}
    &\sum_{i=1}^n\int z_i{}^2\exp\left[-\frac{1}{2}\sum_{j=1}^n\frac{(z_j-\mu_{j})^2}{\sigma_j{}^2}\right]\,d\boldsymbol{z}\\
    &=\sum_{i=1}^n\int \exp\left[-\frac{(z_1-\mu_{1})^2}{2\sigma_1{}^2}\right]\,dz_1\times\\
    &\qquad\dots\times\int z_i{}^2\exp\left[-\frac{(z_j-\mu_{j})^2}{2\sigma_j{}^2}\right]\,dz_i\times\\
    &\qquad\dots\times\int \exp\left[-\frac{(z_n-\mu_{n})^2}{2\sigma_n{}^2}\right]\,dz_n\\
    &= \sum_{i=1}^n(2\pi)^{\frac{n-1}{2}}\prod_{j\neq i}\sigma_j\int z_i{}^2\exp\left[-\frac{(z_j-\mu_{j})^2}{2\sigma_j{}^2}\right]\,dz_i\\
    &= \sum_{i=1}^n(2\pi)^{\frac{n}{2}}\prod_{j=1}^n\sigma_j(\sigma_i{}^2+\mu_{i}{}^2)
\end{aligned}
$$

のように変形できるので，
