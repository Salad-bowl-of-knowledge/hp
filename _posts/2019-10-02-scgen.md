---
toc: true
layout: post
description:
author: 優曇華院
categories: [single-cell-analysis]
title: scGenの解説
---

## 概要
[scGenの論文](https://www.nature.com/articles/s41592-019-0494-8)の解説．実際はほぼVAEの話になった．実験の詳細とかは割愛．LPSが異なる生物に与える影響とかを予測していてかなりアツい．

細胞が外界から刺激を受けたときにどんな反応をするかを予測する．潜在空間で摂動$\delta$が加えられたときに，遺伝子発現空間での変化をニューラルネットワークを使って予測する．

## VAE    　
variational autoencoder(VAE)では確率分布$P(\boldsymbol{x} _ i;\boldsymbol{\theta})$に従って新しいデータ点が生成される．ただし，確率分布は$P(\boldsymbol{x} _ i;\boldsymbol{\theta})$の対数尤度（を各$\boldsymbol{x} _ i$について足したもの）が最大となるように取る．潜在変数を$\boldsymbol{z}$とすれば，この確率は次のようになる：

$$
\begin{aligned}
P(\boldsymbol{x} _ i;\boldsymbol{\theta})=\int P(\boldsymbol{x} _ i\mid \boldsymbol{z} _ i;\boldsymbol{\theta})P(\boldsymbol{z} _ i;\boldsymbol{\theta})\,d\boldsymbol{z} _ i.
\end{aligned}
$$

$\boldsymbol{x} _ i$を生成しそうな$\boldsymbol{z} _ i$が潜在空間から正規分布$P(\boldsymbol{z} _ i;\boldsymbol{\theta})$に従ってサンプリングされるような確率分布を求めることが目標になる．

ここで，$P(\boldsymbol{x} _ i\mid \boldsymbol{z} _ i;\boldsymbol{\theta})$に近い確率分布$Q(\boldsymbol{z} _ i\mid \boldsymbol{x} _ i;\boldsymbol{\theta})$をニューラルネットワークで作る．２つの確率分布の近さの評価としてKullback Leibler divergenceを使う：

$$
\begin{aligned}
&\text{KL}(Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})\|P(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\theta}))\\
&= E _ {Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})}\left[\log Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})-\log P(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\theta})\right]\\
&= E _ {Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})}\Bigl[\log Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})-\log\frac{P(\boldsymbol{z} _ i;\boldsymbol{\theta})}{P(\boldsymbol{x} _ i;\boldsymbol{\theta})}P(\boldsymbol{x} _ i\mid\boldsymbol{z} _ i;\boldsymbol{\theta})\Bigr]\\
&= E _ {Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})}\bigl[\log Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})-\log P(\boldsymbol{z} _ i;\boldsymbol{\theta})-\log P(\boldsymbol{x} _ i\mid\boldsymbol{z} _ i;\boldsymbol{\theta})+\log P(\boldsymbol{x} _ i;\boldsymbol{\theta})\bigr]\\
&= -E _ {Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})}[\log P(\boldsymbol{x} _ i\mid\boldsymbol{z} _ i;\boldsymbol{\theta})]+\log P(\boldsymbol{x} _ i;\boldsymbol{\theta})+\text{KL}(Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})\| P(\boldsymbol{z} _ i;\boldsymbol{\theta})).
\end{aligned}
$$

式変形の途中でBayesの定理を用いた．$Q(\boldsymbol{z} _ i\mid \boldsymbol{x} _ i;\boldsymbol{\theta})$は$P(\boldsymbol{x} _ i\mid \boldsymbol{z} _ i;\boldsymbol{\theta})$の近似であるので，これらの量のKullback Leibler divergenceはほぼ$0$である．よって，

$$
\begin{aligned}
\log P(\boldsymbol{x} _ i;\boldsymbol{\theta})\simeq E _ {Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})}[\log P(\boldsymbol{x} _ i\mid\boldsymbol{z} _ i;\boldsymbol{\theta})]-\text{KL}(Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})\| P(\boldsymbol{z} _ i;\boldsymbol{\theta})).
\end{aligned}
$$

$\eqref{ELBO}$第１項は解析的に解くことは困難なので，Monte Carlo法によるサンプリングによって決める．ただし，$Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})$に従って$\boldsymbol{z} _ i$を選び出す：

$$
\boldsymbol{z} _ i\sim Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})
$$

のは不便であるので，再パラメータ化を考える．すなわち，

$$
\boldsymbol{z} _ i = g _ {\boldsymbol{\phi}}(\boldsymbol{\epsilon}, \boldsymbol{x} _ i),\quad \boldsymbol{\epsilon}\sim p(\boldsymbol{\epsilon})
$$

となる関数$g _ {\boldsymbol{\phi}}$とノイズ$\boldsymbol{\epsilon}$を適当に選んでやる．こうすれば，

$$
\begin{aligned}
E _ {Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})}[f(\boldsymbol{z} _ i)] &= \int Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi}) f(\boldsymbol{z} _ i)d\boldsymbol{z} _ i\\
&= \int p(\boldsymbol{\epsilon}) f(\boldsymbol{z} _ i)d\boldsymbol{\epsilon}\\
&= E _ {p(\boldsymbol{\epsilon})}[f(\boldsymbol{z} _ i)],\quad \boldsymbol{z} _ i = g _ {\boldsymbol{\phi}}(\boldsymbol{\epsilon}, \boldsymbol{x} _ i)
\end{aligned}
$$

となる．よって，再パラメータ化を施せば$\eqref{ELBO}$第１項は

$$
E _ {Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})}[\log P(\boldsymbol{x} _ i\mid\boldsymbol{z} _ i;\boldsymbol{\theta})] = \frac{1}{L}\sum _ {l=1} ^ L\log P(\boldsymbol{x} _ i\mid\boldsymbol{z} _ i ^ {(l)};\boldsymbol{\theta})
$$

となる．ただし，

$$
\boldsymbol{z} _ i ^ {(l)}=g _ {\boldsymbol{\phi}}(\boldsymbol{\epsilon} ^ {(l)}, \boldsymbol{x} _ i),\quad \boldsymbol{\epsilon} ^ {(l)}\sim p(\boldsymbol{\epsilon})
$$

である．よって，$\eqref{ELBO}$は次のようになる：

$$
\log P(\boldsymbol{x} _ i;\boldsymbol{\theta})=\frac{1}{L}\sum _ {l=1} ^ L\log P(\boldsymbol{x} _ i\mid\boldsymbol{z} _ i ^ {(l)};\boldsymbol{\theta})-\text{KL}(Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})\| P(\boldsymbol{z} _ i;\boldsymbol{\theta})).
$$

ここで，$Q(\boldsymbol{z}\mid\boldsymbol{x};\boldsymbol{\theta})$が多変量正規分布であると仮定する：

$$
Q(\boldsymbol{z}\mid\boldsymbol{x};\boldsymbol{\theta})=\frac{1}{\sqrt{(2\pi) ^ n\mid\Sigma\mid}}\exp\left[-\frac{1}{2} ^ t(\boldsymbol{z}-\boldsymbol\mu)\Sigma ^ {-1}(\boldsymbol{z}-\boldsymbol\mu)\right].
$$

ただし，$\Sigma,\boldsymbol\mu$は$\boldsymbol{x}$によって決まり，特に$\Sigma$は対角行列であるとする：

$$
\begin{aligned}
\Sigma =
\begin{pmatrix}
\sigma _ 1 ^ 2 &&&O\\
& \sigma _ 2 ^ 2 &&\\
&&\ddots&\\
O &&& \sigma _ n ^ 2
\end{pmatrix}.
\end{aligned}
$$

さらに，$P(\boldsymbol{z};\boldsymbol{\theta})$も正規分布であるとする：

$$
P(\boldsymbol{z};\boldsymbol{\theta}) = \frac{1}{\sqrt{(2\pi) ^ n}}\exp\left(-\frac{\mid\boldsymbol{z}\mid ^ 2}{2}\right).
$$

まず，

$$
\begin{aligned}
&\int Q(\boldsymbol{z}\mid\boldsymbol{x};\boldsymbol{\theta})\log P(\boldsymbol{z};\boldsymbol{\theta})\,d\boldsymbol{z} \\
&= \frac{1}{\sqrt{(2\pi) ^ n\mid\Sigma\mid}}\int\exp\left[-\frac{1}{2} ^ t(\boldsymbol{z}-\boldsymbol\mu)\Sigma ^ {-1}(\boldsymbol{z}-\boldsymbol\mu)\right]\times\left[-\frac{n}{2}\log(2\pi)-\frac{\mid\boldsymbol{z}\mid ^ 2}{2}\right]\,d\boldsymbol{z}\\
&= -\frac{n}{2}\log(2\pi)-\frac{1}{2\sqrt{(2\pi) ^ n\mid\Sigma\mid}}\times\int\mid\boldsymbol{z}\mid ^ 2\exp\left[-\frac{1}{2} ^ t(\boldsymbol{z}-\boldsymbol\mu)\Sigma ^ {-1}(\boldsymbol{z}-\boldsymbol\mu)\right]\,d\boldsymbol{z}
\end{aligned}
$$

第２項の積分は

$$
\begin{aligned}
&\sum _ {i=1} ^ n\int z _ i ^ 2\exp\left[-\frac{1}{2}\sum _ {j=1} ^ n\frac{(z _ j-\mu _ {j}) ^ 2}{\sigma _ j ^ 2}\right]\,d\boldsymbol{z}\\
&=\sum _ {i=1} ^ n\int \exp\left[-\frac{(z _ 1-\mu _ {1}) ^ 2}{2\sigma _ 1 ^ 2}\right]\,dz _ 1\times\\
&\qquad\dots\times\int z _ i ^ 2\exp\left[-\frac{(z _ j-\mu _ {j}) ^ 2}{2\sigma _ j ^ 2}\right]\,dz _ i\times\\
&\qquad\dots\times\int \exp\left[-\frac{(z _ n-\mu _ {n}) ^ 2}{2\sigma _ n ^ 2}\right]\,dz _ n\\
&= \sum _ {i=1} ^ n(2\pi) ^ {\frac{n-1}{2}}\prod _ {j\neq i}\sigma _ j\int z _ i ^ 2\exp\left[-\frac{(z _ j-\mu _ {j}) ^ 2}{2\sigma _ j ^ 2}\right]\,dz _ i\\
&= \sum _ {i=1} ^ n(2\pi) ^ {\frac{n}{2}}\prod _ {j=1} ^ n\sigma _ j(\sigma _ i ^ 2+\mu _ {i} ^ 2)
\end{aligned}
$$

のように変形できるので，

$$
\int Q(\boldsymbol{z}\mid\boldsymbol{x};\boldsymbol{\theta})\log P(\boldsymbol{z};\boldsymbol{\theta})\,d\boldsymbol{z}= -\frac{n}{2}\log(2\pi)-\frac{1}{2}\sum _ {i=1} ^ n(\sigma _ i ^ 2+\mu _ {i} ^ 2).
$$

次に，

$$
\begin{aligned}
&\int Q(\boldsymbol{z}\mid\boldsymbol{x};\boldsymbol{\theta})\log Q(\boldsymbol{z}\mid\boldsymbol{x};\boldsymbol{\theta})\,d\boldsymbol{z}\\
&=\frac{1}{\sqrt{(2\pi) ^ n\mid\Sigma\mid}}\int\exp\left[-\frac{1}{2}\sum _ {i=1} ^ n\frac{(z _ i-\mu _ {i}) ^ 2}{\sigma _ i ^ 2}\right]\times\left[-\frac{n}{2}\log 2\pi-\frac{1}{2}\log\mid\Sigma\mid-\frac{1}{2}\sum _ {j=1} ^ n\frac{(z _ j-\mu _ {j}) ^ 2}{\sigma _ j ^ 2}\right]\,d\boldsymbol{z}\\
&=-\frac{n}{2}\log 2\pi-\frac{1}{2}\log\mid\Sigma\mid-\frac{1}{2\sqrt{(2\pi) ^ n\mid\Sigma\mid}}\times\sum _ {i=1} ^ n\int\frac{(z _ i-\mu _ {i}) ^ 2}{\sigma _ i ^ 2}\exp\left[-\sum _ {j=1} ^ n\frac{(z _ j-\mu _ {j}) ^ 2}{2\sigma _ j ^ 2}\right]\,d\boldsymbol{z}
\end{aligned}
$$

第３項の積分は

$$
y=x
$$


のように変形できるので，

$$
\int Q(\boldsymbol{z}\mid\boldsymbol{x};\boldsymbol{\theta})\log Q(\boldsymbol{z}\mid\boldsymbol{x};\boldsymbol{\theta})\,d\boldsymbol{z}=-\frac{n}{2}\log(2\pi)-\frac{1}{2}\sum _ {j=1} ^ n(1+\log\sigma _ j ^ 2).
$$

$\eqref{KL _ 1}$，$\eqref{KL _ 2}$から，$\eqref{ELBO}$第２項は，

$$
\begin{aligned}
&-\text{KL}(Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})\| P(\boldsymbol{z} _ i;\boldsymbol{\theta}))\\
&=-\int Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\theta})\log Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\theta})\,d\boldsymbol{z} _ i+\int Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\theta})\log P(\boldsymbol{z} _ i;\boldsymbol{\theta})\,d\boldsymbol{z} _ i\\
&= \frac{1}{2}\sum _ {j=1} ^ n[(1+\log{\sigma _ j} ^ 2)-\mu _ j ^ 2-\sigma _ j ^ 2].
\end{aligned}
$$

また，今回は$Q(\boldsymbol{z} _ i\mid\boldsymbol{x} _ i;\boldsymbol{\phi})$が正規分布$\mathcal{N}(\boldsymbol{z} _ i,\boldsymbol{\mu},\boldsymbol{\sigma}\odot\boldsymbol{\sigma}E)$であるとしたので，再パラメータ化は

$$
g _ {\boldsymbol{\phi}}(\boldsymbol{\epsilon},\boldsymbol{x} _ i) = \boldsymbol{\mu}(\boldsymbol{x} _ i)+\boldsymbol{\sigma}(\boldsymbol{x} _ i)\odot\boldsymbol{\epsilon},\quad\boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{\epsilon},0,E)
$$

とする．これは，

$$
E _ {\mathcal{N}(\boldsymbol{\epsilon},0,E)}[\boldsymbol{\mu}(\boldsymbol{x} _ i)+\boldsymbol{\sigma}(\boldsymbol{x} _ i)\odot\boldsymbol{\epsilon}]=\boldsymbol{\mu}(\boldsymbol{x} _ i)
$$

および

$$
E _ {\mathcal{N}(\boldsymbol{\epsilon},0,E)}[(\mu _ i+\sigma _ i\epsilon _ i-\mu _ i)(\mu _ j+\sigma _ j\epsilon _ j-\mu _ j)]= \delta _ {ij}\sigma _ i\sigma _ j
$$

から分かる．

以上から，

$$
\begin{aligned}
\log P(\boldsymbol{x} _ i;\boldsymbol{\theta}) &= \frac{1}{2}\sum _ {j=1} ^ n[(1+\log{\sigma _ j} ^ 2)-\mu _ j ^ 2-\sigma _ j ^ 2]+\frac{1}{L}\sum _ {l=1} ^ L\log P(\boldsymbol{x} _ i\mid\boldsymbol{z} _ i ^ {(l)};\boldsymbol{\theta}).
\end{aligned}
$$

ただし，

$$
\begin{aligned}
\boldsymbol{z} _ i ^ {(l)}&= \boldsymbol{\mu} ^ {(l)}(\boldsymbol{x} _ i)+\boldsymbol{\sigma} ^ {(l)}(\boldsymbol{x} _ i)\odot\boldsymbol{\epsilon} ^ {(l)},\\
\boldsymbol{\epsilon} ^ {(l)}&\sim\mathcal{N}(\boldsymbol{\epsilon},0,E).
\end{aligned}
$$

これが最大になるように，NNを構成すればよい．

## 摂動$\delta$の予想
各状態にある細胞を抽出し，細胞の数によるバイアスを無くすために，調節する．最後に

$$
\delta=\overline{z _ 1}-\overline{z _ 0}
$$

を計算する．$\overline{z _ 0}$は各状態の潜在変数の平均，$\overline{z _ 1}$は摂動があったときの潜在変数の平均．
